{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "num_qubits = 7\n",
    "dataset_filename = \"dataset/dataset_tesi/NN1_Dataset(<=10Cx)_balanced1.csv\"\n",
    "df = pd.read_csv(dataset_filename)\n",
    "\n",
    "links = [set([0,1]), set([1,2]), set([1,3]), set([3,5]), set([4,5]), set([5,6])]\n",
    "def generate_columns(header, links, in_links=False):\n",
    "    if in_links:\n",
    "        return [header+str(i)+str(j) for i in range(num_qubits) for j in range(num_qubits) if set([i,j]) in links]\n",
    "    else:\n",
    "        return [header+str(i)+str(j) for i in range(num_qubits) for j in range(num_qubits) if set([i,j]) not in links and i!=j]\n",
    "\n",
    "useless_columns = ['Unnamed: 0', 'last_update_date', 'N_qubtis', 'N_measure', 'N_cx', 'backend_name']\n",
    "useless_columns += generate_columns(\"cx_\", links)\n",
    "useless_columns += generate_columns(\"edge_length_\", links)\n",
    "useless_columns += generate_columns(\"edge_error_\", links)\n",
    "useless_columns += [\"measure_\"+str(i) for i in range(num_qubits)]\n",
    "# Note that cx/edge_error/edge_length_xy is not neccessarily the same as cx/edge_length/edge_error_yx\n",
    "df.drop(columns=useless_columns, inplace=True)\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7026, 57), (7026, 49), (1757, 57), (1757, 49))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "df_train_x, df_train_y= train.iloc[:, :-num_qubits], train.iloc[:, -num_qubits:]\n",
    "df_test_x, df_test_y= test.iloc[:, :-num_qubits], test.iloc[:, -num_qubits:]\n",
    "\n",
    "train_x = scaler.fit_transform(df_train_x)\n",
    "test_x = scaler.fit_transform(df_test_x)\n",
    "\n",
    "\n",
    "# for every row in y, convert to 1 hot-encoding and flatten\n",
    "train_y = []\n",
    "for _, row in df_train_y.iterrows():\n",
    "    train_y.append(pd.get_dummies(row).values.flatten())\n",
    "train_y = np.array(train_y)\n",
    "\n",
    "test_y = []\n",
    "for _, row in df_test_y.iterrows():\n",
    "    test_y.append(pd.get_dummies(row).values.flatten())\n",
    "test_y = np.array(test_y)\n",
    "\n",
    "(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacky/anaconda3/envs/eece571f/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.lin1 = Linear(num_features, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin3 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin4 = Linear(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = x.relu()\n",
    "        x = self.lin2(x)\n",
    "        x = x.relu()\n",
    "        x = self.lin3(x)\n",
    "        x = x.relu()\n",
    "        x = self.lin4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(57, 49, hidden_channels=128)\n",
    "criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.03, weight_decay=5e-4)  # Define optimizer.\n",
    "\n",
    "torch_train_x = torch.tensor(train_x, dtype=torch.float)\n",
    "torch_train_y = torch.tensor(train_y, dtype=torch.float)\n",
    "torch_test_x = torch.tensor(test_x, dtype=torch.float)\n",
    "torch_test_y = torch.tensor(test_y, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(torch_train_x)  # Perform a single forward pass.\n",
    "      loss = criterion(out, torch_train_y)  # Compute the loss solely based on the training nodes.\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test(x, y):\n",
    "      model.eval()\n",
    "      pred = model(x)\n",
    "\n",
    "      test_correct = 0\n",
    "      for i, j in zip(pred, y):\n",
    "          pred_i = np.argmax(i.detach().numpy().reshape(7,7), axis=1)\n",
    "          label_j = np.argmax(j.detach().numpy().reshape(7,7), axis=1)\n",
    "          test_correct += np.array_equal(pred_i, label_j)\n",
    "      test_acc = int(test_correct) / int(y.shape[0])  # Derive ratio of correct predictions.\n",
    "      return test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 27.0802\n",
      "Epoch: 002, Loss: 21.9579\n",
      "Epoch: 003, Loss: 21.3872\n",
      "Epoch: 004, Loss: 20.5391\n",
      "Epoch: 005, Loss: 19.8699\n",
      "Epoch: 006, Loss: 19.8736\n",
      "Epoch: 007, Loss: 19.2467\n",
      "Epoch: 008, Loss: 18.8652\n",
      "Epoch: 009, Loss: 18.0400\n",
      "Epoch: 010, Loss: 17.9096\n",
      "Epoch: 011, Loss: 17.4184\n",
      "Epoch: 012, Loss: 17.1653\n",
      "Epoch: 013, Loss: 17.3061\n",
      "Epoch: 014, Loss: 17.5060\n",
      "Epoch: 015, Loss: 17.1400\n",
      "Epoch: 016, Loss: 17.1882\n",
      "Epoch: 017, Loss: 16.9769\n",
      "Epoch: 018, Loss: 17.0051\n",
      "Epoch: 019, Loss: 16.8621\n",
      "Epoch: 020, Loss: 16.8035\n",
      "Epoch: 021, Loss: 16.7840\n",
      "Epoch: 022, Loss: 16.7544\n",
      "Epoch: 023, Loss: 16.6476\n",
      "Epoch: 024, Loss: 16.5748\n",
      "Epoch: 025, Loss: 16.4788\n",
      "Epoch: 026, Loss: 16.4462\n",
      "Epoch: 027, Loss: 16.3778\n",
      "Epoch: 028, Loss: 16.3072\n",
      "Epoch: 029, Loss: 16.2616\n",
      "Epoch: 030, Loss: 16.2379\n",
      "Epoch: 031, Loss: 16.2266\n",
      "Epoch: 032, Loss: 16.1877\n",
      "Epoch: 033, Loss: 16.1534\n",
      "Epoch: 034, Loss: 16.1237\n",
      "Epoch: 035, Loss: 16.1008\n",
      "Epoch: 036, Loss: 16.0890\n",
      "Epoch: 037, Loss: 16.0979\n",
      "Epoch: 038, Loss: 16.0795\n",
      "Epoch: 039, Loss: 16.0170\n",
      "Epoch: 040, Loss: 16.0332\n",
      "Epoch: 041, Loss: 15.9837\n",
      "Epoch: 042, Loss: 16.0409\n",
      "Epoch: 043, Loss: 16.0125\n",
      "Epoch: 044, Loss: 15.9066\n",
      "Epoch: 045, Loss: 15.9600\n",
      "Epoch: 046, Loss: 15.8832\n",
      "Epoch: 047, Loss: 15.8805\n",
      "Epoch: 048, Loss: 15.8454\n",
      "Epoch: 049, Loss: 15.8089\n",
      "Epoch: 050, Loss: 15.7917\n",
      "Epoch: 051, Loss: 15.7810\n",
      "Epoch: 052, Loss: 15.7030\n",
      "Epoch: 053, Loss: 15.7014\n",
      "Epoch: 054, Loss: 15.6859\n",
      "Epoch: 055, Loss: 15.6288\n",
      "Epoch: 056, Loss: 15.5544\n",
      "Epoch: 057, Loss: 15.5186\n",
      "Epoch: 058, Loss: 15.5137\n",
      "Epoch: 059, Loss: 15.5584\n",
      "Epoch: 060, Loss: 15.7761\n",
      "Epoch: 061, Loss: 15.6800\n",
      "Epoch: 062, Loss: 15.4990\n",
      "Epoch: 063, Loss: 15.4883\n",
      "Epoch: 064, Loss: 15.4629\n",
      "Epoch: 065, Loss: 15.5325\n",
      "Epoch: 066, Loss: 15.4359\n",
      "Epoch: 067, Loss: 15.4340\n",
      "Epoch: 068, Loss: 15.3743\n",
      "Epoch: 069, Loss: 15.3508\n",
      "Epoch: 070, Loss: 15.3230\n",
      "Epoch: 071, Loss: 15.2999\n",
      "Epoch: 072, Loss: 15.2724\n",
      "Epoch: 073, Loss: 15.2474\n",
      "Epoch: 074, Loss: 15.2234\n",
      "Epoch: 075, Loss: 15.2039\n",
      "Epoch: 076, Loss: 15.1545\n",
      "Epoch: 077, Loss: 15.1624\n",
      "Epoch: 078, Loss: 15.1284\n",
      "Epoch: 079, Loss: 15.1188\n",
      "Epoch: 080, Loss: 15.1003\n",
      "Epoch: 081, Loss: 15.0855\n",
      "Epoch: 082, Loss: 15.0688\n",
      "Epoch: 083, Loss: 15.0793\n",
      "Epoch: 084, Loss: 15.1125\n",
      "Epoch: 085, Loss: 15.2150\n",
      "Epoch: 086, Loss: 15.3868\n",
      "Epoch: 087, Loss: 15.2869\n",
      "Epoch: 088, Loss: 15.4638\n",
      "Epoch: 089, Loss: 15.2330\n",
      "Epoch: 090, Loss: 15.3620\n",
      "Epoch: 091, Loss: 15.3918\n",
      "Epoch: 092, Loss: 15.4375\n",
      "Epoch: 093, Loss: 15.2550\n",
      "Epoch: 094, Loss: 15.2062\n",
      "Epoch: 095, Loss: 15.1848\n",
      "Epoch: 096, Loss: 15.2044\n",
      "Epoch: 097, Loss: 15.1185\n",
      "Epoch: 098, Loss: 15.1310\n",
      "Epoch: 099, Loss: 15.1047\n",
      "Epoch: 100, Loss: 15.0667\n",
      "Epoch: 101, Loss: 15.0546\n",
      "Epoch: 102, Loss: 15.0525\n",
      "Epoch: 103, Loss: 15.0258\n",
      "Epoch: 104, Loss: 14.9918\n",
      "Epoch: 105, Loss: 14.9905\n",
      "Epoch: 106, Loss: 14.9729\n",
      "Epoch: 107, Loss: 14.9369\n",
      "Epoch: 108, Loss: 14.9439\n",
      "Epoch: 109, Loss: 14.9077\n",
      "Epoch: 110, Loss: 14.9084\n",
      "Epoch: 111, Loss: 14.8828\n",
      "Epoch: 112, Loss: 14.8830\n",
      "Epoch: 113, Loss: 14.8616\n",
      "Epoch: 114, Loss: 14.8491\n",
      "Epoch: 115, Loss: 14.8442\n",
      "Epoch: 116, Loss: 14.8404\n",
      "Epoch: 117, Loss: 14.8413\n",
      "Epoch: 118, Loss: 14.8673\n",
      "Epoch: 119, Loss: 14.8379\n",
      "Epoch: 120, Loss: 14.8106\n",
      "Epoch: 121, Loss: 14.7708\n",
      "Epoch: 122, Loss: 14.8274\n",
      "Epoch: 123, Loss: 14.8189\n",
      "Epoch: 124, Loss: 14.7814\n",
      "Epoch: 125, Loss: 14.7593\n",
      "Epoch: 126, Loss: 14.7603\n",
      "Epoch: 127, Loss: 14.7429\n",
      "Epoch: 128, Loss: 14.7487\n",
      "Epoch: 129, Loss: 14.7135\n",
      "Epoch: 130, Loss: 14.7082\n",
      "Epoch: 131, Loss: 14.7035\n",
      "Epoch: 132, Loss: 14.7076\n",
      "Epoch: 133, Loss: 14.7617\n",
      "Epoch: 134, Loss: 14.7994\n",
      "Epoch: 135, Loss: 14.8048\n",
      "Epoch: 136, Loss: 14.8159\n",
      "Epoch: 137, Loss: 14.7640\n",
      "Epoch: 138, Loss: 14.7406\n",
      "Epoch: 139, Loss: 14.7275\n",
      "Epoch: 140, Loss: 14.6989\n",
      "Epoch: 141, Loss: 14.6907\n",
      "Epoch: 142, Loss: 14.6914\n",
      "Epoch: 143, Loss: 14.6723\n",
      "Epoch: 144, Loss: 14.6687\n",
      "Epoch: 145, Loss: 14.6447\n",
      "Epoch: 146, Loss: 14.6340\n",
      "Epoch: 147, Loss: 14.6192\n",
      "Epoch: 148, Loss: 14.6183\n",
      "Epoch: 149, Loss: 14.6082\n",
      "Epoch: 150, Loss: 14.6198\n",
      "Epoch: 151, Loss: 14.6494\n",
      "Epoch: 152, Loss: 14.6723\n",
      "Epoch: 153, Loss: 14.6818\n",
      "Epoch: 154, Loss: 14.7741\n",
      "Epoch: 155, Loss: 14.7908\n",
      "Epoch: 156, Loss: 14.6627\n",
      "Epoch: 157, Loss: 14.7008\n",
      "Epoch: 158, Loss: 14.6707\n",
      "Epoch: 159, Loss: 14.6404\n",
      "Epoch: 160, Loss: 14.7135\n",
      "Epoch: 161, Loss: 14.6311\n",
      "Epoch: 162, Loss: 14.6686\n",
      "Epoch: 163, Loss: 14.6216\n",
      "Epoch: 164, Loss: 14.6388\n",
      "Epoch: 165, Loss: 14.5858\n",
      "Epoch: 166, Loss: 14.6032\n",
      "Epoch: 167, Loss: 14.5558\n",
      "Epoch: 168, Loss: 14.5761\n",
      "Epoch: 169, Loss: 14.5433\n",
      "Epoch: 170, Loss: 14.5390\n",
      "Epoch: 171, Loss: 14.5329\n",
      "Epoch: 172, Loss: 14.5288\n",
      "Epoch: 173, Loss: 14.5662\n",
      "Epoch: 174, Loss: 14.5784\n",
      "Epoch: 175, Loss: 14.5832\n",
      "Epoch: 176, Loss: 14.6051\n",
      "Epoch: 177, Loss: 14.6097\n",
      "Epoch: 178, Loss: 14.5843\n",
      "Epoch: 179, Loss: 14.5223\n",
      "Epoch: 180, Loss: 14.5530\n",
      "Epoch: 181, Loss: 14.5499\n",
      "Epoch: 182, Loss: 14.5047\n",
      "Epoch: 183, Loss: 14.5659\n",
      "Epoch: 184, Loss: 14.4994\n",
      "Epoch: 185, Loss: 14.4986\n",
      "Epoch: 186, Loss: 14.5241\n",
      "Epoch: 187, Loss: 14.4706\n",
      "Epoch: 188, Loss: 14.4937\n",
      "Epoch: 189, Loss: 14.4573\n",
      "Epoch: 190, Loss: 14.4979\n",
      "Epoch: 191, Loss: 14.4820\n",
      "Epoch: 192, Loss: 14.4426\n",
      "Epoch: 193, Loss: 14.4532\n",
      "Epoch: 194, Loss: 14.4232\n",
      "Epoch: 195, Loss: 14.4250\n",
      "Epoch: 196, Loss: 14.4067\n",
      "Epoch: 197, Loss: 14.3989\n",
      "Epoch: 198, Loss: 14.3859\n",
      "Epoch: 199, Loss: 14.3843\n",
      "Epoch: 200, Loss: 14.3789\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 201):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9180\n",
      "Training Accuracy: 0.8765\n"
     ]
    }
   ],
   "source": [
    "training_acc = test(torch_train_x, torch_train_y)\n",
    "print(f'Training Accuracy: {training_acc:.4f}')\n",
    "\n",
    "testing_acc = test(torch_test_x, torch_test_y)\n",
    "print(f'Training Accuracy: {testing_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Here's how you would parse the data for GNNs if edge features can be added\n",
    "################################################################################\n",
    "\n",
    "# node_prefix = [\"T1\", \"T2\", \"readout_error\"]\n",
    "# node_features_x = []\n",
    "# for k in range(df_train_x.shape[0]):\n",
    "#     node_features_j = []\n",
    "#     for j in range(num_qubits):\n",
    "#         row_features = df_train_x.iloc[k][[i + \"_\" + str(j) for i in node_prefix]].values.flatten()\n",
    "#         node_features_j.append(row_features)\n",
    "#     node_features_j = scaler.fit_transform(node_features_j)\n",
    "#     node_features_x.append(node_features_j)\n",
    "# node_features_x = np.array(node_features_x)\n",
    "\n",
    "# edge_prefix = [\"cx_\", \"edge_length_\", \"edge_error_\"]\n",
    "# edge_index = [[],[]]\n",
    "# edge_features_x = []\n",
    "# for k in range(df_train_x.shape[0]):\n",
    "#     edge_features_j = []\n",
    "#     for i in range(num_qubits):\n",
    "#         for j in range(num_qubits):\n",
    "#             if set([i,j]) in links:\n",
    "#                 row_features = df_train_x.iloc[k][[prefix + str(i) + str(j) for prefix in edge_prefix]].values.flatten()\n",
    "#                 edge_features_j.append(row_features)\n",
    "#                 if k == 0: # only need to do this once\n",
    "#                     edge_index[0].append(i)\n",
    "#                     edge_index[1].append(j)\n",
    "#     edge_features_j = scaler.fit_transform(edge_features_j)\n",
    "#     edge_features_x.append(edge_features_j)\n",
    "# edge_features_x = np.array(edge_features_x)\n",
    "# edge_index = np.array(edge_index)\n",
    "\n",
    "# node_labels = df_train_y.to_numpy()\n",
    "# print(node_features_x.shape, edge_index.shape, edge_features_x.shape, node_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7026, 7, 12) (2, 12) (7026, 49)\n",
      "(1757, 7, 12) (2, 12) (1757, 49)\n"
     ]
    }
   ],
   "source": [
    "# Since GCN doesn't support edge features, we can just concatenate them to the node features\n",
    "def get_datasets(df_x, df_y):\n",
    "    node_prefix = [\"T1\", \"T2\", \"readout_error\"]\n",
    "    edge_prefix = [\"cx\", \"edge_length\", \"edge_error\"]\n",
    "    node_features_x = []\n",
    "    edge_index = [[],[]]\n",
    "    for k in range(df_x.shape[0]):\n",
    "        node_features_i = []\n",
    "        for i in range(num_qubits):\n",
    "            node_features_i.append(list(df_train_x.iloc[k][[l + \"_\" + str(i) for l in node_prefix]].values.flatten()))\n",
    "            for j in range(num_qubits):\n",
    "                if set([i,j]) in links:\n",
    "                    node_features_i[i].extend(df_train_x.iloc[k][[l + \"_\" + str(i)+str(j) for l in edge_prefix]].values.flatten())\n",
    "                    if(k == 0): # only do this once\n",
    "                        edge_index[0].append(i)\n",
    "                        edge_index[1].append(j)\n",
    "            if(len(node_features_i[i]) < 12): # pad features to 12 with standard normal\n",
    "                node_features_i[i].extend(np.random.randn(12-len(node_features_i[i])))\n",
    "        node_features_i = scaler.fit_transform(node_features_i)\n",
    "        node_features_x.append(node_features_i)\n",
    "    node_features_x = np.array(node_features_x)\n",
    "    edge_index = np.array(edge_index)\n",
    "\n",
    "    node_labels = []\n",
    "    for _, row in df_y.iterrows():\n",
    "        node_labels.append(pd.get_dummies(row).values.flatten())\n",
    "    node_labels= np.array(node_labels)\n",
    "\n",
    "    return node_features_x, edge_index, node_labels\n",
    "\n",
    "train_gnn_x, edge_index, train_gnn_y = get_datasets(df_train_x, df_train_y)\n",
    "test_gnn_x, edge_index, test_gnn_y = get_datasets(df_test_x, df_test_y)\n",
    "print(train_gnn_x.shape, edge_index.shape, train_gnn_y.shape)\n",
    "print(test_gnn_x.shape, edge_index.shape, test_gnn_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(12, 12)\n",
      "  (conv2): GCNConv(12, 12)\n",
      "  (classifier): Linear(in_features=84, out_features=49, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(12, 12)\n",
    "        self.conv2 = GCNConv(12, 12)\n",
    "        self.classifier = Linear(84, 49)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = self.conv1(x, edge_index)\n",
    "        h = h.tanh()\n",
    "        h = self.conv2(h, edge_index)\n",
    "        h = h.tanh()\n",
    "        \n",
    "        # Apply a final (linear) classifier.\n",
    "        out = self.classifier(h.flatten(start_dim=1))\n",
    "\n",
    "        return out, h\n",
    "\n",
    "model = GCN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_train_gnn_x = torch.tensor(train_gnn_x, dtype=torch.float)\n",
    "torch_test_gnn_x = torch.tensor(test_gnn_x, dtype=torch.float)\n",
    "torch_edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "# torch_edge_features_x = torch.tensor(edge_features_x, dtype=torch.float)\n",
    "torch_train_gnn_y = torch.tensor(train_gnn_y, dtype=torch.float)\n",
    "torch_test_gnn_y = torch.tensor(test_gnn_y, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 27.120956420898438\n",
      "Epoch: 1, Loss: 19.671884536743164\n",
      "Epoch: 2, Loss: 19.88656234741211\n",
      "Epoch: 3, Loss: 21.882003784179688\n",
      "Epoch: 4, Loss: 20.162994384765625\n",
      "Epoch: 5, Loss: 18.861722946166992\n",
      "Epoch: 6, Loss: 20.08336067199707\n",
      "Epoch: 7, Loss: 17.643638610839844\n",
      "Epoch: 8, Loss: 18.10959243774414\n",
      "Epoch: 9, Loss: 18.14080810546875\n",
      "Epoch: 10, Loss: 18.220672607421875\n",
      "Epoch: 11, Loss: 17.85374641418457\n",
      "Epoch: 12, Loss: 17.284770965576172\n",
      "Epoch: 13, Loss: 17.12647819519043\n",
      "Epoch: 14, Loss: 17.210222244262695\n",
      "Epoch: 15, Loss: 17.32783317565918\n",
      "Epoch: 16, Loss: 17.162330627441406\n",
      "Epoch: 17, Loss: 16.979442596435547\n",
      "Epoch: 18, Loss: 16.803050994873047\n",
      "Epoch: 19, Loss: 16.712888717651367\n",
      "Epoch: 20, Loss: 16.772111892700195\n",
      "Epoch: 21, Loss: 16.7814884185791\n",
      "Epoch: 22, Loss: 16.679656982421875\n",
      "Epoch: 23, Loss: 16.596113204956055\n",
      "Epoch: 24, Loss: 16.55254554748535\n",
      "Epoch: 25, Loss: 16.507272720336914\n",
      "Epoch: 26, Loss: 16.532445907592773\n",
      "Epoch: 27, Loss: 16.5472469329834\n",
      "Epoch: 28, Loss: 16.505443572998047\n",
      "Epoch: 29, Loss: 16.425945281982422\n",
      "Epoch: 30, Loss: 16.379499435424805\n",
      "Epoch: 31, Loss: 16.35335922241211\n",
      "Epoch: 32, Loss: 16.37755012512207\n",
      "Epoch: 33, Loss: 16.366344451904297\n",
      "Epoch: 34, Loss: 16.333927154541016\n",
      "Epoch: 35, Loss: 16.284883499145508\n",
      "Epoch: 36, Loss: 16.26996612548828\n",
      "Epoch: 37, Loss: 16.256534576416016\n",
      "Epoch: 38, Loss: 16.25695037841797\n",
      "Epoch: 39, Loss: 16.23912811279297\n",
      "Epoch: 40, Loss: 16.215984344482422\n",
      "Epoch: 41, Loss: 16.193138122558594\n",
      "Epoch: 42, Loss: 16.1892147064209\n",
      "Epoch: 43, Loss: 16.174884796142578\n",
      "Epoch: 44, Loss: 16.163803100585938\n",
      "Epoch: 45, Loss: 16.14984703063965\n",
      "Epoch: 46, Loss: 16.137216567993164\n",
      "Epoch: 47, Loss: 16.123144149780273\n",
      "Epoch: 48, Loss: 16.11891746520996\n",
      "Epoch: 49, Loss: 16.112340927124023\n",
      "Epoch: 50, Loss: 16.100862503051758\n",
      "Epoch: 51, Loss: 16.090511322021484\n",
      "Epoch: 52, Loss: 16.080026626586914\n",
      "Epoch: 53, Loss: 16.074262619018555\n",
      "Epoch: 54, Loss: 16.066741943359375\n",
      "Epoch: 55, Loss: 16.059432983398438\n",
      "Epoch: 56, Loss: 16.051830291748047\n",
      "Epoch: 57, Loss: 16.042394638061523\n",
      "Epoch: 58, Loss: 16.038000106811523\n",
      "Epoch: 59, Loss: 16.033405303955078\n",
      "Epoch: 60, Loss: 16.024627685546875\n",
      "Epoch: 61, Loss: 16.018356323242188\n",
      "Epoch: 62, Loss: 16.012659072875977\n",
      "Epoch: 63, Loss: 16.00699806213379\n",
      "Epoch: 64, Loss: 16.00347328186035\n",
      "Epoch: 65, Loss: 15.997173309326172\n",
      "Epoch: 66, Loss: 15.99112606048584\n",
      "Epoch: 67, Loss: 15.987203598022461\n",
      "Epoch: 68, Loss: 15.982635498046875\n",
      "Epoch: 69, Loss: 15.977594375610352\n",
      "Epoch: 70, Loss: 15.972430229187012\n",
      "Epoch: 71, Loss: 15.967799186706543\n",
      "Epoch: 72, Loss: 15.963223457336426\n",
      "Epoch: 73, Loss: 15.958979606628418\n",
      "Epoch: 74, Loss: 15.954516410827637\n",
      "Epoch: 75, Loss: 15.950335502624512\n",
      "Epoch: 76, Loss: 15.946087837219238\n",
      "Epoch: 77, Loss: 15.941779136657715\n",
      "Epoch: 78, Loss: 15.937844276428223\n",
      "Epoch: 79, Loss: 15.93386173248291\n",
      "Epoch: 80, Loss: 15.930130958557129\n",
      "Epoch: 81, Loss: 15.926107406616211\n",
      "Epoch: 82, Loss: 15.922443389892578\n",
      "Epoch: 83, Loss: 15.918679237365723\n",
      "Epoch: 84, Loss: 15.915241241455078\n",
      "Epoch: 85, Loss: 15.911624908447266\n",
      "Epoch: 86, Loss: 15.90813159942627\n",
      "Epoch: 87, Loss: 15.904533386230469\n",
      "Epoch: 88, Loss: 15.901080131530762\n",
      "Epoch: 89, Loss: 15.89776611328125\n",
      "Epoch: 90, Loss: 15.894430160522461\n",
      "Epoch: 91, Loss: 15.8910493850708\n",
      "Epoch: 92, Loss: 15.887789726257324\n",
      "Epoch: 93, Loss: 15.884576797485352\n",
      "Epoch: 94, Loss: 15.88138484954834\n",
      "Epoch: 95, Loss: 15.878169059753418\n",
      "Epoch: 96, Loss: 15.875199317932129\n",
      "Epoch: 97, Loss: 15.872381210327148\n",
      "Epoch: 98, Loss: 15.870171546936035\n",
      "Epoch: 99, Loss: 15.869558334350586\n",
      "Epoch: 100, Loss: 15.874735832214355\n",
      "Epoch: 101, Loss: 15.892827987670898\n",
      "Epoch: 102, Loss: 15.961270332336426\n",
      "Epoch: 103, Loss: 16.008955001831055\n",
      "Epoch: 104, Loss: 16.08359146118164\n",
      "Epoch: 105, Loss: 15.875139236450195\n",
      "Epoch: 106, Loss: 15.92946720123291\n",
      "Epoch: 107, Loss: 16.045801162719727\n",
      "Epoch: 108, Loss: 15.867402076721191\n",
      "Epoch: 109, Loss: 16.01642608642578\n",
      "Epoch: 110, Loss: 15.943431854248047\n",
      "Epoch: 111, Loss: 15.932024002075195\n",
      "Epoch: 112, Loss: 15.907161712646484\n",
      "Epoch: 113, Loss: 15.893701553344727\n",
      "Epoch: 114, Loss: 15.894227027893066\n",
      "Epoch: 115, Loss: 15.896227836608887\n",
      "Epoch: 116, Loss: 15.857484817504883\n",
      "Epoch: 117, Loss: 15.88734245300293\n",
      "Epoch: 118, Loss: 15.846622467041016\n",
      "Epoch: 119, Loss: 15.878453254699707\n",
      "Epoch: 120, Loss: 15.830753326416016\n",
      "Epoch: 121, Loss: 15.867793083190918\n",
      "Epoch: 122, Loss: 15.82614803314209\n",
      "Epoch: 123, Loss: 15.848760604858398\n",
      "Epoch: 124, Loss: 15.823080062866211\n",
      "Epoch: 125, Loss: 15.83504867553711\n",
      "Epoch: 126, Loss: 15.815757751464844\n",
      "Epoch: 127, Loss: 15.82253646850586\n",
      "Epoch: 128, Loss: 15.809377670288086\n",
      "Epoch: 129, Loss: 15.812504768371582\n",
      "Epoch: 130, Loss: 15.801023483276367\n",
      "Epoch: 131, Loss: 15.804118156433105\n",
      "Epoch: 132, Loss: 15.792917251586914\n",
      "Epoch: 133, Loss: 15.796700477600098\n",
      "Epoch: 134, Loss: 15.7860689163208\n",
      "Epoch: 135, Loss: 15.788596153259277\n",
      "Epoch: 136, Loss: 15.780171394348145\n",
      "Epoch: 137, Loss: 15.779815673828125\n",
      "Epoch: 138, Loss: 15.776461601257324\n",
      "Epoch: 139, Loss: 15.771286964416504\n",
      "Epoch: 140, Loss: 15.772298812866211\n",
      "Epoch: 141, Loss: 15.764270782470703\n",
      "Epoch: 142, Loss: 15.765625953674316\n",
      "Epoch: 143, Loss: 15.76071548461914\n",
      "Epoch: 144, Loss: 15.757610321044922\n",
      "Epoch: 145, Loss: 15.756775856018066\n",
      "Epoch: 146, Loss: 15.752420425415039\n",
      "Epoch: 147, Loss: 15.749944686889648\n",
      "Epoch: 148, Loss: 15.74927043914795\n",
      "Epoch: 149, Loss: 15.744500160217285\n",
      "Epoch: 150, Loss: 15.743125915527344\n",
      "Epoch: 151, Loss: 15.741511344909668\n",
      "Epoch: 152, Loss: 15.737977981567383\n",
      "Epoch: 153, Loss: 15.735445022583008\n",
      "Epoch: 154, Loss: 15.734692573547363\n",
      "Epoch: 155, Loss: 15.731368064880371\n",
      "Epoch: 156, Loss: 15.72903060913086\n",
      "Epoch: 157, Loss: 15.727259635925293\n",
      "Epoch: 158, Loss: 15.725791931152344\n",
      "Epoch: 159, Loss: 15.722772598266602\n",
      "Epoch: 160, Loss: 15.720691680908203\n",
      "Epoch: 161, Loss: 15.719109535217285\n",
      "Epoch: 162, Loss: 15.717476844787598\n",
      "Epoch: 163, Loss: 15.715036392211914\n",
      "Epoch: 164, Loss: 15.712806701660156\n",
      "Epoch: 165, Loss: 15.71114444732666\n",
      "Epoch: 166, Loss: 15.709546089172363\n",
      "Epoch: 167, Loss: 15.707911491394043\n",
      "Epoch: 168, Loss: 15.705777168273926\n",
      "Epoch: 169, Loss: 15.703777313232422\n",
      "Epoch: 170, Loss: 15.701858520507812\n",
      "Epoch: 171, Loss: 15.700318336486816\n",
      "Epoch: 172, Loss: 15.698758125305176\n",
      "Epoch: 173, Loss: 15.697277069091797\n",
      "Epoch: 174, Loss: 15.695720672607422\n",
      "Epoch: 175, Loss: 15.694108009338379\n",
      "Epoch: 176, Loss: 15.692553520202637\n",
      "Epoch: 177, Loss: 15.690962791442871\n",
      "Epoch: 178, Loss: 15.689620018005371\n",
      "Epoch: 179, Loss: 15.688324928283691\n",
      "Epoch: 180, Loss: 15.687661170959473\n",
      "Epoch: 181, Loss: 15.68767261505127\n",
      "Epoch: 182, Loss: 15.690195083618164\n",
      "Epoch: 183, Loss: 15.69601058959961\n",
      "Epoch: 184, Loss: 15.715469360351562\n",
      "Epoch: 185, Loss: 15.740935325622559\n",
      "Epoch: 186, Loss: 15.813544273376465\n",
      "Epoch: 187, Loss: 15.78266429901123\n",
      "Epoch: 188, Loss: 15.750247955322266\n",
      "Epoch: 189, Loss: 15.680109977722168\n",
      "Epoch: 190, Loss: 15.704490661621094\n",
      "Epoch: 191, Loss: 15.756591796875\n",
      "Epoch: 192, Loss: 15.693490028381348\n",
      "Epoch: 193, Loss: 15.686988830566406\n",
      "Epoch: 194, Loss: 15.727883338928223\n",
      "Epoch: 195, Loss: 15.68781566619873\n",
      "Epoch: 196, Loss: 15.677069664001465\n",
      "Epoch: 197, Loss: 15.706761360168457\n",
      "Epoch: 198, Loss: 15.680127143859863\n",
      "Epoch: 199, Loss: 15.669721603393555\n",
      "Epoch: 200, Loss: 15.688901901245117\n",
      "Epoch: 201, Loss: 15.676451683044434\n",
      "Epoch: 202, Loss: 15.661771774291992\n",
      "Epoch: 203, Loss: 15.673623085021973\n",
      "Epoch: 204, Loss: 15.671117782592773\n",
      "Epoch: 205, Loss: 15.657228469848633\n",
      "Epoch: 206, Loss: 15.660783767700195\n",
      "Epoch: 207, Loss: 15.66379451751709\n",
      "Epoch: 208, Loss: 15.657150268554688\n",
      "Epoch: 209, Loss: 15.652388572692871\n",
      "Epoch: 210, Loss: 15.654533386230469\n",
      "Epoch: 211, Loss: 15.656819343566895\n",
      "Epoch: 212, Loss: 15.651237487792969\n",
      "Epoch: 213, Loss: 15.645467758178711\n",
      "Epoch: 214, Loss: 15.647217750549316\n",
      "Epoch: 215, Loss: 15.649665832519531\n",
      "Epoch: 216, Loss: 15.646629333496094\n",
      "Epoch: 217, Loss: 15.64263916015625\n",
      "Epoch: 218, Loss: 15.640900611877441\n",
      "Epoch: 219, Loss: 15.639927864074707\n",
      "Epoch: 220, Loss: 15.640393257141113\n",
      "Epoch: 221, Loss: 15.639711380004883\n",
      "Epoch: 222, Loss: 15.637008666992188\n",
      "Epoch: 223, Loss: 15.634967803955078\n",
      "Epoch: 224, Loss: 15.63395881652832\n",
      "Epoch: 225, Loss: 15.63290023803711\n",
      "Epoch: 226, Loss: 15.633169174194336\n",
      "Epoch: 227, Loss: 15.636008262634277\n",
      "Epoch: 228, Loss: 15.645748138427734\n",
      "Epoch: 229, Loss: 15.661930084228516\n",
      "Epoch: 230, Loss: 15.710533142089844\n",
      "Epoch: 231, Loss: 15.693843841552734\n",
      "Epoch: 232, Loss: 15.665292739868164\n",
      "Epoch: 233, Loss: 15.660698890686035\n",
      "Epoch: 234, Loss: 15.663788795471191\n",
      "Epoch: 235, Loss: 15.67465591430664\n",
      "Epoch: 236, Loss: 15.668322563171387\n",
      "Epoch: 237, Loss: 15.665163040161133\n",
      "Epoch: 238, Loss: 15.647600173950195\n",
      "Epoch: 239, Loss: 15.631613731384277\n",
      "Epoch: 240, Loss: 15.633540153503418\n",
      "Epoch: 241, Loss: 15.633622169494629\n",
      "Epoch: 242, Loss: 15.627076148986816\n",
      "Epoch: 243, Loss: 15.63174057006836\n",
      "Epoch: 244, Loss: 15.633563995361328\n",
      "Epoch: 245, Loss: 15.620881080627441\n",
      "Epoch: 246, Loss: 15.616406440734863\n",
      "Epoch: 247, Loss: 15.62091064453125\n",
      "Epoch: 248, Loss: 15.615521430969238\n",
      "Epoch: 249, Loss: 15.610638618469238\n",
      "Epoch: 250, Loss: 15.61994743347168\n",
      "Epoch: 251, Loss: 15.6218843460083\n",
      "Epoch: 252, Loss: 15.612573623657227\n",
      "Epoch: 253, Loss: 15.616958618164062\n",
      "Epoch: 254, Loss: 15.620474815368652\n",
      "Epoch: 255, Loss: 15.612445831298828\n",
      "Epoch: 256, Loss: 15.61325454711914\n",
      "Epoch: 257, Loss: 15.616951942443848\n",
      "Epoch: 258, Loss: 15.615426063537598\n",
      "Epoch: 259, Loss: 15.615826606750488\n",
      "Epoch: 260, Loss: 15.620667457580566\n",
      "Epoch: 261, Loss: 15.626287460327148\n",
      "Epoch: 262, Loss: 15.638166427612305\n",
      "Epoch: 263, Loss: 15.636055946350098\n",
      "Epoch: 264, Loss: 15.64648723602295\n",
      "Epoch: 265, Loss: 15.63388729095459\n",
      "Epoch: 266, Loss: 15.614084243774414\n",
      "Epoch: 267, Loss: 15.595651626586914\n",
      "Epoch: 268, Loss: 15.593335151672363\n",
      "Epoch: 269, Loss: 15.599620819091797\n",
      "Epoch: 270, Loss: 15.605192184448242\n",
      "Epoch: 271, Loss: 15.608171463012695\n",
      "Epoch: 272, Loss: 15.599119186401367\n",
      "Epoch: 273, Loss: 15.590665817260742\n",
      "Epoch: 274, Loss: 15.585529327392578\n",
      "Epoch: 275, Loss: 15.58565616607666\n",
      "Epoch: 276, Loss: 15.589852333068848\n",
      "Epoch: 277, Loss: 15.594647407531738\n",
      "Epoch: 278, Loss: 15.597070693969727\n",
      "Epoch: 279, Loss: 15.593825340270996\n",
      "Epoch: 280, Loss: 15.590837478637695\n",
      "Epoch: 281, Loss: 15.586328506469727\n",
      "Epoch: 282, Loss: 15.582145690917969\n",
      "Epoch: 283, Loss: 15.57820987701416\n",
      "Epoch: 284, Loss: 15.575603485107422\n",
      "Epoch: 285, Loss: 15.57419204711914\n",
      "Epoch: 286, Loss: 15.574732780456543\n",
      "Epoch: 287, Loss: 15.577280044555664\n",
      "Epoch: 288, Loss: 15.586400985717773\n",
      "Epoch: 289, Loss: 15.595998764038086\n",
      "Epoch: 290, Loss: 15.626302719116211\n",
      "Epoch: 291, Loss: 15.614654541015625\n",
      "Epoch: 292, Loss: 15.620002746582031\n",
      "Epoch: 293, Loss: 15.653563499450684\n",
      "Epoch: 294, Loss: 15.699605941772461\n",
      "Epoch: 295, Loss: 15.801033020019531\n",
      "Epoch: 296, Loss: 15.694087028503418\n",
      "Epoch: 297, Loss: 15.587240219116211\n",
      "Epoch: 298, Loss: 15.60704231262207\n",
      "Epoch: 299, Loss: 15.653029441833496\n",
      "Epoch: 300, Loss: 15.614542961120605\n",
      "Epoch: 301, Loss: 15.584025382995605\n",
      "Epoch: 302, Loss: 15.63443374633789\n",
      "Epoch: 303, Loss: 15.629256248474121\n",
      "Epoch: 304, Loss: 15.575536727905273\n",
      "Epoch: 305, Loss: 15.626317977905273\n",
      "Epoch: 306, Loss: 15.643754005432129\n",
      "Epoch: 307, Loss: 15.573808670043945\n",
      "Epoch: 308, Loss: 15.614677429199219\n",
      "Epoch: 309, Loss: 15.646615982055664\n",
      "Epoch: 310, Loss: 15.579859733581543\n",
      "Epoch: 311, Loss: 15.603926658630371\n",
      "Epoch: 312, Loss: 15.646509170532227\n",
      "Epoch: 313, Loss: 15.590429306030273\n",
      "Epoch: 314, Loss: 15.593796730041504\n",
      "Epoch: 315, Loss: 15.647562026977539\n",
      "Epoch: 316, Loss: 15.592290878295898\n",
      "Epoch: 317, Loss: 15.58997917175293\n",
      "Epoch: 318, Loss: 15.626542091369629\n",
      "Epoch: 319, Loss: 15.593844413757324\n",
      "Epoch: 320, Loss: 15.581985473632812\n",
      "Epoch: 321, Loss: 15.587188720703125\n",
      "Epoch: 322, Loss: 15.61676025390625\n",
      "Epoch: 323, Loss: 15.564621925354004\n",
      "Epoch: 324, Loss: 15.577198028564453\n",
      "Epoch: 325, Loss: 15.61486530303955\n",
      "Epoch: 326, Loss: 15.553030967712402\n",
      "Epoch: 327, Loss: 15.585556983947754\n",
      "Epoch: 328, Loss: 15.591707229614258\n",
      "Epoch: 329, Loss: 15.560007095336914\n",
      "Epoch: 330, Loss: 15.59937858581543\n",
      "Epoch: 331, Loss: 15.589729309082031\n",
      "Epoch: 332, Loss: 15.566457748413086\n",
      "Epoch: 333, Loss: 15.610699653625488\n",
      "Epoch: 334, Loss: 15.590251922607422\n",
      "Epoch: 335, Loss: 15.580375671386719\n",
      "Epoch: 336, Loss: 15.596294403076172\n",
      "Epoch: 337, Loss: 15.565621376037598\n",
      "Epoch: 338, Loss: 15.573678016662598\n",
      "Epoch: 339, Loss: 15.557380676269531\n",
      "Epoch: 340, Loss: 15.555644989013672\n",
      "Epoch: 341, Loss: 15.545948028564453\n",
      "Epoch: 342, Loss: 15.553802490234375\n",
      "Epoch: 343, Loss: 15.545613288879395\n",
      "Epoch: 344, Loss: 15.544837951660156\n",
      "Epoch: 345, Loss: 15.553046226501465\n",
      "Epoch: 346, Loss: 15.529027938842773\n",
      "Epoch: 347, Loss: 15.545631408691406\n",
      "Epoch: 348, Loss: 15.53268814086914\n",
      "Epoch: 349, Loss: 15.529023170471191\n",
      "Epoch: 350, Loss: 15.538827896118164\n",
      "Epoch: 351, Loss: 15.527902603149414\n",
      "Epoch: 352, Loss: 15.534329414367676\n",
      "Epoch: 353, Loss: 15.528482437133789\n",
      "Epoch: 354, Loss: 15.531071662902832\n",
      "Epoch: 355, Loss: 15.532393455505371\n",
      "Epoch: 356, Loss: 15.528655052185059\n",
      "Epoch: 357, Loss: 15.542207717895508\n",
      "Epoch: 358, Loss: 15.557465553283691\n",
      "Epoch: 359, Loss: 15.598119735717773\n",
      "Epoch: 360, Loss: 15.630094528198242\n",
      "Epoch: 361, Loss: 15.707256317138672\n",
      "Epoch: 362, Loss: 15.612231254577637\n",
      "Epoch: 363, Loss: 15.528528213500977\n",
      "Epoch: 364, Loss: 15.56981086730957\n",
      "Epoch: 365, Loss: 15.608316421508789\n",
      "Epoch: 366, Loss: 15.53703498840332\n",
      "Epoch: 367, Loss: 15.5438814163208\n",
      "Epoch: 368, Loss: 15.58956241607666\n",
      "Epoch: 369, Loss: 15.556954383850098\n",
      "Epoch: 370, Loss: 15.524491310119629\n",
      "Epoch: 371, Loss: 15.564531326293945\n",
      "Epoch: 372, Loss: 15.572582244873047\n",
      "Epoch: 373, Loss: 15.527533531188965\n",
      "Epoch: 374, Loss: 15.5393648147583\n",
      "Epoch: 375, Loss: 15.579516410827637\n",
      "Epoch: 376, Loss: 15.543415069580078\n",
      "Epoch: 377, Loss: 15.521580696105957\n",
      "Epoch: 378, Loss: 15.577807426452637\n",
      "Epoch: 379, Loss: 15.568297386169434\n",
      "Epoch: 380, Loss: 15.519554138183594\n",
      "Epoch: 381, Loss: 15.553028106689453\n",
      "Epoch: 382, Loss: 15.589255332946777\n",
      "Epoch: 383, Loss: 15.52836799621582\n",
      "Epoch: 384, Loss: 15.548959732055664\n",
      "Epoch: 385, Loss: 15.597387313842773\n",
      "Epoch: 386, Loss: 15.540594100952148\n",
      "Epoch: 387, Loss: 15.607598304748535\n",
      "Epoch: 388, Loss: 15.617060661315918\n",
      "Epoch: 389, Loss: 15.564896583557129\n",
      "Epoch: 390, Loss: 15.670654296875\n",
      "Epoch: 391, Loss: 15.624746322631836\n",
      "Epoch: 392, Loss: 15.623665809631348\n",
      "Epoch: 393, Loss: 15.665548324584961\n",
      "Epoch: 394, Loss: 15.548723220825195\n",
      "Epoch: 395, Loss: 15.6415376663208\n",
      "Epoch: 396, Loss: 15.544018745422363\n",
      "Epoch: 397, Loss: 15.575786590576172\n",
      "Epoch: 398, Loss: 15.570524215698242\n",
      "Epoch: 399, Loss: 15.574453353881836\n",
      "Epoch: 400, Loss: 15.552580833435059\n"
     ]
    }
   ],
   "source": [
    "model = GCN()\n",
    "criterion = torch.nn.CrossEntropyLoss()  #Initialize the CrossEntropyLoss function.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1, weight_decay=5e-4)  # Initialize the Adam optimizer.\n",
    "\n",
    "def train(node_features, edge_index, node_labels):\n",
    "\n",
    "    optimizer.zero_grad()  # Clear gradients.\n",
    "    out, h = model(node_features, edge_index)  # Perform a single forward pass.\n",
    "    loss = criterion(out, node_labels)  # Compute the loss solely based on the training nodes.\n",
    "    loss.backward()  # Derive gradients.\n",
    "    optimizer.step()  # Update parameters based on gradients.\n",
    "\n",
    "    return loss, h\n",
    "\n",
    "\n",
    "for epoch in range(401):\n",
    "    #torch_edge_weight = torch.tensor(np.sum(edge_features_x, axis=2), dtype=torch.float)\n",
    "    #torch_edge_weight = torch.tensor(np.random.randn(12,1), dtype=torch.float)\n",
    "    loss, h = train(torch_train_gnn_x, torch_edge_index, torch_train_gnn_y)\n",
    "    print(f'Epoch: {epoch}, Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(x, e, y):\n",
    "    pred, h = model(x, e)\n",
    "\n",
    "    test_correct = 0\n",
    "    for i, j in zip(pred, y):\n",
    "        pred_i  = np.argmax(i.detach().numpy().reshape(7,7), axis=1)\n",
    "        label_j = np.argmax(j.detach().numpy().reshape(7,7), axis=1)\n",
    "        test_correct += np.array_equal(pred_i, label_j)\n",
    "        test_acc = int(test_correct) / int(y.shape[0])  # Derive ratio of correct predictions.\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN Training =  0.7421007685738685 GNN Testing =  0.40922026180990323\n"
     ]
    }
   ],
   "source": [
    "print(\"GNN Training = \", test(torch_train_gnn_x, torch_edge_index, torch_train_gnn_y), \"GNN Testing = \", test(torch_test_gnn_x, torch_edge_index, torch_test_gnn_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical ML methods: RF-Gini and SVM-RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_svm_y(y):\n",
    "    svm_y = []\n",
    "    for i in y:\n",
    "        list_mapping = np.argmax(i.reshape(7,7), axis=1)\n",
    "        value = 0\n",
    "        for i in range(len(list_mapping)):\n",
    "            value += list_mapping[i]*num_qubits**i\n",
    "        svm_y.append(value)\n",
    "    return svm_y\n",
    "\n",
    "svm_train_y = get_svm_y(train_y)\n",
    "svm_test_y = get_svm_y(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM-RBF Training =  0.9611443210930828 SVM-RBF Testing =  0.7006260671599317\n",
      "SVM-RBF Training =  0.9987190435525192 SVM-RBF Testing =  0.7194080819578828\n",
      "SVM-RBF Training =  0.9991460290350128 SVM-RBF Testing =  0.7182697780307342\n",
      "SVM-RBF Training =  0.9991460290350128 SVM-RBF Testing =  0.7182697780307342\n",
      "SVM-RBF Training =  0.9991460290350128 SVM-RBF Testing =  0.7182697780307342\n",
      "SVM-RBF Training =  0.9991460290350128 SVM-RBF Testing =  0.7182697780307342\n",
      "SVM-RBF Training =  0.9991460290350128 SVM-RBF Testing =  0.7182697780307342\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "c_list = [1, 20, 40, 60, 80, 100, 500]\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "for c in c_list:\n",
    "    svc_model = SVC(kernel='rbf', C=c, gamma=1,decision_function_shape='ovo')\n",
    "    svc_model.fit(train_x, svm_train_y)\n",
    "    print(\"SVM-RBF Training = \", svc_model.score(train_x, svm_train_y), \"SVM-RBF Testing = \", svc_model.score(test_x, svm_test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF-Gini-2 Training =  0.6498719043552519 RF-Gini-2 Testing =  0.6465566306203756\n",
      "RF-Gini-2 Training =  0.6578423000284657 RF-Gini-2 Testing =  0.6528173022196927\n",
      "RF-Gini-2 Training =  0.6598348989467692 RF-Gini-2 Testing =  0.6573705179282868\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "n_estimators = [50, 500, 1000]\n",
    "for n_est in n_estimators:\n",
    "    clf = RandomForestClassifier(n_estimators=n_est, max_depth=2, random_state=0)\n",
    "    clf.fit(train_x, svm_train_y)\n",
    "    print(\"RF-Gini-2 Training = \", clf.score(train_x, svm_train_y), \"RF-Gini-2 Testing = \", clf.score(test_x, svm_test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF-Gini-10 Training =  0.9588670651864504 RF-Gini-10 Testing =  0.9248719408081958\n",
      "RF-Gini-10 Training =  0.9581554227156277 RF-Gini-10 Testing =  0.9237336368810473\n",
      "RF-Gini-10 Training =  0.9585824081981212 RF-Gini-10 Testing =  0.9237336368810473\n"
     ]
    }
   ],
   "source": [
    "n_estimators = [50, 500, 1000]\n",
    "for n_est in n_estimators:\n",
    "    clf = RandomForestClassifier(n_estimators=n_est, max_depth=10, random_state=0)\n",
    "    clf.fit(train_x, svm_train_y)\n",
    "    print(\"RF-Gini-10 Training = \", clf.score(train_x, svm_train_y), \"RF-Gini-10 Testing = \", clf.score(test_x, svm_test_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eece571f",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
