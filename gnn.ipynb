{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "num_qubits = 7\n",
    "dataset_filename = \"dataset/dataset_tesi/NN1_Dataset(<=10Cx)_balanced1.csv\"\n",
    "df = pd.read_csv(dataset_filename)\n",
    "\n",
    "links = [set([0,1]), set([1,2]), set([1,3]), set([3,5]), set([4,5]), set([5,6])]\n",
    "def generate_columns(header, links, in_links=False):\n",
    "    if in_links:\n",
    "        return [header+str(i)+str(j) for i in range(num_qubits) for j in range(num_qubits) if set([i,j]) in links]\n",
    "    else:\n",
    "        return [header+str(i)+str(j) for i in range(num_qubits) for j in range(num_qubits) if set([i,j]) not in links and i!=j]\n",
    "\n",
    "useless_columns = ['Unnamed: 0', 'last_update_date', 'N_qubtis', 'N_measure', 'N_cx', 'backend_name']\n",
    "useless_columns += generate_columns(\"cx_\", links)\n",
    "useless_columns += generate_columns(\"edge_length_\", links)\n",
    "useless_columns += generate_columns(\"edge_error_\", links)\n",
    "useless_columns += [\"measure_\"+str(i) for i in range(num_qubits)]\n",
    "# Note that cx/edge_error/edge_length_xy is not neccessarily the same as cx/edge_length/edge_error_yx\n",
    "df.drop(columns=useless_columns, inplace=True)\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7026, 57), (7026, 49), (1757, 57), (1757, 49))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "df_train_x, df_train_y= train.iloc[:, :-num_qubits], train.iloc[:, -num_qubits:]\n",
    "df_test_x, df_test_y= test.iloc[:, :-num_qubits], test.iloc[:, -num_qubits:]\n",
    "\n",
    "train_x = scaler.fit_transform(df_train_x)\n",
    "test_x = scaler.fit_transform(df_test_x)\n",
    "\n",
    "\n",
    "# for every row in y, convert to 1 hot-encoding and flatten\n",
    "train_y = []\n",
    "for _, row in df_train_y.iterrows():\n",
    "    train_y.append(pd.get_dummies(row).values.flatten())\n",
    "train_y = np.array(train_y)\n",
    "\n",
    "test_y = []\n",
    "for _, row in df_test_y.iterrows():\n",
    "    test_y.append(pd.get_dummies(row).values.flatten())\n",
    "test_y = np.array(test_y)\n",
    "\n",
    "(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacky/anaconda3/envs/eece571f/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.lin1 = Linear(num_features, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin3 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin4 = Linear(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = x.relu()\n",
    "        x = self.lin2(x)\n",
    "        x = x.relu()\n",
    "        x = self.lin3(x)\n",
    "        x = x.relu()\n",
    "        x = self.lin4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(57, 49, hidden_channels=128)\n",
    "criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)  # Define optimizer.\n",
    "\n",
    "torch_train_x = torch.tensor(train_x, dtype=torch.float)\n",
    "torch_train_y = torch.tensor(train_y, dtype=torch.float)\n",
    "torch_test_x = torch.tensor(test_x, dtype=torch.float)\n",
    "torch_test_y = torch.tensor(test_y, dtype=torch.float)\n",
    "\n",
    "def train():\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(torch_train_x)  # Perform a single forward pass.\n",
    "      loss = criterion(out, torch_train_y)  # Compute the loss solely based on the training nodes.\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test():\n",
    "      model.eval()\n",
    "      out = model(torch_train_x)\n",
    "      pred = out  # Use the class with highest probability.\n",
    "\n",
    "      test_correct = 0\n",
    "      for i, j in zip(pred, torch_train_y):\n",
    "          pred_i = np.argmax(i.detach().numpy().reshape(7,7), axis=1)\n",
    "          train_y = np.argmax(j.detach().numpy().reshape(7,7), axis=1)\n",
    "          test_correct += np.array_equal(pred_i, train_y)\n",
    "\n",
    "      test_acc = int(test_correct) / int(torch_train_y.shape[0])  # Derive ratio of correct predictions.\n",
    "      return test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 27.0803\n",
      "Epoch: 002, Loss: 25.3286\n",
      "Epoch: 003, Loss: 21.0375\n",
      "Epoch: 004, Loss: 18.8970\n",
      "Epoch: 005, Loss: 18.7950\n",
      "Epoch: 006, Loss: 18.8510\n",
      "Epoch: 007, Loss: 17.6396\n",
      "Epoch: 008, Loss: 17.4886\n",
      "Epoch: 009, Loss: 17.6235\n",
      "Epoch: 010, Loss: 17.5336\n",
      "Epoch: 011, Loss: 17.3202\n",
      "Epoch: 012, Loss: 17.0892\n",
      "Epoch: 013, Loss: 16.8502\n",
      "Epoch: 014, Loss: 16.7429\n",
      "Epoch: 015, Loss: 16.7733\n",
      "Epoch: 016, Loss: 16.7455\n",
      "Epoch: 017, Loss: 16.6404\n",
      "Epoch: 018, Loss: 16.5606\n",
      "Epoch: 019, Loss: 16.5409\n",
      "Epoch: 020, Loss: 16.5243\n",
      "Epoch: 021, Loss: 16.4662\n",
      "Epoch: 022, Loss: 16.3874\n",
      "Epoch: 023, Loss: 16.3353\n",
      "Epoch: 024, Loss: 16.3327\n",
      "Epoch: 025, Loss: 16.3433\n",
      "Epoch: 026, Loss: 16.3184\n",
      "Epoch: 027, Loss: 16.2742\n",
      "Epoch: 028, Loss: 16.2438\n",
      "Epoch: 029, Loss: 16.2258\n",
      "Epoch: 030, Loss: 16.2062\n",
      "Epoch: 031, Loss: 16.1782\n",
      "Epoch: 032, Loss: 16.1525\n",
      "Epoch: 033, Loss: 16.1382\n",
      "Epoch: 034, Loss: 16.1202\n",
      "Epoch: 035, Loss: 16.0872\n",
      "Epoch: 036, Loss: 16.0579\n",
      "Epoch: 037, Loss: 16.0411\n",
      "Epoch: 038, Loss: 16.0165\n",
      "Epoch: 039, Loss: 15.9886\n",
      "Epoch: 040, Loss: 15.9650\n",
      "Epoch: 041, Loss: 15.9305\n",
      "Epoch: 042, Loss: 15.8977\n",
      "Epoch: 043, Loss: 15.8712\n",
      "Epoch: 044, Loss: 15.8347\n",
      "Epoch: 045, Loss: 15.7976\n",
      "Epoch: 046, Loss: 15.7615\n",
      "Epoch: 047, Loss: 15.7242\n",
      "Epoch: 048, Loss: 15.6931\n",
      "Epoch: 049, Loss: 15.6624\n",
      "Epoch: 050, Loss: 15.6296\n",
      "Epoch: 051, Loss: 15.6019\n",
      "Epoch: 052, Loss: 15.5776\n",
      "Epoch: 053, Loss: 15.5524\n",
      "Epoch: 054, Loss: 15.5314\n",
      "Epoch: 055, Loss: 15.5110\n",
      "Epoch: 056, Loss: 15.4876\n",
      "Epoch: 057, Loss: 15.4684\n",
      "Epoch: 058, Loss: 15.4481\n",
      "Epoch: 059, Loss: 15.4292\n",
      "Epoch: 060, Loss: 15.4131\n",
      "Epoch: 061, Loss: 15.3986\n",
      "Epoch: 062, Loss: 15.3962\n",
      "Epoch: 063, Loss: 15.3699\n",
      "Epoch: 064, Loss: 15.3385\n",
      "Epoch: 065, Loss: 15.3040\n",
      "Epoch: 066, Loss: 15.2942\n",
      "Epoch: 067, Loss: 15.2883\n",
      "Epoch: 068, Loss: 15.2441\n",
      "Epoch: 069, Loss: 15.2227\n",
      "Epoch: 070, Loss: 15.2206\n",
      "Epoch: 071, Loss: 15.1920\n",
      "Epoch: 072, Loss: 15.1681\n",
      "Epoch: 073, Loss: 15.1515\n",
      "Epoch: 074, Loss: 15.1465\n",
      "Epoch: 075, Loss: 15.1489\n",
      "Epoch: 076, Loss: 15.1049\n",
      "Epoch: 077, Loss: 15.0707\n",
      "Epoch: 078, Loss: 15.0596\n",
      "Epoch: 079, Loss: 15.0565\n",
      "Epoch: 080, Loss: 15.0579\n",
      "Epoch: 081, Loss: 15.0213\n",
      "Epoch: 082, Loss: 15.0002\n",
      "Epoch: 083, Loss: 14.9958\n",
      "Epoch: 084, Loss: 14.9930\n",
      "Epoch: 085, Loss: 15.0085\n",
      "Epoch: 086, Loss: 15.0220\n",
      "Epoch: 087, Loss: 14.9909\n",
      "Epoch: 088, Loss: 14.9761\n",
      "Epoch: 089, Loss: 14.9473\n",
      "Epoch: 090, Loss: 14.9541\n",
      "Epoch: 091, Loss: 14.9529\n",
      "Epoch: 092, Loss: 14.9394\n",
      "Epoch: 093, Loss: 14.9366\n",
      "Epoch: 094, Loss: 14.9087\n",
      "Epoch: 095, Loss: 14.9166\n",
      "Epoch: 096, Loss: 14.9055\n",
      "Epoch: 097, Loss: 14.8794\n",
      "Epoch: 098, Loss: 14.8978\n",
      "Epoch: 099, Loss: 14.8772\n",
      "Epoch: 100, Loss: 14.8565\n",
      "Epoch: 101, Loss: 14.8550\n",
      "Epoch: 102, Loss: 14.8480\n",
      "Epoch: 103, Loss: 14.8281\n",
      "Epoch: 104, Loss: 14.8269\n",
      "Epoch: 105, Loss: 14.8039\n",
      "Epoch: 106, Loss: 14.8050\n",
      "Epoch: 107, Loss: 14.7966\n",
      "Epoch: 108, Loss: 14.7727\n",
      "Epoch: 109, Loss: 14.7810\n",
      "Epoch: 110, Loss: 14.7730\n",
      "Epoch: 111, Loss: 14.7486\n",
      "Epoch: 112, Loss: 14.7490\n",
      "Epoch: 113, Loss: 14.7571\n",
      "Epoch: 114, Loss: 14.7257\n",
      "Epoch: 115, Loss: 14.7376\n",
      "Epoch: 116, Loss: 14.7867\n",
      "Epoch: 117, Loss: 14.7711\n",
      "Epoch: 118, Loss: 14.7759\n",
      "Epoch: 119, Loss: 14.7500\n",
      "Epoch: 120, Loss: 14.7050\n",
      "Epoch: 121, Loss: 14.7890\n",
      "Epoch: 122, Loss: 14.7556\n",
      "Epoch: 123, Loss: 14.8013\n",
      "Epoch: 124, Loss: 14.6849\n",
      "Epoch: 125, Loss: 14.7895\n",
      "Epoch: 126, Loss: 14.8938\n",
      "Epoch: 127, Loss: 14.7473\n",
      "Epoch: 128, Loss: 14.7889\n",
      "Epoch: 129, Loss: 14.7325\n",
      "Epoch: 130, Loss: 14.7167\n",
      "Epoch: 131, Loss: 14.7344\n",
      "Epoch: 132, Loss: 14.6709\n",
      "Epoch: 133, Loss: 14.7010\n",
      "Epoch: 134, Loss: 14.6584\n",
      "Epoch: 135, Loss: 14.6675\n",
      "Epoch: 136, Loss: 14.6505\n",
      "Epoch: 137, Loss: 14.6261\n",
      "Epoch: 138, Loss: 14.6604\n",
      "Epoch: 139, Loss: 14.6072\n",
      "Epoch: 140, Loss: 14.6291\n",
      "Epoch: 141, Loss: 14.5882\n",
      "Epoch: 142, Loss: 14.6070\n",
      "Epoch: 143, Loss: 14.5959\n",
      "Epoch: 144, Loss: 14.5776\n",
      "Epoch: 145, Loss: 14.5788\n",
      "Epoch: 146, Loss: 14.5525\n",
      "Epoch: 147, Loss: 14.5698\n",
      "Epoch: 148, Loss: 14.5505\n",
      "Epoch: 149, Loss: 14.5534\n",
      "Epoch: 150, Loss: 14.5312\n",
      "Epoch: 151, Loss: 14.5367\n",
      "Epoch: 152, Loss: 14.5202\n",
      "Epoch: 153, Loss: 14.5313\n",
      "Epoch: 154, Loss: 14.5271\n",
      "Epoch: 155, Loss: 14.5317\n",
      "Epoch: 156, Loss: 14.5325\n",
      "Epoch: 157, Loss: 14.5202\n",
      "Epoch: 158, Loss: 14.4992\n",
      "Epoch: 159, Loss: 14.4957\n",
      "Epoch: 160, Loss: 14.5053\n",
      "Epoch: 161, Loss: 14.5157\n",
      "Epoch: 162, Loss: 14.5147\n",
      "Epoch: 163, Loss: 14.5069\n",
      "Epoch: 164, Loss: 14.4970\n",
      "Epoch: 165, Loss: 14.4796\n",
      "Epoch: 166, Loss: 14.4802\n",
      "Epoch: 167, Loss: 14.4779\n",
      "Epoch: 168, Loss: 14.4706\n",
      "Epoch: 169, Loss: 14.4657\n",
      "Epoch: 170, Loss: 14.4563\n",
      "Epoch: 171, Loss: 14.4545\n",
      "Epoch: 172, Loss: 14.4513\n",
      "Epoch: 173, Loss: 14.4505\n",
      "Epoch: 174, Loss: 14.4483\n",
      "Epoch: 175, Loss: 14.4370\n",
      "Epoch: 176, Loss: 14.4327\n",
      "Epoch: 177, Loss: 14.4341\n",
      "Epoch: 178, Loss: 14.4268\n",
      "Epoch: 179, Loss: 14.4217\n",
      "Epoch: 180, Loss: 14.4143\n",
      "Epoch: 181, Loss: 14.4127\n",
      "Epoch: 182, Loss: 14.4166\n",
      "Epoch: 183, Loss: 14.4257\n",
      "Epoch: 184, Loss: 14.4199\n",
      "Epoch: 185, Loss: 14.4199\n",
      "Epoch: 186, Loss: 14.4094\n",
      "Epoch: 187, Loss: 14.4037\n",
      "Epoch: 188, Loss: 14.3933\n",
      "Epoch: 189, Loss: 14.3878\n",
      "Epoch: 190, Loss: 14.3893\n",
      "Epoch: 191, Loss: 14.3952\n",
      "Epoch: 192, Loss: 14.4023\n",
      "Epoch: 193, Loss: 14.3956\n",
      "Epoch: 194, Loss: 14.3881\n",
      "Epoch: 195, Loss: 14.4067\n",
      "Epoch: 196, Loss: 14.4153\n",
      "Epoch: 197, Loss: 14.3918\n",
      "Epoch: 198, Loss: 14.3750\n",
      "Epoch: 199, Loss: 14.3809\n",
      "Epoch: 200, Loss: 14.3804\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 201):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9183\n"
     ]
    }
   ],
   "source": [
    "training_acc = test()\n",
    "print(f'Training Accuracy: {training_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(3, 2)\n",
      "  (conv2): GCNConv(2, 2)\n",
      "  (conv3): GCNConv(2, 1)\n",
      "  (classifier): Linear(in_features=1, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(3, 2)\n",
    "        self.conv2 = GCNConv(2, 2)\n",
    "        self.conv3 = GCNConv(2, 1)\n",
    "        self.classifier = Linear(1, 7)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = self.conv1(x, edge_index)\n",
    "        h = h.tanh()\n",
    "        h = self.conv2(h, edge_index)\n",
    "        h = h.tanh()\n",
    "        h = self.conv3(h, edge_index)\n",
    "        h = h.tanh()  # Final GNN embedding space.\n",
    "        \n",
    "        # Apply a final (linear) classifier.\n",
    "        out = self.classifier(h)\n",
    "\n",
    "        return out, h\n",
    "\n",
    "model = GCN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7026, 7, 3) (2, 12) (7026, 12, 3) (7026, 7)\n"
     ]
    }
   ],
   "source": [
    "node_prefix = [\"T1\", \"T2\", \"readout_error\"]\n",
    "node_features_x = []\n",
    "for k in range(df_train_x.shape[0]):\n",
    "    node_features_j = []\n",
    "    for j in range(num_qubits):\n",
    "        row_features = df_train_x.iloc[k][[i + \"_\" + str(j) for i in node_prefix]].values.flatten()\n",
    "        node_features_j.append(row_features)\n",
    "    node_features_j = scaler.fit_transform(node_features_j)\n",
    "    node_features_x.append(node_features_j)\n",
    "node_features_x = np.array(node_features_x)\n",
    "\n",
    "edge_prefix = [\"cx_\", \"edge_length_\", \"edge_error_\"]\n",
    "edge_index = [[],[]]\n",
    "edge_features_x = []\n",
    "for k in range(df_train_x.shape[0]):\n",
    "    edge_features_j = []\n",
    "    for i in range(num_qubits):\n",
    "        for j in range(num_qubits):\n",
    "            if set([i,j]) in links:\n",
    "                row_features = df_train_x.iloc[k][[prefix + str(i) + str(j) for prefix in edge_prefix]].values.flatten()\n",
    "                edge_features_j.append(row_features)\n",
    "                if k == 0: # only need to do this once\n",
    "                    edge_index[0].append(i)\n",
    "                    edge_index[1].append(j)\n",
    "    edge_features_j = scaler.fit_transform(edge_features_j)\n",
    "    edge_features_x.append(edge_features_j)\n",
    "edge_features_x = np.array(edge_features_x)\n",
    "edge_index = np.array(edge_index)\n",
    "\n",
    "node_labels = df_train_y.to_numpy()\n",
    "print(node_features_x.shape, edge_index.shape, edge_features_x.shape, node_labels.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_node_features_x = torch.tensor(node_features_x, dtype=torch.float)\n",
    "torch_edge_index = torch.tensor(edge_index, dtype=torch.int)\n",
    "torch_edge_features_x = torch.tensor(edge_features_x, dtype=torch.float)\n",
    "torch_node_labels = torch.tensor(node_labels, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 2.109177827835083\n",
      "Epoch: 1, Loss: 2.0964386463165283\n",
      "Epoch: 2, Loss: 2.0848934650421143\n",
      "Epoch: 3, Loss: 2.074511766433716\n",
      "Epoch: 4, Loss: 2.0652151107788086\n",
      "Epoch: 5, Loss: 2.0568840503692627\n",
      "Epoch: 6, Loss: 2.049377202987671\n",
      "Epoch: 7, Loss: 2.042555809020996\n",
      "Epoch: 8, Loss: 2.0363032817840576\n",
      "Epoch: 9, Loss: 2.0305304527282715\n",
      "Epoch: 10, Loss: 2.0251712799072266\n",
      "Epoch: 11, Loss: 2.0201785564422607\n",
      "Epoch: 12, Loss: 2.0155184268951416\n",
      "Epoch: 13, Loss: 2.011164903640747\n",
      "Epoch: 14, Loss: 2.0070948600769043\n",
      "Epoch: 15, Loss: 2.003286361694336\n",
      "Epoch: 16, Loss: 1.999714732170105\n",
      "Epoch: 17, Loss: 1.9963533878326416\n",
      "Epoch: 18, Loss: 1.993174433708191\n",
      "Epoch: 19, Loss: 1.9901496171951294\n",
      "Epoch: 20, Loss: 1.9872545003890991\n",
      "Epoch: 21, Loss: 1.9844681024551392\n",
      "Epoch: 22, Loss: 1.981775164604187\n",
      "Epoch: 23, Loss: 1.9791641235351562\n",
      "Epoch: 24, Loss: 1.9766274690628052\n",
      "Epoch: 25, Loss: 1.9741599559783936\n",
      "Epoch: 26, Loss: 1.9717581272125244\n",
      "Epoch: 27, Loss: 1.9694193601608276\n",
      "Epoch: 28, Loss: 1.967141032218933\n",
      "Epoch: 29, Loss: 1.9649200439453125\n",
      "Epoch: 30, Loss: 1.9627526998519897\n",
      "Epoch: 31, Loss: 1.9606330394744873\n",
      "Epoch: 32, Loss: 1.9585539102554321\n",
      "Epoch: 33, Loss: 1.956505537033081\n",
      "Epoch: 34, Loss: 1.9544750452041626\n",
      "Epoch: 35, Loss: 1.9524470567703247\n",
      "Epoch: 36, Loss: 1.9504032135009766\n",
      "Epoch: 37, Loss: 1.948320746421814\n",
      "Epoch: 38, Loss: 1.9461746215820312\n",
      "Epoch: 39, Loss: 1.9439352750778198\n",
      "Epoch: 40, Loss: 1.9415713548660278\n",
      "Epoch: 41, Loss: 1.9390490055084229\n",
      "Epoch: 42, Loss: 1.9363325834274292\n",
      "Epoch: 43, Loss: 1.9333875179290771\n",
      "Epoch: 44, Loss: 1.9301797151565552\n",
      "Epoch: 45, Loss: 1.926679015159607\n",
      "Epoch: 46, Loss: 1.9228613376617432\n",
      "Epoch: 47, Loss: 1.918709397315979\n",
      "Epoch: 48, Loss: 1.9142142534255981\n",
      "Epoch: 49, Loss: 1.9093782901763916\n",
      "Epoch: 50, Loss: 1.9042110443115234\n",
      "Epoch: 51, Loss: 1.8987276554107666\n",
      "Epoch: 52, Loss: 1.8929435014724731\n",
      "Epoch: 53, Loss: 1.8868663311004639\n",
      "Epoch: 54, Loss: 1.8804919719696045\n",
      "Epoch: 55, Loss: 1.8738048076629639\n",
      "Epoch: 56, Loss: 1.8667867183685303\n",
      "Epoch: 57, Loss: 1.8594284057617188\n",
      "Epoch: 58, Loss: 1.851738691329956\n",
      "Epoch: 59, Loss: 1.843746304512024\n",
      "Epoch: 60, Loss: 1.8354947566986084\n",
      "Epoch: 61, Loss: 1.8270351886749268\n",
      "Epoch: 62, Loss: 1.818415880203247\n",
      "Epoch: 63, Loss: 1.8096729516983032\n",
      "Epoch: 64, Loss: 1.8008253574371338\n",
      "Epoch: 65, Loss: 1.7918764352798462\n",
      "Epoch: 66, Loss: 1.7828212976455688\n",
      "Epoch: 67, Loss: 1.7736618518829346\n",
      "Epoch: 68, Loss: 1.7644158601760864\n",
      "Epoch: 69, Loss: 1.7551193237304688\n",
      "Epoch: 70, Loss: 1.7458173036575317\n",
      "Epoch: 71, Loss: 1.7365487813949585\n",
      "Epoch: 72, Loss: 1.7273331880569458\n",
      "Epoch: 73, Loss: 1.7181705236434937\n",
      "Epoch: 74, Loss: 1.7090569734573364\n",
      "Epoch: 75, Loss: 1.7000004053115845\n",
      "Epoch: 76, Loss: 1.6910275220870972\n",
      "Epoch: 77, Loss: 1.6821727752685547\n",
      "Epoch: 78, Loss: 1.673461675643921\n",
      "Epoch: 79, Loss: 1.6648980379104614\n",
      "Epoch: 80, Loss: 1.65647292137146\n",
      "Epoch: 81, Loss: 1.6481798887252808\n",
      "Epoch: 82, Loss: 1.6400290727615356\n",
      "Epoch: 83, Loss: 1.6320412158966064\n",
      "Epoch: 84, Loss: 1.6242328882217407\n",
      "Epoch: 85, Loss: 1.6166023015975952\n",
      "Epoch: 86, Loss: 1.6091382503509521\n",
      "Epoch: 87, Loss: 1.6018351316452026\n",
      "Epoch: 88, Loss: 1.5947015285491943\n",
      "Epoch: 89, Loss: 1.5877496004104614\n",
      "Epoch: 90, Loss: 1.5809811353683472\n",
      "Epoch: 91, Loss: 1.5743855237960815\n",
      "Epoch: 92, Loss: 1.5679528713226318\n",
      "Epoch: 93, Loss: 1.56168532371521\n",
      "Epoch: 94, Loss: 1.5555896759033203\n",
      "Epoch: 95, Loss: 1.5496656894683838\n",
      "Epoch: 96, Loss: 1.5439027547836304\n",
      "Epoch: 97, Loss: 1.5382916927337646\n",
      "Epoch: 98, Loss: 1.5328313112258911\n",
      "Epoch: 99, Loss: 1.5275239944458008\n",
      "Epoch: 100, Loss: 1.522364616394043\n",
      "Epoch: 101, Loss: 1.517342448234558\n",
      "Epoch: 102, Loss: 1.5124518871307373\n",
      "Epoch: 103, Loss: 1.5076930522918701\n",
      "Epoch: 104, Loss: 1.503064513206482\n",
      "Epoch: 105, Loss: 1.4985597133636475\n",
      "Epoch: 106, Loss: 1.4941699504852295\n",
      "Epoch: 107, Loss: 1.4898937940597534\n",
      "Epoch: 108, Loss: 1.4857300519943237\n",
      "Epoch: 109, Loss: 1.4816731214523315\n",
      "Epoch: 110, Loss: 1.4777157306671143\n",
      "Epoch: 111, Loss: 1.473854899406433\n",
      "Epoch: 112, Loss: 1.4700889587402344\n",
      "Epoch: 113, Loss: 1.4664140939712524\n",
      "Epoch: 114, Loss: 1.4628239870071411\n",
      "Epoch: 115, Loss: 1.4593154191970825\n",
      "Epoch: 116, Loss: 1.4558871984481812\n",
      "Epoch: 117, Loss: 1.4525353908538818\n",
      "Epoch: 118, Loss: 1.4492547512054443\n",
      "Epoch: 119, Loss: 1.446042776107788\n",
      "Epoch: 120, Loss: 1.4428980350494385\n",
      "Epoch: 121, Loss: 1.4398165941238403\n",
      "Epoch: 122, Loss: 1.4367948770523071\n",
      "Epoch: 123, Loss: 1.4338301420211792\n",
      "Epoch: 124, Loss: 1.430920958518982\n",
      "Epoch: 125, Loss: 1.4280637502670288\n",
      "Epoch: 126, Loss: 1.4252557754516602\n",
      "Epoch: 127, Loss: 1.4224951267242432\n",
      "Epoch: 128, Loss: 1.4197800159454346\n",
      "Epoch: 129, Loss: 1.4171074628829956\n",
      "Epoch: 130, Loss: 1.414475440979004\n",
      "Epoch: 131, Loss: 1.4118826389312744\n",
      "Epoch: 132, Loss: 1.4093265533447266\n",
      "Epoch: 133, Loss: 1.4068056344985962\n",
      "Epoch: 134, Loss: 1.404317855834961\n",
      "Epoch: 135, Loss: 1.4018620252609253\n",
      "Epoch: 136, Loss: 1.3994358777999878\n",
      "Epoch: 137, Loss: 1.3970385789871216\n",
      "Epoch: 138, Loss: 1.3946683406829834\n",
      "Epoch: 139, Loss: 1.3923242092132568\n",
      "Epoch: 140, Loss: 1.3900047540664673\n",
      "Epoch: 141, Loss: 1.3877085447311401\n",
      "Epoch: 142, Loss: 1.3854347467422485\n",
      "Epoch: 143, Loss: 1.383182168006897\n",
      "Epoch: 144, Loss: 1.380949854850769\n",
      "Epoch: 145, Loss: 1.3787367343902588\n",
      "Epoch: 146, Loss: 1.3765422105789185\n",
      "Epoch: 147, Loss: 1.3743654489517212\n",
      "Epoch: 148, Loss: 1.3722056150436401\n",
      "Epoch: 149, Loss: 1.3700617551803589\n",
      "Epoch: 150, Loss: 1.3679333925247192\n",
      "Epoch: 151, Loss: 1.365820050239563\n",
      "Epoch: 152, Loss: 1.3637211322784424\n",
      "Epoch: 153, Loss: 1.3616359233856201\n",
      "Epoch: 154, Loss: 1.3595640659332275\n",
      "Epoch: 155, Loss: 1.3575047254562378\n",
      "Epoch: 156, Loss: 1.35545814037323\n",
      "Epoch: 157, Loss: 1.3534234762191772\n",
      "Epoch: 158, Loss: 1.35140061378479\n",
      "Epoch: 159, Loss: 1.3493890762329102\n",
      "Epoch: 160, Loss: 1.347388505935669\n",
      "Epoch: 161, Loss: 1.3453987836837769\n",
      "Epoch: 162, Loss: 1.3434194326400757\n",
      "Epoch: 163, Loss: 1.341450572013855\n",
      "Epoch: 164, Loss: 1.3394917249679565\n",
      "Epoch: 165, Loss: 1.3375425338745117\n",
      "Epoch: 166, Loss: 1.3356033563613892\n",
      "Epoch: 167, Loss: 1.333673357963562\n",
      "Epoch: 168, Loss: 1.3317530155181885\n",
      "Epoch: 169, Loss: 1.3298416137695312\n",
      "Epoch: 170, Loss: 1.3279396295547485\n",
      "Epoch: 171, Loss: 1.326046347618103\n",
      "Epoch: 172, Loss: 1.3241618871688843\n",
      "Epoch: 173, Loss: 1.3222864866256714\n",
      "Epoch: 174, Loss: 1.3204195499420166\n",
      "Epoch: 175, Loss: 1.3185614347457886\n",
      "Epoch: 176, Loss: 1.316711664199829\n",
      "Epoch: 177, Loss: 1.3148705959320068\n",
      "Epoch: 178, Loss: 1.3130375146865845\n",
      "Epoch: 179, Loss: 1.3112128973007202\n",
      "Epoch: 180, Loss: 1.309396743774414\n",
      "Epoch: 181, Loss: 1.3075885772705078\n",
      "Epoch: 182, Loss: 1.3057886362075806\n",
      "Epoch: 183, Loss: 1.3039969205856323\n",
      "Epoch: 184, Loss: 1.3022130727767944\n",
      "Epoch: 185, Loss: 1.3004376888275146\n",
      "Epoch: 186, Loss: 1.2986700534820557\n",
      "Epoch: 187, Loss: 1.2969104051589966\n",
      "Epoch: 188, Loss: 1.2951589822769165\n",
      "Epoch: 189, Loss: 1.2934153079986572\n",
      "Epoch: 190, Loss: 1.2916796207427979\n",
      "Epoch: 191, Loss: 1.289952039718628\n",
      "Epoch: 192, Loss: 1.2882322072982788\n",
      "Epoch: 193, Loss: 1.2865206003189087\n",
      "Epoch: 194, Loss: 1.2848167419433594\n",
      "Epoch: 195, Loss: 1.2831209897994995\n",
      "Epoch: 196, Loss: 1.28143310546875\n",
      "Epoch: 197, Loss: 1.27975332736969\n",
      "Epoch: 198, Loss: 1.2780816555023193\n",
      "Epoch: 199, Loss: 1.2764177322387695\n",
      "Epoch: 200, Loss: 1.2747621536254883\n",
      "Epoch: 201, Loss: 1.273114562034607\n",
      "Epoch: 202, Loss: 1.2714751958847046\n",
      "Epoch: 203, Loss: 1.2698440551757812\n",
      "Epoch: 204, Loss: 1.2682210206985474\n",
      "Epoch: 205, Loss: 1.2666064500808716\n",
      "Epoch: 206, Loss: 1.2650002241134644\n",
      "Epoch: 207, Loss: 1.2634023427963257\n",
      "Epoch: 208, Loss: 1.2618128061294556\n",
      "Epoch: 209, Loss: 1.260231852531433\n",
      "Epoch: 210, Loss: 1.2586594820022583\n",
      "Epoch: 211, Loss: 1.2570956945419312\n",
      "Epoch: 212, Loss: 1.2555406093597412\n",
      "Epoch: 213, Loss: 1.2539942264556885\n",
      "Epoch: 214, Loss: 1.2524566650390625\n",
      "Epoch: 215, Loss: 1.2509280443191528\n",
      "Epoch: 216, Loss: 1.2494083642959595\n",
      "Epoch: 217, Loss: 1.247897744178772\n",
      "Epoch: 218, Loss: 1.2463959455490112\n",
      "Epoch: 219, Loss: 1.244903326034546\n",
      "Epoch: 220, Loss: 1.243419885635376\n",
      "Epoch: 221, Loss: 1.2419456243515015\n",
      "Epoch: 222, Loss: 1.240480661392212\n",
      "Epoch: 223, Loss: 1.2390249967575073\n",
      "Epoch: 224, Loss: 1.2375786304473877\n",
      "Epoch: 225, Loss: 1.2361418008804321\n",
      "Epoch: 226, Loss: 1.2347142696380615\n",
      "Epoch: 227, Loss: 1.233296275138855\n",
      "Epoch: 228, Loss: 1.2318875789642334\n",
      "Epoch: 229, Loss: 1.2304884195327759\n",
      "Epoch: 230, Loss: 1.2290985584259033\n",
      "Epoch: 231, Loss: 1.2277183532714844\n",
      "Epoch: 232, Loss: 1.22634756565094\n",
      "Epoch: 233, Loss: 1.22498619556427\n",
      "Epoch: 234, Loss: 1.2236344814300537\n",
      "Epoch: 235, Loss: 1.222292184829712\n",
      "Epoch: 236, Loss: 1.2209593057632446\n",
      "Epoch: 237, Loss: 1.2196358442306519\n",
      "Epoch: 238, Loss: 1.218321681022644\n",
      "Epoch: 239, Loss: 1.2170169353485107\n",
      "Epoch: 240, Loss: 1.2157213687896729\n",
      "Epoch: 241, Loss: 1.2144349813461304\n",
      "Epoch: 242, Loss: 1.2131578922271729\n",
      "Epoch: 243, Loss: 1.2118899822235107\n",
      "Epoch: 244, Loss: 1.2106307744979858\n",
      "Epoch: 245, Loss: 1.209380865097046\n",
      "Epoch: 246, Loss: 1.2081396579742432\n",
      "Epoch: 247, Loss: 1.2069072723388672\n",
      "Epoch: 248, Loss: 1.2056838274002075\n",
      "Epoch: 249, Loss: 1.204468846321106\n",
      "Epoch: 250, Loss: 1.203262448310852\n",
      "Epoch: 251, Loss: 1.2020645141601562\n",
      "Epoch: 252, Loss: 1.200874924659729\n",
      "Epoch: 253, Loss: 1.1996935606002808\n",
      "Epoch: 254, Loss: 1.1985204219818115\n",
      "Epoch: 255, Loss: 1.1973552703857422\n",
      "Epoch: 256, Loss: 1.1961982250213623\n",
      "Epoch: 257, Loss: 1.1950489282608032\n",
      "Epoch: 258, Loss: 1.1939074993133545\n",
      "Epoch: 259, Loss: 1.192773699760437\n",
      "Epoch: 260, Loss: 1.1916472911834717\n",
      "Epoch: 261, Loss: 1.1905285120010376\n",
      "Epoch: 262, Loss: 1.1894168853759766\n",
      "Epoch: 263, Loss: 1.1883127689361572\n",
      "Epoch: 264, Loss: 1.1872154474258423\n",
      "Epoch: 265, Loss: 1.1861255168914795\n",
      "Epoch: 266, Loss: 1.185042381286621\n",
      "Epoch: 267, Loss: 1.1839659214019775\n",
      "Epoch: 268, Loss: 1.182896375656128\n",
      "Epoch: 269, Loss: 1.1818333864212036\n",
      "Epoch: 270, Loss: 1.1807771921157837\n",
      "Epoch: 271, Loss: 1.1797271966934204\n",
      "Epoch: 272, Loss: 1.1786835193634033\n",
      "Epoch: 273, Loss: 1.177646279335022\n",
      "Epoch: 274, Loss: 1.1766149997711182\n",
      "Epoch: 275, Loss: 1.17559015750885\n",
      "Epoch: 276, Loss: 1.1745710372924805\n",
      "Epoch: 277, Loss: 1.173557996749878\n",
      "Epoch: 278, Loss: 1.1725507974624634\n",
      "Epoch: 279, Loss: 1.1715494394302368\n",
      "Epoch: 280, Loss: 1.1705535650253296\n",
      "Epoch: 281, Loss: 1.1695635318756104\n",
      "Epoch: 282, Loss: 1.1685791015625\n",
      "Epoch: 283, Loss: 1.1675999164581299\n",
      "Epoch: 284, Loss: 1.1666265726089478\n",
      "Epoch: 285, Loss: 1.1656582355499268\n",
      "Epoch: 286, Loss: 1.1646955013275146\n",
      "Epoch: 287, Loss: 1.1637378931045532\n",
      "Epoch: 288, Loss: 1.1627854108810425\n",
      "Epoch: 289, Loss: 1.1618379354476929\n",
      "Epoch: 290, Loss: 1.1608957052230835\n",
      "Epoch: 291, Loss: 1.1599586009979248\n",
      "Epoch: 292, Loss: 1.1590262651443481\n",
      "Epoch: 293, Loss: 1.1580990552902222\n",
      "Epoch: 294, Loss: 1.1571766138076782\n",
      "Epoch: 295, Loss: 1.1562589406967163\n",
      "Epoch: 296, Loss: 1.155346155166626\n",
      "Epoch: 297, Loss: 1.1544380187988281\n",
      "Epoch: 298, Loss: 1.1535346508026123\n",
      "Epoch: 299, Loss: 1.152635931968689\n",
      "Epoch: 300, Loss: 1.151741862297058\n",
      "Epoch: 301, Loss: 1.1508523225784302\n",
      "Epoch: 302, Loss: 1.1499673128128052\n",
      "Epoch: 303, Loss: 1.1490869522094727\n",
      "Epoch: 304, Loss: 1.1482107639312744\n",
      "Epoch: 305, Loss: 1.1473392248153687\n",
      "Epoch: 306, Loss: 1.1464719772338867\n",
      "Epoch: 307, Loss: 1.1456091403961182\n",
      "Epoch: 308, Loss: 1.1447508335113525\n",
      "Epoch: 309, Loss: 1.143896460533142\n",
      "Epoch: 310, Loss: 1.143046498298645\n",
      "Epoch: 311, Loss: 1.1422008275985718\n",
      "Epoch: 312, Loss: 1.1413594484329224\n",
      "Epoch: 313, Loss: 1.1405221223831177\n",
      "Epoch: 314, Loss: 1.1396888494491577\n",
      "Epoch: 315, Loss: 1.138859748840332\n",
      "Epoch: 316, Loss: 1.1380349397659302\n",
      "Epoch: 317, Loss: 1.137213945388794\n",
      "Epoch: 318, Loss: 1.1363970041275024\n",
      "Epoch: 319, Loss: 1.1355839967727661\n",
      "Epoch: 320, Loss: 1.134775161743164\n",
      "Epoch: 321, Loss: 1.1339701414108276\n",
      "Epoch: 322, Loss: 1.133169174194336\n",
      "Epoch: 323, Loss: 1.1323719024658203\n",
      "Epoch: 324, Loss: 1.1315785646438599\n",
      "Epoch: 325, Loss: 1.1307891607284546\n",
      "Epoch: 326, Loss: 1.130003571510315\n",
      "Epoch: 327, Loss: 1.1292216777801514\n",
      "Epoch: 328, Loss: 1.1284434795379639\n",
      "Epoch: 329, Loss: 1.1276692152023315\n",
      "Epoch: 330, Loss: 1.1268984079360962\n",
      "Epoch: 331, Loss: 1.1261316537857056\n",
      "Epoch: 332, Loss: 1.1253682374954224\n",
      "Epoch: 333, Loss: 1.1246085166931152\n",
      "Epoch: 334, Loss: 1.1238523721694946\n",
      "Epoch: 335, Loss: 1.1231000423431396\n",
      "Epoch: 336, Loss: 1.1223509311676025\n",
      "Epoch: 337, Loss: 1.121605634689331\n",
      "Epoch: 338, Loss: 1.120863914489746\n",
      "Epoch: 339, Loss: 1.120125412940979\n",
      "Epoch: 340, Loss: 1.119390606880188\n",
      "Epoch: 341, Loss: 1.1186590194702148\n",
      "Epoch: 342, Loss: 1.1179311275482178\n",
      "Epoch: 343, Loss: 1.117206335067749\n",
      "Epoch: 344, Loss: 1.1164851188659668\n",
      "Epoch: 345, Loss: 1.1157671213150024\n",
      "Epoch: 346, Loss: 1.1150524616241455\n",
      "Epoch: 347, Loss: 1.114341378211975\n",
      "Epoch: 348, Loss: 1.1136332750320435\n",
      "Epoch: 349, Loss: 1.1129285097122192\n",
      "Epoch: 350, Loss: 1.1122268438339233\n",
      "Epoch: 351, Loss: 1.1115286350250244\n",
      "Epoch: 352, Loss: 1.1108334064483643\n",
      "Epoch: 353, Loss: 1.1101415157318115\n",
      "Epoch: 354, Loss: 1.1094528436660767\n",
      "Epoch: 355, Loss: 1.1087669134140015\n",
      "Epoch: 356, Loss: 1.1080843210220337\n",
      "Epoch: 357, Loss: 1.1074049472808838\n",
      "Epoch: 358, Loss: 1.106728434562683\n",
      "Epoch: 359, Loss: 1.1060551404953003\n",
      "Epoch: 360, Loss: 1.1053848266601562\n",
      "Epoch: 361, Loss: 1.1047173738479614\n",
      "Epoch: 362, Loss: 1.1040529012680054\n",
      "Epoch: 363, Loss: 1.103391408920288\n",
      "Epoch: 364, Loss: 1.1027328968048096\n",
      "Epoch: 365, Loss: 1.1020773649215698\n",
      "Epoch: 366, Loss: 1.1014245748519897\n",
      "Epoch: 367, Loss: 1.1007747650146484\n",
      "Epoch: 368, Loss: 1.1001278162002563\n",
      "Epoch: 369, Loss: 1.0994837284088135\n",
      "Epoch: 370, Loss: 1.0988423824310303\n",
      "Epoch: 371, Loss: 1.0982037782669067\n",
      "Epoch: 372, Loss: 1.0975680351257324\n",
      "Epoch: 373, Loss: 1.0969350337982178\n",
      "Epoch: 374, Loss: 1.0963048934936523\n",
      "Epoch: 375, Loss: 1.0956772565841675\n",
      "Epoch: 376, Loss: 1.0950523614883423\n",
      "Epoch: 377, Loss: 1.0944303274154663\n",
      "Epoch: 378, Loss: 1.093810796737671\n",
      "Epoch: 379, Loss: 1.0931941270828247\n",
      "Epoch: 380, Loss: 1.092579960823059\n",
      "Epoch: 381, Loss: 1.091968297958374\n",
      "Epoch: 382, Loss: 1.0913593769073486\n",
      "Epoch: 383, Loss: 1.0907529592514038\n",
      "Epoch: 384, Loss: 1.090149164199829\n",
      "Epoch: 385, Loss: 1.089547872543335\n",
      "Epoch: 386, Loss: 1.0889490842819214\n",
      "Epoch: 387, Loss: 1.088352918624878\n",
      "Epoch: 388, Loss: 1.0877591371536255\n",
      "Epoch: 389, Loss: 1.0871678590774536\n",
      "Epoch: 390, Loss: 1.0865790843963623\n",
      "Epoch: 391, Loss: 1.0859928131103516\n",
      "Epoch: 392, Loss: 1.0854088068008423\n",
      "Epoch: 393, Loss: 1.0848273038864136\n",
      "Epoch: 394, Loss: 1.0842483043670654\n",
      "Epoch: 395, Loss: 1.0836715698242188\n",
      "Epoch: 396, Loss: 1.083097219467163\n",
      "Epoch: 397, Loss: 1.082525372505188\n",
      "Epoch: 398, Loss: 1.0819555521011353\n",
      "Epoch: 399, Loss: 1.081388235092163\n",
      "Epoch: 400, Loss: 1.080823302268982\n"
     ]
    }
   ],
   "source": [
    "model = GCN()\n",
    "criterion = torch.nn.CrossEntropyLoss()  #Initialize the CrossEntropyLoss function.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # Initialize the Adam optimizer.\n",
    "\n",
    "def train(node_features, edge_index, node_labels):\n",
    "\n",
    "    optimizer.zero_grad()  # Clear gradients.\n",
    "    out, h = model(node_features, edge_index)  # Perform a single forward pass.\n",
    "    loss = criterion(out, node_labels)  # Compute the loss solely based on the training nodes.\n",
    "    loss.backward()  # Derive gradients.\n",
    "    optimizer.step()  # Update parameters based on gradients.\n",
    "    return loss, h\n",
    "\n",
    "for epoch in range(401):\n",
    "    loss, h = train(torch_node_features_x[0], torch_edge_index, torch_node_labels[0])\n",
    "    print(f'Epoch: {epoch}, Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eece571f",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
