{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "num_qubits = 7\n",
    "dataset_filename = \"dataset/dataset_tesi/NN1_Dataset(<=10Cx)_balanced1.csv\"\n",
    "df = pd.read_csv(dataset_filename)\n",
    "\n",
    "links = [set([0,1]), set([1,2]), set([1,3]), set([3,5]), set([4,5]), set([5,6])]\n",
    "def generate_columns(header, links, in_links=False):\n",
    "    if in_links:\n",
    "        return [header+str(i)+str(j) for i in range(num_qubits) for j in range(num_qubits) if set([i,j]) in links]\n",
    "    else:\n",
    "        return [header+str(i)+str(j) for i in range(num_qubits) for j in range(num_qubits) if set([i,j]) not in links and i!=j]\n",
    "\n",
    "useless_columns = ['Unnamed: 0', 'last_update_date', 'N_qubtis', 'N_measure', 'N_cx', 'backend_name']\n",
    "useless_columns += generate_columns(\"cx_\", links)\n",
    "useless_columns += generate_columns(\"edge_length_\", links)\n",
    "useless_columns += generate_columns(\"edge_error_\", links)\n",
    "useless_columns += [\"measure_\"+str(i) for i in range(num_qubits)]\n",
    "# Note that cx/edge_error/edge_length_xy is not neccessarily the same as cx/edge_length/edge_error_yx\n",
    "df.drop(columns=useless_columns, inplace=True)\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7026, 57), (7026, 49), (1757, 57), (1757, 49))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "df_train_x, df_train_y= train.iloc[:, :-num_qubits], train.iloc[:, -num_qubits:]\n",
    "df_test_x, df_test_y= test.iloc[:, :-num_qubits], test.iloc[:, -num_qubits:]\n",
    "\n",
    "train_x = scaler.fit_transform(df_train_x)\n",
    "test_x = scaler.fit_transform(df_test_x)\n",
    "\n",
    "\n",
    "# for every row in y, convert to 1 hot-encoding and flatten\n",
    "train_y = []\n",
    "for _, row in df_train_y.iterrows():\n",
    "    train_y.append(pd.get_dummies(row).values.flatten())\n",
    "train_y = np.array(train_y)\n",
    "\n",
    "test_y = []\n",
    "for _, row in df_test_y.iterrows():\n",
    "    test_y.append(pd.get_dummies(row).values.flatten())\n",
    "test_y = np.array(test_y)\n",
    "\n",
    "(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacky/anaconda3/envs/eece571f/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.lin1 = Linear(num_features, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin3 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin4 = Linear(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = x.relu()\n",
    "        x = self.lin2(x)\n",
    "        x = x.relu()\n",
    "        x = self.lin3(x)\n",
    "        x = x.relu()\n",
    "        x = self.lin4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(57, 49, hidden_channels=128)\n",
    "criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.03, weight_decay=5e-4)  # Define optimizer.\n",
    "\n",
    "torch_train_x = torch.tensor(train_x, dtype=torch.float)\n",
    "torch_train_y = torch.tensor(train_y, dtype=torch.float)\n",
    "torch_test_x = torch.tensor(test_x, dtype=torch.float)\n",
    "torch_test_y = torch.tensor(test_y, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(torch_train_x)  # Perform a single forward pass.\n",
    "      loss = criterion(out, torch_train_y)  # Compute the loss solely based on the training nodes.\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test(x, y):\n",
    "      model.eval()\n",
    "      pred = model(x)\n",
    "\n",
    "      test_correct = 0\n",
    "      for i, j in zip(pred, y):\n",
    "          pred_i = np.argmax(i.detach().numpy().reshape(7,7), axis=1)\n",
    "          label_j = np.argmax(j.detach().numpy().reshape(7,7), axis=1)\n",
    "          test_correct += np.array_equal(pred_i, label_j)\n",
    "      test_acc = int(test_correct) / int(y.shape[0])  # Derive ratio of correct predictions.\n",
    "      return test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 27.0795\n",
      "Epoch: 002, Loss: 21.8784\n",
      "Epoch: 003, Loss: 21.3658\n",
      "Epoch: 004, Loss: 20.5554\n",
      "Epoch: 005, Loss: 19.8338\n",
      "Epoch: 006, Loss: 19.8830\n",
      "Epoch: 007, Loss: 19.3283\n",
      "Epoch: 008, Loss: 19.0172\n",
      "Epoch: 009, Loss: 18.3522\n",
      "Epoch: 010, Loss: 18.2405\n",
      "Epoch: 011, Loss: 17.8325\n",
      "Epoch: 012, Loss: 17.4914\n",
      "Epoch: 013, Loss: 17.1906\n",
      "Epoch: 014, Loss: 17.1535\n",
      "Epoch: 015, Loss: 17.0007\n",
      "Epoch: 016, Loss: 16.9130\n",
      "Epoch: 017, Loss: 17.0232\n",
      "Epoch: 018, Loss: 16.9232\n",
      "Epoch: 019, Loss: 16.8283\n",
      "Epoch: 020, Loss: 16.7672\n",
      "Epoch: 021, Loss: 16.7065\n",
      "Epoch: 022, Loss: 16.6304\n",
      "Epoch: 023, Loss: 16.5409\n",
      "Epoch: 024, Loss: 16.4578\n",
      "Epoch: 025, Loss: 16.3857\n",
      "Epoch: 026, Loss: 16.3437\n",
      "Epoch: 027, Loss: 16.2987\n",
      "Epoch: 028, Loss: 16.2615\n",
      "Epoch: 029, Loss: 16.2335\n",
      "Epoch: 030, Loss: 16.1881\n",
      "Epoch: 031, Loss: 16.1539\n",
      "Epoch: 032, Loss: 16.1089\n",
      "Epoch: 033, Loss: 16.0738\n",
      "Epoch: 034, Loss: 16.0460\n",
      "Epoch: 035, Loss: 16.0170\n",
      "Epoch: 036, Loss: 15.9896\n",
      "Epoch: 037, Loss: 15.9751\n",
      "Epoch: 038, Loss: 15.9979\n",
      "Epoch: 039, Loss: 16.0408\n",
      "Epoch: 040, Loss: 15.9564\n",
      "Epoch: 041, Loss: 15.8724\n",
      "Epoch: 042, Loss: 15.9082\n",
      "Epoch: 043, Loss: 15.8559\n",
      "Epoch: 044, Loss: 15.8440\n",
      "Epoch: 045, Loss: 15.8060\n",
      "Epoch: 046, Loss: 15.8457\n",
      "Epoch: 047, Loss: 15.7570\n",
      "Epoch: 048, Loss: 15.7224\n",
      "Epoch: 049, Loss: 15.6722\n",
      "Epoch: 050, Loss: 15.7258\n",
      "Epoch: 051, Loss: 15.7055\n",
      "Epoch: 052, Loss: 15.6145\n",
      "Epoch: 053, Loss: 15.5340\n",
      "Epoch: 054, Loss: 15.5897\n",
      "Epoch: 055, Loss: 15.5095\n",
      "Epoch: 056, Loss: 15.4352\n",
      "Epoch: 057, Loss: 15.4227\n",
      "Epoch: 058, Loss: 15.4015\n",
      "Epoch: 059, Loss: 15.5437\n",
      "Epoch: 060, Loss: 15.7429\n",
      "Epoch: 061, Loss: 15.4180\n",
      "Epoch: 062, Loss: 15.4829\n",
      "Epoch: 063, Loss: 15.4993\n",
      "Epoch: 064, Loss: 15.4473\n",
      "Epoch: 065, Loss: 15.4022\n",
      "Epoch: 066, Loss: 15.3729\n",
      "Epoch: 067, Loss: 15.3600\n",
      "Epoch: 068, Loss: 15.3299\n",
      "Epoch: 069, Loss: 15.3102\n",
      "Epoch: 070, Loss: 15.3168\n",
      "Epoch: 071, Loss: 15.2544\n",
      "Epoch: 072, Loss: 15.2654\n",
      "Epoch: 073, Loss: 15.1777\n",
      "Epoch: 074, Loss: 15.2282\n",
      "Epoch: 075, Loss: 15.1559\n",
      "Epoch: 076, Loss: 15.1583\n",
      "Epoch: 077, Loss: 15.1566\n",
      "Epoch: 078, Loss: 15.0948\n",
      "Epoch: 079, Loss: 15.1122\n",
      "Epoch: 080, Loss: 15.0967\n",
      "Epoch: 081, Loss: 15.0726\n",
      "Epoch: 082, Loss: 15.0628\n",
      "Epoch: 083, Loss: 15.0493\n",
      "Epoch: 084, Loss: 15.0183\n",
      "Epoch: 085, Loss: 15.0278\n",
      "Epoch: 086, Loss: 15.0170\n",
      "Epoch: 087, Loss: 15.0286\n",
      "Epoch: 088, Loss: 15.2176\n",
      "Epoch: 089, Loss: 15.0547\n",
      "Epoch: 090, Loss: 15.0603\n",
      "Epoch: 091, Loss: 14.9857\n",
      "Epoch: 092, Loss: 15.0179\n",
      "Epoch: 093, Loss: 14.9930\n",
      "Epoch: 094, Loss: 14.9906\n",
      "Epoch: 095, Loss: 14.9479\n",
      "Epoch: 096, Loss: 14.9587\n",
      "Epoch: 097, Loss: 14.9384\n",
      "Epoch: 098, Loss: 14.8948\n",
      "Epoch: 099, Loss: 14.9198\n",
      "Epoch: 100, Loss: 14.8647\n",
      "Epoch: 101, Loss: 14.8785\n",
      "Epoch: 102, Loss: 14.8822\n",
      "Epoch: 103, Loss: 14.8651\n",
      "Epoch: 104, Loss: 14.9310\n",
      "Epoch: 105, Loss: 14.8882\n",
      "Epoch: 106, Loss: 14.8776\n",
      "Epoch: 107, Loss: 14.8043\n",
      "Epoch: 108, Loss: 14.8415\n",
      "Epoch: 109, Loss: 14.8402\n",
      "Epoch: 110, Loss: 14.7785\n",
      "Epoch: 111, Loss: 14.8151\n",
      "Epoch: 112, Loss: 14.8012\n",
      "Epoch: 113, Loss: 14.7863\n",
      "Epoch: 114, Loss: 14.7872\n",
      "Epoch: 115, Loss: 14.7488\n",
      "Epoch: 116, Loss: 14.7698\n",
      "Epoch: 117, Loss: 14.7796\n",
      "Epoch: 118, Loss: 14.7044\n",
      "Epoch: 119, Loss: 14.7471\n",
      "Epoch: 120, Loss: 14.7245\n",
      "Epoch: 121, Loss: 14.7901\n",
      "Epoch: 122, Loss: 14.8766\n",
      "Epoch: 123, Loss: 14.7272\n",
      "Epoch: 124, Loss: 14.7308\n",
      "Epoch: 125, Loss: 14.7352\n",
      "Epoch: 126, Loss: 14.7054\n",
      "Epoch: 127, Loss: 14.7152\n",
      "Epoch: 128, Loss: 14.6883\n",
      "Epoch: 129, Loss: 14.6685\n",
      "Epoch: 130, Loss: 14.6696\n",
      "Epoch: 131, Loss: 14.7026\n",
      "Epoch: 132, Loss: 14.7049\n",
      "Epoch: 133, Loss: 14.8600\n",
      "Epoch: 134, Loss: 14.7340\n",
      "Epoch: 135, Loss: 14.7142\n",
      "Epoch: 136, Loss: 14.7058\n",
      "Epoch: 137, Loss: 14.7317\n",
      "Epoch: 138, Loss: 14.6961\n",
      "Epoch: 139, Loss: 14.6838\n",
      "Epoch: 140, Loss: 14.6701\n",
      "Epoch: 141, Loss: 14.6515\n",
      "Epoch: 142, Loss: 14.6350\n",
      "Epoch: 143, Loss: 14.6192\n",
      "Epoch: 144, Loss: 14.6378\n",
      "Epoch: 145, Loss: 14.5771\n",
      "Epoch: 146, Loss: 14.5978\n",
      "Epoch: 147, Loss: 14.5788\n",
      "Epoch: 148, Loss: 14.5612\n",
      "Epoch: 149, Loss: 14.5589\n",
      "Epoch: 150, Loss: 14.5406\n",
      "Epoch: 151, Loss: 14.5517\n",
      "Epoch: 152, Loss: 14.5534\n",
      "Epoch: 153, Loss: 14.5200\n",
      "Epoch: 154, Loss: 14.5357\n",
      "Epoch: 155, Loss: 14.5428\n",
      "Epoch: 156, Loss: 14.5084\n",
      "Epoch: 157, Loss: 14.5382\n",
      "Epoch: 158, Loss: 14.5910\n",
      "Epoch: 159, Loss: 14.5481\n",
      "Epoch: 160, Loss: 14.5815\n",
      "Epoch: 161, Loss: 14.5194\n",
      "Epoch: 162, Loss: 14.5235\n",
      "Epoch: 163, Loss: 14.5229\n",
      "Epoch: 164, Loss: 14.4916\n",
      "Epoch: 165, Loss: 14.4814\n",
      "Epoch: 166, Loss: 14.4770\n",
      "Epoch: 167, Loss: 14.4544\n",
      "Epoch: 168, Loss: 14.4596\n",
      "Epoch: 169, Loss: 14.4287\n",
      "Epoch: 170, Loss: 14.4414\n",
      "Epoch: 171, Loss: 14.4177\n",
      "Epoch: 172, Loss: 14.4237\n",
      "Epoch: 173, Loss: 14.4174\n",
      "Epoch: 174, Loss: 14.4388\n",
      "Epoch: 175, Loss: 14.5604\n",
      "Epoch: 176, Loss: 14.8266\n",
      "Epoch: 177, Loss: 15.0823\n",
      "Epoch: 178, Loss: 15.2256\n",
      "Epoch: 179, Loss: 17.5150\n",
      "Epoch: 180, Loss: 15.5513\n",
      "Epoch: 181, Loss: 16.1048\n",
      "Epoch: 182, Loss: 15.5280\n",
      "Epoch: 183, Loss: 15.3954\n",
      "Epoch: 184, Loss: 15.6897\n",
      "Epoch: 185, Loss: 15.5113\n",
      "Epoch: 186, Loss: 15.4379\n",
      "Epoch: 187, Loss: 15.2022\n",
      "Epoch: 188, Loss: 15.1923\n",
      "Epoch: 189, Loss: 15.2256\n",
      "Epoch: 190, Loss: 15.0882\n",
      "Epoch: 191, Loss: 15.0285\n",
      "Epoch: 192, Loss: 15.0178\n",
      "Epoch: 193, Loss: 14.9931\n",
      "Epoch: 194, Loss: 14.9547\n",
      "Epoch: 195, Loss: 14.9038\n",
      "Epoch: 196, Loss: 14.8499\n",
      "Epoch: 197, Loss: 14.8375\n",
      "Epoch: 198, Loss: 14.7893\n",
      "Epoch: 199, Loss: 14.7770\n",
      "Epoch: 200, Loss: 14.7434\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 201):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8898\n",
      "Training Accuracy: 0.8577\n"
     ]
    }
   ],
   "source": [
    "training_acc = test(torch_train_x, torch_train_y)\n",
    "print(f'Training Accuracy: {training_acc:.4f}')\n",
    "\n",
    "testing_acc = test(torch_test_x, torch_test_y)\n",
    "print(f'Training Accuracy: {testing_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Here's how you would parse the data for GNNs if edge features can be added\n",
    "################################################################################\n",
    "\n",
    "# node_prefix = [\"T1\", \"T2\", \"readout_error\"]\n",
    "# node_features_x = []\n",
    "# for k in range(df_train_x.shape[0]):\n",
    "#     node_features_j = []\n",
    "#     for j in range(num_qubits):\n",
    "#         row_features = df_train_x.iloc[k][[i + \"_\" + str(j) for i in node_prefix]].values.flatten()\n",
    "#         node_features_j.append(row_features)\n",
    "#     node_features_j = scaler.fit_transform(node_features_j)\n",
    "#     node_features_x.append(node_features_j)\n",
    "# node_features_x = np.array(node_features_x)\n",
    "\n",
    "# edge_prefix = [\"cx_\", \"edge_length_\", \"edge_error_\"]\n",
    "# edge_index = [[],[]]\n",
    "# edge_features_x = []\n",
    "# for k in range(df_train_x.shape[0]):\n",
    "#     edge_features_j = []\n",
    "#     for i in range(num_qubits):\n",
    "#         for j in range(num_qubits):\n",
    "#             if set([i,j]) in links:\n",
    "#                 row_features = df_train_x.iloc[k][[prefix + str(i) + str(j) for prefix in edge_prefix]].values.flatten()\n",
    "#                 edge_features_j.append(row_features)\n",
    "#                 if k == 0: # only need to do this once\n",
    "#                     edge_index[0].append(i)\n",
    "#                     edge_index[1].append(j)\n",
    "#     edge_features_j = scaler.fit_transform(edge_features_j)\n",
    "#     edge_features_x.append(edge_features_j)\n",
    "# edge_features_x = np.array(edge_features_x)\n",
    "# edge_index = np.array(edge_index)\n",
    "\n",
    "# node_labels = df_train_y.to_numpy()\n",
    "# print(node_features_x.shape, edge_index.shape, edge_features_x.shape, node_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7026, 7, 12) (2, 12) (7026, 49)\n",
      "(1757, 7, 12) (2, 12) (1757, 49)\n"
     ]
    }
   ],
   "source": [
    "# Since GCN doesn't support edge features, we can just concatenate them to the node features\n",
    "def get_datasets(df_x, df_y):\n",
    "    node_prefix = [\"T1\", \"T2\", \"readout_error\"]\n",
    "    edge_prefix = [\"cx\", \"edge_length\", \"edge_error\"]\n",
    "    node_features_x = []\n",
    "    edge_index = [[],[]]\n",
    "    for k in range(df_x.shape[0]):\n",
    "        node_features_i = []\n",
    "        for i in range(num_qubits):\n",
    "            node_features_i.append(list(df_train_x.iloc[k][[l + \"_\" + str(i) for l in node_prefix]].values.flatten()))\n",
    "            for j in range(num_qubits):\n",
    "                if set([i,j]) in links:\n",
    "                    node_features_i[i].extend(df_train_x.iloc[k][[l + \"_\" + str(i)+str(j) for l in edge_prefix]].values.flatten())\n",
    "                    if(k == 0): # only do this once\n",
    "                        edge_index[0].append(i)\n",
    "                        edge_index[1].append(j)\n",
    "            if(len(node_features_i[i]) < 12): # pad features to 12 with standard normal\n",
    "                node_features_i[i].extend(np.random.randn(12-len(node_features_i[i])))\n",
    "        node_features_i = scaler.fit_transform(node_features_i)\n",
    "        node_features_x.append(node_features_i)\n",
    "    node_features_x = np.array(node_features_x)\n",
    "    edge_index = np.array(edge_index)\n",
    "\n",
    "    node_labels = []\n",
    "    for _, row in df_y.iterrows():\n",
    "        node_labels.append(pd.get_dummies(row).values.flatten())\n",
    "    node_labels= np.array(node_labels)\n",
    "\n",
    "    return node_features_x, edge_index, node_labels\n",
    "\n",
    "train_gnn_x, edge_index, train_gnn_y = get_datasets(df_train_x, df_train_y)\n",
    "test_gnn_x, edge_index, test_gnn_y = get_datasets(df_test_x, df_test_y)\n",
    "print(train_gnn_x.shape, edge_index.shape, train_gnn_y.shape)\n",
    "print(test_gnn_x.shape, edge_index.shape, test_gnn_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(12, 12)\n",
      "  (conv2): GCNConv(12, 12)\n",
      "  (classifier): Linear(in_features=84, out_features=49, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(12, 12)\n",
    "        self.conv2 = GCNConv(12, 12)\n",
    "        self.classifier = Linear(84, 49)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = self.conv1(x, edge_index)\n",
    "        h = h.tanh()\n",
    "        h = self.conv2(h, edge_index)\n",
    "        h = h.tanh()\n",
    "        \n",
    "        # Apply a final (linear) classifier.\n",
    "        out = self.classifier(h.flatten(start_dim=1))\n",
    "\n",
    "        return out, h\n",
    "\n",
    "model = GCN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_train_gnn_x = torch.tensor(train_gnn_x, dtype=torch.float)\n",
    "torch_test_gnn_x = torch.tensor(test_gnn_x, dtype=torch.float)\n",
    "torch_edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "# torch_edge_features_x = torch.tensor(edge_features_x, dtype=torch.float)\n",
    "torch_train_gnn_y = torch.tensor(train_gnn_y, dtype=torch.float)\n",
    "torch_test_gnn_y = torch.tensor(test_gnn_y, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 27.118804931640625\n",
      "Epoch: 1, Loss: 19.647502899169922\n",
      "Epoch: 2, Loss: 19.865690231323242\n",
      "Epoch: 3, Loss: 21.975114822387695\n",
      "Epoch: 4, Loss: 20.44400978088379\n",
      "Epoch: 5, Loss: 18.982654571533203\n",
      "Epoch: 6, Loss: 19.71537208557129\n",
      "Epoch: 7, Loss: 17.776226043701172\n",
      "Epoch: 8, Loss: 18.031949996948242\n",
      "Epoch: 9, Loss: 17.96746826171875\n",
      "Epoch: 10, Loss: 17.93565559387207\n",
      "Epoch: 11, Loss: 17.92234230041504\n",
      "Epoch: 12, Loss: 17.103437423706055\n",
      "Epoch: 13, Loss: 17.012619018554688\n",
      "Epoch: 14, Loss: 17.175559997558594\n",
      "Epoch: 15, Loss: 17.24861717224121\n",
      "Epoch: 16, Loss: 17.076213836669922\n",
      "Epoch: 17, Loss: 16.87339973449707\n",
      "Epoch: 18, Loss: 16.71900749206543\n",
      "Epoch: 19, Loss: 16.68765640258789\n",
      "Epoch: 20, Loss: 16.723678588867188\n",
      "Epoch: 21, Loss: 16.712688446044922\n",
      "Epoch: 22, Loss: 16.64507484436035\n",
      "Epoch: 23, Loss: 16.52597999572754\n",
      "Epoch: 24, Loss: 16.4981689453125\n",
      "Epoch: 25, Loss: 16.489370346069336\n",
      "Epoch: 26, Loss: 16.53192138671875\n",
      "Epoch: 27, Loss: 16.499183654785156\n",
      "Epoch: 28, Loss: 16.45952606201172\n",
      "Epoch: 29, Loss: 16.395517349243164\n",
      "Epoch: 30, Loss: 16.374101638793945\n",
      "Epoch: 31, Loss: 16.364444732666016\n",
      "Epoch: 32, Loss: 16.362775802612305\n",
      "Epoch: 33, Loss: 16.341581344604492\n",
      "Epoch: 34, Loss: 16.313676834106445\n",
      "Epoch: 35, Loss: 16.275732040405273\n",
      "Epoch: 36, Loss: 16.257850646972656\n",
      "Epoch: 37, Loss: 16.242584228515625\n",
      "Epoch: 38, Loss: 16.241241455078125\n",
      "Epoch: 39, Loss: 16.230175018310547\n",
      "Epoch: 40, Loss: 16.2119140625\n",
      "Epoch: 41, Loss: 16.18527603149414\n",
      "Epoch: 42, Loss: 16.177377700805664\n",
      "Epoch: 43, Loss: 16.165668487548828\n",
      "Epoch: 44, Loss: 16.151199340820312\n",
      "Epoch: 45, Loss: 16.137304306030273\n",
      "Epoch: 46, Loss: 16.124826431274414\n",
      "Epoch: 47, Loss: 16.1186580657959\n",
      "Epoch: 48, Loss: 16.108366012573242\n",
      "Epoch: 49, Loss: 16.09893226623535\n",
      "Epoch: 50, Loss: 16.087656021118164\n",
      "Epoch: 51, Loss: 16.076265335083008\n",
      "Epoch: 52, Loss: 16.067848205566406\n",
      "Epoch: 53, Loss: 16.06061553955078\n",
      "Epoch: 54, Loss: 16.053377151489258\n",
      "Epoch: 55, Loss: 16.046201705932617\n",
      "Epoch: 56, Loss: 16.037220001220703\n",
      "Epoch: 57, Loss: 16.030698776245117\n",
      "Epoch: 58, Loss: 16.024648666381836\n",
      "Epoch: 59, Loss: 16.015432357788086\n",
      "Epoch: 60, Loss: 16.01009178161621\n",
      "Epoch: 61, Loss: 16.005197525024414\n",
      "Epoch: 62, Loss: 15.99897289276123\n",
      "Epoch: 63, Loss: 15.992615699768066\n",
      "Epoch: 64, Loss: 15.987006187438965\n",
      "Epoch: 65, Loss: 15.982463836669922\n",
      "Epoch: 66, Loss: 15.977724075317383\n",
      "Epoch: 67, Loss: 15.972755432128906\n",
      "Epoch: 68, Loss: 15.967630386352539\n",
      "Epoch: 69, Loss: 15.96296215057373\n",
      "Epoch: 70, Loss: 15.958471298217773\n",
      "Epoch: 71, Loss: 15.953864097595215\n",
      "Epoch: 72, Loss: 15.949628829956055\n",
      "Epoch: 73, Loss: 15.945591926574707\n",
      "Epoch: 74, Loss: 15.941109657287598\n",
      "Epoch: 75, Loss: 15.936644554138184\n",
      "Epoch: 76, Loss: 15.932073593139648\n",
      "Epoch: 77, Loss: 15.92764663696289\n",
      "Epoch: 78, Loss: 15.92375659942627\n",
      "Epoch: 79, Loss: 15.919731140136719\n",
      "Epoch: 80, Loss: 15.915688514709473\n",
      "Epoch: 81, Loss: 15.911688804626465\n",
      "Epoch: 82, Loss: 15.907623291015625\n",
      "Epoch: 83, Loss: 15.903779029846191\n",
      "Epoch: 84, Loss: 15.899927139282227\n",
      "Epoch: 85, Loss: 15.896095275878906\n",
      "Epoch: 86, Loss: 15.892297744750977\n",
      "Epoch: 87, Loss: 15.888569831848145\n",
      "Epoch: 88, Loss: 15.884840965270996\n",
      "Epoch: 89, Loss: 15.881237030029297\n",
      "Epoch: 90, Loss: 15.877605438232422\n",
      "Epoch: 91, Loss: 15.874131202697754\n",
      "Epoch: 92, Loss: 15.870607376098633\n",
      "Epoch: 93, Loss: 15.867239952087402\n",
      "Epoch: 94, Loss: 15.864005088806152\n",
      "Epoch: 95, Loss: 15.861102104187012\n",
      "Epoch: 96, Loss: 15.858710289001465\n",
      "Epoch: 97, Loss: 15.85803508758545\n",
      "Epoch: 98, Loss: 15.859777450561523\n",
      "Epoch: 99, Loss: 15.870683670043945\n",
      "Epoch: 100, Loss: 15.886216163635254\n",
      "Epoch: 101, Loss: 15.933379173278809\n",
      "Epoch: 102, Loss: 15.912681579589844\n",
      "Epoch: 103, Loss: 15.890856742858887\n",
      "Epoch: 104, Loss: 15.835369110107422\n",
      "Epoch: 105, Loss: 15.843982696533203\n",
      "Epoch: 106, Loss: 15.880666732788086\n",
      "Epoch: 107, Loss: 15.843015670776367\n",
      "Epoch: 108, Loss: 15.82068920135498\n",
      "Epoch: 109, Loss: 15.840283393859863\n",
      "Epoch: 110, Loss: 15.832169532775879\n",
      "Epoch: 111, Loss: 15.812132835388184\n",
      "Epoch: 112, Loss: 15.816471099853516\n",
      "Epoch: 113, Loss: 15.819581985473633\n",
      "Epoch: 114, Loss: 15.807146072387695\n",
      "Epoch: 115, Loss: 15.800222396850586\n",
      "Epoch: 116, Loss: 15.806525230407715\n",
      "Epoch: 117, Loss: 15.803520202636719\n",
      "Epoch: 118, Loss: 15.790027618408203\n",
      "Epoch: 119, Loss: 15.791254043579102\n",
      "Epoch: 120, Loss: 15.795642852783203\n",
      "Epoch: 121, Loss: 15.785704612731934\n",
      "Epoch: 122, Loss: 15.777177810668945\n",
      "Epoch: 123, Loss: 15.77816104888916\n",
      "Epoch: 124, Loss: 15.77946662902832\n",
      "Epoch: 125, Loss: 15.775018692016602\n",
      "Epoch: 126, Loss: 15.766684532165527\n",
      "Epoch: 127, Loss: 15.762299537658691\n",
      "Epoch: 128, Loss: 15.762296676635742\n",
      "Epoch: 129, Loss: 15.763025283813477\n",
      "Epoch: 130, Loss: 15.762675285339355\n",
      "Epoch: 131, Loss: 15.758265495300293\n",
      "Epoch: 132, Loss: 15.753911972045898\n",
      "Epoch: 133, Loss: 15.748406410217285\n",
      "Epoch: 134, Loss: 15.74432373046875\n",
      "Epoch: 135, Loss: 15.740525245666504\n",
      "Epoch: 136, Loss: 15.737388610839844\n",
      "Epoch: 137, Loss: 15.734466552734375\n",
      "Epoch: 138, Loss: 15.73190689086914\n",
      "Epoch: 139, Loss: 15.729412078857422\n",
      "Epoch: 140, Loss: 15.727364540100098\n",
      "Epoch: 141, Loss: 15.726677894592285\n",
      "Epoch: 142, Loss: 15.730483055114746\n",
      "Epoch: 143, Loss: 15.746416091918945\n",
      "Epoch: 144, Loss: 15.817687034606934\n",
      "Epoch: 145, Loss: 15.92129898071289\n",
      "Epoch: 146, Loss: 16.19344139099121\n",
      "Epoch: 147, Loss: 15.773301124572754\n",
      "Epoch: 148, Loss: 16.012229919433594\n",
      "Epoch: 149, Loss: 16.275436401367188\n",
      "Epoch: 150, Loss: 16.009458541870117\n",
      "Epoch: 151, Loss: 16.143783569335938\n",
      "Epoch: 152, Loss: 15.874736785888672\n",
      "Epoch: 153, Loss: 15.950228691101074\n",
      "Epoch: 154, Loss: 15.956573486328125\n",
      "Epoch: 155, Loss: 15.834270477294922\n",
      "Epoch: 156, Loss: 15.912567138671875\n",
      "Epoch: 157, Loss: 15.870908737182617\n",
      "Epoch: 158, Loss: 15.799198150634766\n",
      "Epoch: 159, Loss: 15.883641242980957\n",
      "Epoch: 160, Loss: 15.809208869934082\n",
      "Epoch: 161, Loss: 15.790188789367676\n",
      "Epoch: 162, Loss: 15.82900333404541\n",
      "Epoch: 163, Loss: 15.784470558166504\n",
      "Epoch: 164, Loss: 15.761551856994629\n",
      "Epoch: 165, Loss: 15.794147491455078\n",
      "Epoch: 166, Loss: 15.754534721374512\n",
      "Epoch: 167, Loss: 15.74682903289795\n",
      "Epoch: 168, Loss: 15.754931449890137\n",
      "Epoch: 169, Loss: 15.738119125366211\n",
      "Epoch: 170, Loss: 15.726322174072266\n",
      "Epoch: 171, Loss: 15.732467651367188\n",
      "Epoch: 172, Loss: 15.715993881225586\n",
      "Epoch: 173, Loss: 15.714387893676758\n",
      "Epoch: 174, Loss: 15.710288047790527\n",
      "Epoch: 175, Loss: 15.699400901794434\n",
      "Epoch: 176, Loss: 15.702763557434082\n",
      "Epoch: 177, Loss: 15.69257926940918\n",
      "Epoch: 178, Loss: 15.689005851745605\n",
      "Epoch: 179, Loss: 15.688091278076172\n",
      "Epoch: 180, Loss: 15.679919242858887\n",
      "Epoch: 181, Loss: 15.680249214172363\n",
      "Epoch: 182, Loss: 15.674901008605957\n",
      "Epoch: 183, Loss: 15.670594215393066\n",
      "Epoch: 184, Loss: 15.670783042907715\n",
      "Epoch: 185, Loss: 15.663796424865723\n",
      "Epoch: 186, Loss: 15.664538383483887\n",
      "Epoch: 187, Loss: 15.660252571105957\n",
      "Epoch: 188, Loss: 15.657418251037598\n",
      "Epoch: 189, Loss: 15.656879425048828\n",
      "Epoch: 190, Loss: 15.651897430419922\n",
      "Epoch: 191, Loss: 15.652493476867676\n",
      "Epoch: 192, Loss: 15.648180961608887\n",
      "Epoch: 193, Loss: 15.647237777709961\n",
      "Epoch: 194, Loss: 15.64469051361084\n",
      "Epoch: 195, Loss: 15.642411231994629\n",
      "Epoch: 196, Loss: 15.64108657836914\n",
      "Epoch: 197, Loss: 15.638689041137695\n",
      "Epoch: 198, Loss: 15.636992454528809\n",
      "Epoch: 199, Loss: 15.634700775146484\n",
      "Epoch: 200, Loss: 15.633489608764648\n",
      "Epoch: 201, Loss: 15.63102912902832\n",
      "Epoch: 202, Loss: 15.629650115966797\n",
      "Epoch: 203, Loss: 15.627572059631348\n",
      "Epoch: 204, Loss: 15.625946044921875\n",
      "Epoch: 205, Loss: 15.624272346496582\n",
      "Epoch: 206, Loss: 15.622148513793945\n",
      "Epoch: 207, Loss: 15.621068000793457\n",
      "Epoch: 208, Loss: 15.618783950805664\n",
      "Epoch: 209, Loss: 15.61750316619873\n",
      "Epoch: 210, Loss: 15.615699768066406\n",
      "Epoch: 211, Loss: 15.614163398742676\n",
      "Epoch: 212, Loss: 15.612526893615723\n",
      "Epoch: 213, Loss: 15.610899925231934\n",
      "Epoch: 214, Loss: 15.609346389770508\n",
      "Epoch: 215, Loss: 15.607721328735352\n",
      "Epoch: 216, Loss: 15.606278419494629\n",
      "Epoch: 217, Loss: 15.604716300964355\n",
      "Epoch: 218, Loss: 15.603246688842773\n",
      "Epoch: 219, Loss: 15.601720809936523\n",
      "Epoch: 220, Loss: 15.600292205810547\n",
      "Epoch: 221, Loss: 15.598803520202637\n",
      "Epoch: 222, Loss: 15.597524642944336\n",
      "Epoch: 223, Loss: 15.596367835998535\n",
      "Epoch: 224, Loss: 15.595953941345215\n",
      "Epoch: 225, Loss: 15.5979585647583\n",
      "Epoch: 226, Loss: 15.606554985046387\n",
      "Epoch: 227, Loss: 15.64223575592041\n",
      "Epoch: 228, Loss: 15.683635711669922\n",
      "Epoch: 229, Loss: 15.757932662963867\n",
      "Epoch: 230, Loss: 15.658306121826172\n",
      "Epoch: 231, Loss: 15.6862154006958\n",
      "Epoch: 232, Loss: 15.776538848876953\n",
      "Epoch: 233, Loss: 15.687087059020996\n",
      "Epoch: 234, Loss: 15.844669342041016\n",
      "Epoch: 235, Loss: 15.728168487548828\n",
      "Epoch: 236, Loss: 15.836668968200684\n",
      "Epoch: 237, Loss: 15.627699851989746\n",
      "Epoch: 238, Loss: 15.819303512573242\n",
      "Epoch: 239, Loss: 15.649909973144531\n",
      "Epoch: 240, Loss: 15.757308006286621\n",
      "Epoch: 241, Loss: 15.677024841308594\n",
      "Epoch: 242, Loss: 15.680122375488281\n",
      "Epoch: 243, Loss: 15.69573974609375\n",
      "Epoch: 244, Loss: 15.627431869506836\n",
      "Epoch: 245, Loss: 15.686064720153809\n",
      "Epoch: 246, Loss: 15.637768745422363\n",
      "Epoch: 247, Loss: 15.632436752319336\n",
      "Epoch: 248, Loss: 15.643718719482422\n",
      "Epoch: 249, Loss: 15.6196928024292\n",
      "Epoch: 250, Loss: 15.62694263458252\n",
      "Epoch: 251, Loss: 15.623029708862305\n",
      "Epoch: 252, Loss: 15.608391761779785\n",
      "Epoch: 253, Loss: 15.61318302154541\n",
      "Epoch: 254, Loss: 15.594880104064941\n",
      "Epoch: 255, Loss: 15.606529235839844\n",
      "Epoch: 256, Loss: 15.592073440551758\n",
      "Epoch: 257, Loss: 15.596182823181152\n",
      "Epoch: 258, Loss: 15.592854499816895\n",
      "Epoch: 259, Loss: 15.588236808776855\n",
      "Epoch: 260, Loss: 15.581925392150879\n",
      "Epoch: 261, Loss: 15.581332206726074\n",
      "Epoch: 262, Loss: 15.574989318847656\n",
      "Epoch: 263, Loss: 15.574397087097168\n",
      "Epoch: 264, Loss: 15.56988525390625\n",
      "Epoch: 265, Loss: 15.571800231933594\n",
      "Epoch: 266, Loss: 15.56624984741211\n",
      "Epoch: 267, Loss: 15.570836067199707\n",
      "Epoch: 268, Loss: 15.568368911743164\n",
      "Epoch: 269, Loss: 15.575695037841797\n",
      "Epoch: 270, Loss: 15.589349746704102\n",
      "Epoch: 271, Loss: 15.615592956542969\n",
      "Epoch: 272, Loss: 15.694320678710938\n",
      "Epoch: 273, Loss: 15.710070610046387\n",
      "Epoch: 274, Loss: 15.774803161621094\n",
      "Epoch: 275, Loss: 15.608026504516602\n",
      "Epoch: 276, Loss: 15.58919620513916\n",
      "Epoch: 277, Loss: 15.70088005065918\n",
      "Epoch: 278, Loss: 15.632814407348633\n",
      "Epoch: 279, Loss: 15.578385353088379\n",
      "Epoch: 280, Loss: 15.64465618133545\n",
      "Epoch: 281, Loss: 15.611250877380371\n",
      "Epoch: 282, Loss: 15.567358016967773\n",
      "Epoch: 283, Loss: 15.597479820251465\n",
      "Epoch: 284, Loss: 15.57582950592041\n",
      "Epoch: 285, Loss: 15.563862800598145\n",
      "Epoch: 286, Loss: 15.57394790649414\n",
      "Epoch: 287, Loss: 15.566435813903809\n",
      "Epoch: 288, Loss: 15.555574417114258\n",
      "Epoch: 289, Loss: 15.561899185180664\n",
      "Epoch: 290, Loss: 15.55335521697998\n",
      "Epoch: 291, Loss: 15.551538467407227\n",
      "Epoch: 292, Loss: 15.550808906555176\n",
      "Epoch: 293, Loss: 15.547286033630371\n",
      "Epoch: 294, Loss: 15.547183990478516\n",
      "Epoch: 295, Loss: 15.541465759277344\n",
      "Epoch: 296, Loss: 15.54431438446045\n",
      "Epoch: 297, Loss: 15.541873931884766\n",
      "Epoch: 298, Loss: 15.536947250366211\n",
      "Epoch: 299, Loss: 15.537569046020508\n",
      "Epoch: 300, Loss: 15.537559509277344\n",
      "Epoch: 301, Loss: 15.533902168273926\n",
      "Epoch: 302, Loss: 15.532209396362305\n",
      "Epoch: 303, Loss: 15.531007766723633\n",
      "Epoch: 304, Loss: 15.531266212463379\n",
      "Epoch: 305, Loss: 15.528234481811523\n",
      "Epoch: 306, Loss: 15.527841567993164\n",
      "Epoch: 307, Loss: 15.527227401733398\n",
      "Epoch: 308, Loss: 15.524065017700195\n",
      "Epoch: 309, Loss: 15.523754119873047\n",
      "Epoch: 310, Loss: 15.524362564086914\n",
      "Epoch: 311, Loss: 15.522171974182129\n",
      "Epoch: 312, Loss: 15.520759582519531\n",
      "Epoch: 313, Loss: 15.519177436828613\n",
      "Epoch: 314, Loss: 15.517646789550781\n",
      "Epoch: 315, Loss: 15.516850471496582\n",
      "Epoch: 316, Loss: 15.515174865722656\n",
      "Epoch: 317, Loss: 15.513409614562988\n",
      "Epoch: 318, Loss: 15.512824058532715\n",
      "Epoch: 319, Loss: 15.512499809265137\n",
      "Epoch: 320, Loss: 15.511957168579102\n",
      "Epoch: 321, Loss: 15.512569427490234\n",
      "Epoch: 322, Loss: 15.51730728149414\n",
      "Epoch: 323, Loss: 15.534232139587402\n",
      "Epoch: 324, Loss: 15.584864616394043\n",
      "Epoch: 325, Loss: 15.717329025268555\n",
      "Epoch: 326, Loss: 15.906249046325684\n",
      "Epoch: 327, Loss: 15.674100875854492\n",
      "Epoch: 328, Loss: 15.546257019042969\n",
      "Epoch: 329, Loss: 15.593794822692871\n",
      "Epoch: 330, Loss: 15.674690246582031\n",
      "Epoch: 331, Loss: 15.59583568572998\n",
      "Epoch: 332, Loss: 15.614645957946777\n",
      "Epoch: 333, Loss: 15.67707633972168\n",
      "Epoch: 334, Loss: 15.602502822875977\n",
      "Epoch: 335, Loss: 15.68938159942627\n",
      "Epoch: 336, Loss: 15.676584243774414\n",
      "Epoch: 337, Loss: 15.64969539642334\n",
      "Epoch: 338, Loss: 15.618426322937012\n",
      "Epoch: 339, Loss: 15.573331832885742\n",
      "Epoch: 340, Loss: 15.608423233032227\n",
      "Epoch: 341, Loss: 15.5768404006958\n",
      "Epoch: 342, Loss: 15.565874099731445\n",
      "Epoch: 343, Loss: 15.569918632507324\n",
      "Epoch: 344, Loss: 15.546738624572754\n",
      "Epoch: 345, Loss: 15.569330215454102\n",
      "Epoch: 346, Loss: 15.539770126342773\n",
      "Epoch: 347, Loss: 15.550097465515137\n",
      "Epoch: 348, Loss: 15.53504467010498\n",
      "Epoch: 349, Loss: 15.539837837219238\n",
      "Epoch: 350, Loss: 15.522611618041992\n",
      "Epoch: 351, Loss: 15.52605152130127\n",
      "Epoch: 352, Loss: 15.521941184997559\n",
      "Epoch: 353, Loss: 15.518593788146973\n",
      "Epoch: 354, Loss: 15.511993408203125\n",
      "Epoch: 355, Loss: 15.507772445678711\n",
      "Epoch: 356, Loss: 15.51366901397705\n",
      "Epoch: 357, Loss: 15.502116203308105\n",
      "Epoch: 358, Loss: 15.507330894470215\n",
      "Epoch: 359, Loss: 15.496794700622559\n",
      "Epoch: 360, Loss: 15.501649856567383\n",
      "Epoch: 361, Loss: 15.49589729309082\n",
      "Epoch: 362, Loss: 15.498169898986816\n",
      "Epoch: 363, Loss: 15.49190616607666\n",
      "Epoch: 364, Loss: 15.493715286254883\n",
      "Epoch: 365, Loss: 15.488917350769043\n",
      "Epoch: 366, Loss: 15.48901081085205\n",
      "Epoch: 367, Loss: 15.48720645904541\n",
      "Epoch: 368, Loss: 15.487557411193848\n",
      "Epoch: 369, Loss: 15.486592292785645\n",
      "Epoch: 370, Loss: 15.488537788391113\n",
      "Epoch: 371, Loss: 15.493431091308594\n",
      "Epoch: 372, Loss: 15.505620002746582\n",
      "Epoch: 373, Loss: 15.530868530273438\n",
      "Epoch: 374, Loss: 15.602323532104492\n",
      "Epoch: 375, Loss: 15.616220474243164\n",
      "Epoch: 376, Loss: 15.599135398864746\n",
      "Epoch: 377, Loss: 15.547516822814941\n",
      "Epoch: 378, Loss: 15.514228820800781\n",
      "Epoch: 379, Loss: 15.559283256530762\n",
      "Epoch: 380, Loss: 15.564321517944336\n",
      "Epoch: 381, Loss: 15.51296329498291\n",
      "Epoch: 382, Loss: 15.59302043914795\n",
      "Epoch: 383, Loss: 15.61233139038086\n",
      "Epoch: 384, Loss: 15.570590019226074\n",
      "Epoch: 385, Loss: 15.708967208862305\n",
      "Epoch: 386, Loss: 15.61855697631836\n",
      "Epoch: 387, Loss: 15.681950569152832\n",
      "Epoch: 388, Loss: 15.560430526733398\n",
      "Epoch: 389, Loss: 15.599364280700684\n",
      "Epoch: 390, Loss: 15.553850173950195\n",
      "Epoch: 391, Loss: 15.602110862731934\n",
      "Epoch: 392, Loss: 15.53227710723877\n",
      "Epoch: 393, Loss: 15.586169242858887\n",
      "Epoch: 394, Loss: 15.516581535339355\n",
      "Epoch: 395, Loss: 15.579161643981934\n",
      "Epoch: 396, Loss: 15.519492149353027\n",
      "Epoch: 397, Loss: 15.571958541870117\n",
      "Epoch: 398, Loss: 15.496696472167969\n",
      "Epoch: 399, Loss: 15.547123908996582\n",
      "Epoch: 400, Loss: 15.515727043151855\n"
     ]
    }
   ],
   "source": [
    "model = GCN()\n",
    "criterion = torch.nn.CrossEntropyLoss()  #Initialize the CrossEntropyLoss function.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1, weight_decay=5e-4)  # Initialize the Adam optimizer.\n",
    "\n",
    "def train(node_features, edge_index, node_labels):\n",
    "\n",
    "    optimizer.zero_grad()  # Clear gradients.\n",
    "    out, h = model(node_features, edge_index)  # Perform a single forward pass.\n",
    "    loss = criterion(out, node_labels)  # Compute the loss solely based on the training nodes.\n",
    "    loss.backward()  # Derive gradients.\n",
    "    optimizer.step()  # Update parameters based on gradients.\n",
    "\n",
    "    return loss, h\n",
    "\n",
    "\n",
    "for epoch in range(401):\n",
    "    #torch_edge_weight = torch.tensor(np.sum(edge_features_x, axis=2), dtype=torch.float)\n",
    "    #torch_edge_weight = torch.tensor(np.random.randn(12,1), dtype=torch.float)\n",
    "    loss, h = train(torch_train_gnn_x, torch_edge_index, torch_train_gnn_y)\n",
    "    print(f'Epoch: {epoch}, Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(x, e, y):\n",
    "    pred, h = model(x, e)\n",
    "\n",
    "    test_correct = 0\n",
    "    for i, j in zip(pred, y):\n",
    "        pred_i  = np.argmax(i.detach().numpy().reshape(7,7), axis=1)\n",
    "        label_j = np.argmax(j.detach().numpy().reshape(7,7), axis=1)\n",
    "        test_correct += np.array_equal(pred_i, label_j)\n",
    "        test_acc = int(test_correct) / int(y.shape[0])  # Derive ratio of correct predictions.\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7110731568460006"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(torch_train_gnn_x, torch_edge_index, torch_train_gnn_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40751280591918043"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(torch_test_gnn_x, torch_edge_index, torch_test_gnn_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical ML methods: RF-Gini and SVM-RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_svm_y(y):\n",
    "    svm_y = []\n",
    "    for i in y:\n",
    "        list_mapping = np.argmax(i.reshape(7,7), axis=1)\n",
    "        value = 0\n",
    "        for i in range(len(list_mapping)):\n",
    "            value += list_mapping[i]*num_qubits**i\n",
    "        svm_y.append(value)\n",
    "    return svm_y\n",
    "\n",
    "svm_train_y = get_svm_y(train_y)\n",
    "svm_test_y = get_svm_y(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "c_list = [1, 20, 40, 60, 80, 100, 500]\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "for c in c_list:\n",
    "    svc_model = SVC(kernel='rbf', C=c, gamma=1,decision_function_shape='ovo')\n",
    "    svc_model.fit(train_x, svm_train_y)\n",
    "    train_acc.append(svc_model.score(train_x, svm_train_y))\n",
    "    test_acc.append(svc_model.score(test_x, svm_test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6545687446626814 0.6346044393853159\n",
      "0.6601195559350982 0.6414342629482072\n",
      "0.6636777682892115 0.6448491747296529\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "n_estimators = [50, 500, 1000]\n",
    "for n_est in n_estimators:\n",
    "    clf = RandomForestClassifier(n_estimators=n_est, max_depth=2, random_state=0)\n",
    "    clf.fit(train_x, svm_train_y)\n",
    "    print(clf.score(train_x, svm_train_y), clf.score(test_x, svm_test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.2)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(c_list, test_acc, label=\"train\", marker='o')\n",
    "plt.xlabel(\"C\")\n",
    "plt.ylabel(\"Test Accuracy\")\n",
    "plt.ylim(0,1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eece571f",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
