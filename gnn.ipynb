{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "num_qubits = 7\n",
    "dataset_filename = \"dataset/dataset_tesi/NN1_Dataset(<=10Cx)_balanced1.csv\"\n",
    "df = pd.read_csv(dataset_filename)\n",
    "\n",
    "links = [set([0,1]), set([1,2]), set([1,3]), set([3,5]), set([4,5]), set([5,6])]\n",
    "def generate_columns(header, links, in_links=False):\n",
    "    if in_links:\n",
    "        return [header+str(i)+str(j) for i in range(num_qubits) for j in range(num_qubits) if set([i,j]) in links]\n",
    "    else:\n",
    "        return [header+str(i)+str(j) for i in range(num_qubits) for j in range(num_qubits) if set([i,j]) not in links and i!=j]\n",
    "\n",
    "useless_columns = ['Unnamed: 0', 'last_update_date', 'N_qubtis', 'N_measure', 'N_cx', 'backend_name']\n",
    "useless_columns += generate_columns(\"cx_\", links)\n",
    "useless_columns += generate_columns(\"edge_length_\", links)\n",
    "useless_columns += generate_columns(\"edge_error_\", links)\n",
    "useless_columns += [\"measure_\"+str(i) for i in range(num_qubits)]\n",
    "# Note that cx/edge_error/edge_length_xy is not neccessarily the same as cx/edge_length/edge_error_yx\n",
    "df.drop(columns=useless_columns, inplace=True)\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7026, 57), (7026, 49), (1757, 57), (1757, 49))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "df_train_x, df_train_y= train.iloc[:, :-num_qubits], train.iloc[:, -num_qubits:]\n",
    "df_test_x, df_test_y= test.iloc[:, :-num_qubits], test.iloc[:, -num_qubits:]\n",
    "\n",
    "train_x = scaler.fit_transform(df_train_x)\n",
    "test_x = scaler.fit_transform(df_test_x)\n",
    "\n",
    "\n",
    "# for every row in y, convert to 1 hot-encoding and flatten\n",
    "train_y = []\n",
    "for _, row in df_train_y.iterrows():\n",
    "    train_y.append(pd.get_dummies(row).values.flatten())\n",
    "train_y = np.array(train_y)\n",
    "\n",
    "test_y = []\n",
    "for _, row in df_test_y.iterrows():\n",
    "    test_y.append(pd.get_dummies(row).values.flatten())\n",
    "test_y = np.array(test_y)\n",
    "\n",
    "(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacky/anaconda3/envs/eece571f/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.lin1 = Linear(num_features, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin3 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin4 = Linear(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = x.relu()\n",
    "        x = self.lin2(x)\n",
    "        x = x.relu()\n",
    "        x = self.lin3(x)\n",
    "        x = x.relu()\n",
    "        x = self.lin4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(57, 49, hidden_channels=128)\n",
    "criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.03, weight_decay=5e-4)  # Define optimizer.\n",
    "\n",
    "torch_train_x = torch.tensor(train_x, dtype=torch.float)\n",
    "torch_train_y = torch.tensor(train_y, dtype=torch.float)\n",
    "torch_test_x = torch.tensor(test_x, dtype=torch.float)\n",
    "torch_test_y = torch.tensor(test_y, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(torch_train_x)  # Perform a single forward pass.\n",
    "      loss = criterion(out, torch_train_y)  # Compute the loss solely based on the training nodes.\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test(x, y):\n",
    "      model.eval()\n",
    "      pred = model(x)\n",
    "\n",
    "      test_correct = 0\n",
    "      for i, j in zip(pred, y):\n",
    "          pred_i = np.argmax(i.detach().numpy().reshape(7,7), axis=1)\n",
    "          label_j = np.argmax(j.detach().numpy().reshape(7,7), axis=1)\n",
    "          test_correct += np.array_equal(pred_i, label_j)\n",
    "      test_acc = int(test_correct) / int(y.shape[0])  # Derive ratio of correct predictions.\n",
    "      return test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 27.0800\n",
      "Epoch: 002, Loss: 21.9036\n",
      "Epoch: 003, Loss: 21.3844\n",
      "Epoch: 004, Loss: 20.5604\n",
      "Epoch: 005, Loss: 19.8649\n",
      "Epoch: 006, Loss: 19.5504\n",
      "Epoch: 007, Loss: 19.3385\n",
      "Epoch: 008, Loss: 18.7612\n",
      "Epoch: 009, Loss: 18.3172\n",
      "Epoch: 010, Loss: 17.9141\n",
      "Epoch: 011, Loss: 17.4990\n",
      "Epoch: 012, Loss: 17.2550\n",
      "Epoch: 013, Loss: 17.1155\n",
      "Epoch: 014, Loss: 17.0426\n",
      "Epoch: 015, Loss: 17.0478\n",
      "Epoch: 016, Loss: 17.0295\n",
      "Epoch: 017, Loss: 16.9399\n",
      "Epoch: 018, Loss: 16.8995\n",
      "Epoch: 019, Loss: 16.8222\n",
      "Epoch: 020, Loss: 16.7528\n",
      "Epoch: 021, Loss: 16.6812\n",
      "Epoch: 022, Loss: 16.6238\n",
      "Epoch: 023, Loss: 16.5539\n",
      "Epoch: 024, Loss: 16.4822\n",
      "Epoch: 025, Loss: 16.4323\n",
      "Epoch: 026, Loss: 16.3780\n",
      "Epoch: 027, Loss: 16.3590\n",
      "Epoch: 028, Loss: 16.3174\n",
      "Epoch: 029, Loss: 16.2600\n",
      "Epoch: 030, Loss: 16.2344\n",
      "Epoch: 031, Loss: 16.1893\n",
      "Epoch: 032, Loss: 16.1512\n",
      "Epoch: 033, Loss: 16.1436\n",
      "Epoch: 034, Loss: 16.1210\n",
      "Epoch: 035, Loss: 16.0966\n",
      "Epoch: 036, Loss: 16.0782\n",
      "Epoch: 037, Loss: 16.0537\n",
      "Epoch: 038, Loss: 16.0302\n",
      "Epoch: 039, Loss: 16.0262\n",
      "Epoch: 040, Loss: 16.0264\n",
      "Epoch: 041, Loss: 15.9744\n",
      "Epoch: 042, Loss: 15.9355\n",
      "Epoch: 043, Loss: 15.9705\n",
      "Epoch: 044, Loss: 16.0905\n",
      "Epoch: 045, Loss: 15.9119\n",
      "Epoch: 046, Loss: 15.9134\n",
      "Epoch: 047, Loss: 15.8925\n",
      "Epoch: 048, Loss: 15.7734\n",
      "Epoch: 049, Loss: 15.8487\n",
      "Epoch: 050, Loss: 15.7073\n",
      "Epoch: 051, Loss: 15.6873\n",
      "Epoch: 052, Loss: 15.6665\n",
      "Epoch: 053, Loss: 15.5709\n",
      "Epoch: 054, Loss: 15.5178\n",
      "Epoch: 055, Loss: 15.5580\n",
      "Epoch: 056, Loss: 15.6395\n",
      "Epoch: 057, Loss: 15.4488\n",
      "Epoch: 058, Loss: 15.4497\n",
      "Epoch: 059, Loss: 15.5807\n",
      "Epoch: 060, Loss: 15.4349\n",
      "Epoch: 061, Loss: 15.4125\n",
      "Epoch: 062, Loss: 15.3639\n",
      "Epoch: 063, Loss: 15.3457\n",
      "Epoch: 064, Loss: 15.3297\n",
      "Epoch: 065, Loss: 15.2980\n",
      "Epoch: 066, Loss: 15.2930\n",
      "Epoch: 067, Loss: 15.2185\n",
      "Epoch: 068, Loss: 15.2213\n",
      "Epoch: 069, Loss: 15.1990\n",
      "Epoch: 070, Loss: 15.2103\n",
      "Epoch: 071, Loss: 15.1419\n",
      "Epoch: 072, Loss: 15.1382\n",
      "Epoch: 073, Loss: 15.1409\n",
      "Epoch: 074, Loss: 15.0858\n",
      "Epoch: 075, Loss: 15.1488\n",
      "Epoch: 076, Loss: 15.1982\n",
      "Epoch: 077, Loss: 15.3702\n",
      "Epoch: 078, Loss: 15.1955\n",
      "Epoch: 079, Loss: 15.1405\n",
      "Epoch: 080, Loss: 15.2460\n",
      "Epoch: 081, Loss: 15.1418\n",
      "Epoch: 082, Loss: 15.1669\n",
      "Epoch: 083, Loss: 15.1009\n",
      "Epoch: 084, Loss: 15.0701\n",
      "Epoch: 085, Loss: 15.0600\n",
      "Epoch: 086, Loss: 15.0387\n",
      "Epoch: 087, Loss: 15.0259\n",
      "Epoch: 088, Loss: 14.9998\n",
      "Epoch: 089, Loss: 14.9913\n",
      "Epoch: 090, Loss: 14.9840\n",
      "Epoch: 091, Loss: 14.9498\n",
      "Epoch: 092, Loss: 14.9391\n",
      "Epoch: 093, Loss: 14.9323\n",
      "Epoch: 094, Loss: 14.9518\n",
      "Epoch: 095, Loss: 14.9497\n",
      "Epoch: 096, Loss: 14.9909\n",
      "Epoch: 097, Loss: 14.9849\n",
      "Epoch: 098, Loss: 15.0099\n",
      "Epoch: 099, Loss: 14.9286\n",
      "Epoch: 100, Loss: 14.9146\n",
      "Epoch: 101, Loss: 14.9266\n",
      "Epoch: 102, Loss: 14.8861\n",
      "Epoch: 103, Loss: 14.8761\n",
      "Epoch: 104, Loss: 14.8747\n",
      "Epoch: 105, Loss: 14.8305\n",
      "Epoch: 106, Loss: 14.8398\n",
      "Epoch: 107, Loss: 14.8412\n",
      "Epoch: 108, Loss: 14.9157\n",
      "Epoch: 109, Loss: 14.9811\n",
      "Epoch: 110, Loss: 14.8265\n",
      "Epoch: 111, Loss: 14.8752\n",
      "Epoch: 112, Loss: 14.8231\n",
      "Epoch: 113, Loss: 14.8312\n",
      "Epoch: 114, Loss: 14.8122\n",
      "Epoch: 115, Loss: 14.8134\n",
      "Epoch: 116, Loss: 14.8681\n",
      "Epoch: 117, Loss: 14.9722\n",
      "Epoch: 118, Loss: 15.1944\n",
      "Epoch: 119, Loss: 15.0581\n",
      "Epoch: 120, Loss: 14.8966\n",
      "Epoch: 121, Loss: 15.0533\n",
      "Epoch: 122, Loss: 14.9994\n",
      "Epoch: 123, Loss: 14.9090\n",
      "Epoch: 124, Loss: 14.9346\n",
      "Epoch: 125, Loss: 14.9034\n",
      "Epoch: 126, Loss: 14.8962\n",
      "Epoch: 127, Loss: 14.8691\n",
      "Epoch: 128, Loss: 14.8606\n",
      "Epoch: 129, Loss: 14.8336\n",
      "Epoch: 130, Loss: 14.8465\n",
      "Epoch: 131, Loss: 14.8014\n",
      "Epoch: 132, Loss: 14.8042\n",
      "Epoch: 133, Loss: 14.7908\n",
      "Epoch: 134, Loss: 14.7511\n",
      "Epoch: 135, Loss: 14.7490\n",
      "Epoch: 136, Loss: 14.7650\n",
      "Epoch: 137, Loss: 14.7170\n",
      "Epoch: 138, Loss: 14.7312\n",
      "Epoch: 139, Loss: 14.7699\n",
      "Epoch: 140, Loss: 14.7541\n",
      "Epoch: 141, Loss: 14.7845\n",
      "Epoch: 142, Loss: 14.7372\n",
      "Epoch: 143, Loss: 14.6687\n",
      "Epoch: 144, Loss: 14.7174\n",
      "Epoch: 145, Loss: 14.6884\n",
      "Epoch: 146, Loss: 14.6563\n",
      "Epoch: 147, Loss: 14.6672\n",
      "Epoch: 148, Loss: 14.6164\n",
      "Epoch: 149, Loss: 14.6387\n",
      "Epoch: 150, Loss: 14.6248\n",
      "Epoch: 151, Loss: 14.5895\n",
      "Epoch: 152, Loss: 14.6106\n",
      "Epoch: 153, Loss: 14.5766\n",
      "Epoch: 154, Loss: 14.5977\n",
      "Epoch: 155, Loss: 14.7166\n",
      "Epoch: 156, Loss: 14.7443\n",
      "Epoch: 157, Loss: 15.4330\n",
      "Epoch: 158, Loss: 14.8419\n",
      "Epoch: 159, Loss: 15.4665\n",
      "Epoch: 160, Loss: 15.7309\n",
      "Epoch: 161, Loss: 15.4032\n",
      "Epoch: 162, Loss: 15.0569\n",
      "Epoch: 163, Loss: 15.3135\n",
      "Epoch: 164, Loss: 15.2748\n",
      "Epoch: 165, Loss: 15.0643\n",
      "Epoch: 166, Loss: 15.1492\n",
      "Epoch: 167, Loss: 15.1039\n",
      "Epoch: 168, Loss: 14.9728\n",
      "Epoch: 169, Loss: 15.0047\n",
      "Epoch: 170, Loss: 14.9792\n",
      "Epoch: 171, Loss: 14.9121\n",
      "Epoch: 172, Loss: 14.8804\n",
      "Epoch: 173, Loss: 14.9018\n",
      "Epoch: 174, Loss: 14.8081\n",
      "Epoch: 175, Loss: 14.7890\n",
      "Epoch: 176, Loss: 14.8072\n",
      "Epoch: 177, Loss: 14.7676\n",
      "Epoch: 178, Loss: 14.7249\n",
      "Epoch: 179, Loss: 14.7563\n",
      "Epoch: 180, Loss: 14.6977\n",
      "Epoch: 181, Loss: 14.6853\n",
      "Epoch: 182, Loss: 14.6778\n",
      "Epoch: 183, Loss: 14.6415\n",
      "Epoch: 184, Loss: 14.6360\n",
      "Epoch: 185, Loss: 14.6169\n",
      "Epoch: 186, Loss: 14.5993\n",
      "Epoch: 187, Loss: 14.5905\n",
      "Epoch: 188, Loss: 14.5614\n",
      "Epoch: 189, Loss: 14.5537\n",
      "Epoch: 190, Loss: 14.5400\n",
      "Epoch: 191, Loss: 14.5266\n",
      "Epoch: 192, Loss: 14.5120\n",
      "Epoch: 193, Loss: 14.5056\n",
      "Epoch: 194, Loss: 14.4890\n",
      "Epoch: 195, Loss: 14.4823\n",
      "Epoch: 196, Loss: 14.4753\n",
      "Epoch: 197, Loss: 14.4677\n",
      "Epoch: 198, Loss: 14.4648\n",
      "Epoch: 199, Loss: 14.4670\n",
      "Epoch: 200, Loss: 14.4724\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 201):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9069\n",
      "Training Accuracy: 0.8645\n"
     ]
    }
   ],
   "source": [
    "training_acc = test(torch_train_x, torch_train_y)\n",
    "print(f'Training Accuracy: {training_acc:.4f}')\n",
    "\n",
    "testing_acc = test(torch_test_x, torch_test_y)\n",
    "print(f'Training Accuracy: {testing_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_prefix = [\"T1\", \"T2\", \"readout_error\"]\n",
    "node_features_x = []\n",
    "for k in range(df_train_x.shape[0]):\n",
    "    node_features_j = []\n",
    "    for j in range(num_qubits):\n",
    "        row_features = df_train_x.iloc[k][[i + \"_\" + str(j) for i in node_prefix]].values.flatten()\n",
    "        node_features_j.append(row_features)\n",
    "    node_features_j = scaler.fit_transform(node_features_j)\n",
    "    node_features_x.append(node_features_j)\n",
    "node_features_x = np.array(node_features_x)\n",
    "\n",
    "edge_prefix = [\"cx_\", \"edge_length_\", \"edge_error_\"]\n",
    "edge_index = [[],[]]\n",
    "edge_features_x = []\n",
    "for k in range(df_train_x.shape[0]):\n",
    "    edge_features_j = []\n",
    "    for i in range(num_qubits):\n",
    "        for j in range(num_qubits):\n",
    "            if set([i,j]) in links:\n",
    "                row_features = df_train_x.iloc[k][[prefix + str(i) + str(j) for prefix in edge_prefix]].values.flatten()\n",
    "                edge_features_j.append(row_features)\n",
    "                if k == 0: # only need to do this once\n",
    "                    edge_index[0].append(i)\n",
    "                    edge_index[1].append(j)\n",
    "    edge_features_j = scaler.fit_transform(edge_features_j)\n",
    "    edge_features_x.append(edge_features_j)\n",
    "edge_features_x = np.array(edge_features_x)\n",
    "edge_index = np.array(edge_index)\n",
    "\n",
    "node_labels = df_train_y.to_numpy()\n",
    "print(node_features_x.shape, edge_index.shape, edge_features_x.shape, node_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(3, 2)\n",
    "        self.conv2 = GCNConv(2, 2)\n",
    "        self.conv3 = GCNConv(2, 1)\n",
    "        self.classifier = Linear(1, 7)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        h = self.conv1(x, edge_index, edge_attr)\n",
    "        h = h.tanh()\n",
    "        h = self.conv2(h, edge_index, edge_attr)\n",
    "        h = h.tanh()\n",
    "        h = self.conv3(h, edge_index, edge_attr)\n",
    "        h = h.tanh()  # Final GNN embedding space.\n",
    "        \n",
    "        # Apply a final (linear) classifier.\n",
    "        out = self.classifier(h)\n",
    "\n",
    "        return out, h\n",
    "\n",
    "model = GCN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_node_features_x = torch.tensor(node_features_x, dtype=torch.float)\n",
    "torch_edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "torch_edge_features_x = torch.tensor(edge_features_x, dtype=torch.float)\n",
    "torch_node_labels = torch.tensor(node_labels, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: nan\n",
      "Epoch: 1, Loss: nan\n",
      "Epoch: 2, Loss: nan\n",
      "Epoch: 3, Loss: nan\n",
      "Epoch: 4, Loss: nan\n",
      "Epoch: 5, Loss: nan\n",
      "Epoch: 6, Loss: nan\n",
      "Epoch: 7, Loss: nan\n",
      "Epoch: 8, Loss: nan\n",
      "Epoch: 9, Loss: nan\n",
      "Epoch: 10, Loss: nan\n",
      "Epoch: 11, Loss: nan\n",
      "Epoch: 12, Loss: nan\n",
      "Epoch: 13, Loss: nan\n",
      "Epoch: 14, Loss: nan\n",
      "Epoch: 15, Loss: nan\n",
      "Epoch: 16, Loss: nan\n",
      "Epoch: 17, Loss: nan\n",
      "Epoch: 18, Loss: nan\n",
      "Epoch: 19, Loss: nan\n",
      "Epoch: 20, Loss: nan\n",
      "Epoch: 21, Loss: nan\n",
      "Epoch: 22, Loss: nan\n",
      "Epoch: 23, Loss: nan\n",
      "Epoch: 24, Loss: nan\n",
      "Epoch: 25, Loss: nan\n",
      "Epoch: 26, Loss: nan\n",
      "Epoch: 27, Loss: nan\n",
      "Epoch: 28, Loss: nan\n",
      "Epoch: 29, Loss: nan\n",
      "Epoch: 30, Loss: nan\n",
      "Epoch: 31, Loss: nan\n",
      "Epoch: 32, Loss: nan\n",
      "Epoch: 33, Loss: nan\n",
      "Epoch: 34, Loss: nan\n",
      "Epoch: 35, Loss: nan\n",
      "Epoch: 36, Loss: nan\n",
      "Epoch: 37, Loss: nan\n",
      "Epoch: 38, Loss: nan\n",
      "Epoch: 39, Loss: nan\n",
      "Epoch: 40, Loss: nan\n",
      "Epoch: 41, Loss: nan\n",
      "Epoch: 42, Loss: nan\n",
      "Epoch: 43, Loss: nan\n",
      "Epoch: 44, Loss: nan\n",
      "Epoch: 45, Loss: nan\n",
      "Epoch: 46, Loss: nan\n",
      "Epoch: 47, Loss: nan\n",
      "Epoch: 48, Loss: nan\n",
      "Epoch: 49, Loss: nan\n",
      "Epoch: 50, Loss: nan\n",
      "Epoch: 51, Loss: nan\n",
      "Epoch: 52, Loss: nan\n",
      "Epoch: 53, Loss: nan\n",
      "Epoch: 54, Loss: nan\n",
      "Epoch: 55, Loss: nan\n",
      "Epoch: 56, Loss: nan\n",
      "Epoch: 57, Loss: nan\n",
      "Epoch: 58, Loss: nan\n",
      "Epoch: 59, Loss: nan\n",
      "Epoch: 60, Loss: nan\n",
      "Epoch: 61, Loss: nan\n",
      "Epoch: 62, Loss: nan\n",
      "Epoch: 63, Loss: nan\n",
      "Epoch: 64, Loss: nan\n",
      "Epoch: 65, Loss: nan\n",
      "Epoch: 66, Loss: nan\n",
      "Epoch: 67, Loss: nan\n",
      "Epoch: 68, Loss: nan\n",
      "Epoch: 69, Loss: nan\n",
      "Epoch: 70, Loss: nan\n",
      "Epoch: 71, Loss: nan\n",
      "Epoch: 72, Loss: nan\n",
      "Epoch: 73, Loss: nan\n",
      "Epoch: 74, Loss: nan\n",
      "Epoch: 75, Loss: nan\n",
      "Epoch: 76, Loss: nan\n",
      "Epoch: 77, Loss: nan\n",
      "Epoch: 78, Loss: nan\n",
      "Epoch: 79, Loss: nan\n",
      "Epoch: 80, Loss: nan\n",
      "Epoch: 81, Loss: nan\n",
      "Epoch: 82, Loss: nan\n",
      "Epoch: 83, Loss: nan\n",
      "Epoch: 84, Loss: nan\n",
      "Epoch: 85, Loss: nan\n",
      "Epoch: 86, Loss: nan\n",
      "Epoch: 87, Loss: nan\n",
      "Epoch: 88, Loss: nan\n",
      "Epoch: 89, Loss: nan\n",
      "Epoch: 90, Loss: nan\n",
      "Epoch: 91, Loss: nan\n",
      "Epoch: 92, Loss: nan\n",
      "Epoch: 93, Loss: nan\n",
      "Epoch: 94, Loss: nan\n",
      "Epoch: 95, Loss: nan\n",
      "Epoch: 96, Loss: nan\n",
      "Epoch: 97, Loss: nan\n",
      "Epoch: 98, Loss: nan\n",
      "Epoch: 99, Loss: nan\n",
      "Epoch: 100, Loss: nan\n",
      "Epoch: 101, Loss: nan\n",
      "Epoch: 102, Loss: nan\n",
      "Epoch: 103, Loss: nan\n",
      "Epoch: 104, Loss: nan\n",
      "Epoch: 105, Loss: nan\n",
      "Epoch: 106, Loss: nan\n",
      "Epoch: 107, Loss: nan\n",
      "Epoch: 108, Loss: nan\n",
      "Epoch: 109, Loss: nan\n",
      "Epoch: 110, Loss: nan\n",
      "Epoch: 111, Loss: nan\n",
      "Epoch: 112, Loss: nan\n",
      "Epoch: 113, Loss: nan\n",
      "Epoch: 114, Loss: nan\n",
      "Epoch: 115, Loss: nan\n",
      "Epoch: 116, Loss: nan\n",
      "Epoch: 117, Loss: nan\n",
      "Epoch: 118, Loss: nan\n",
      "Epoch: 119, Loss: nan\n",
      "Epoch: 120, Loss: nan\n",
      "Epoch: 121, Loss: nan\n",
      "Epoch: 122, Loss: nan\n",
      "Epoch: 123, Loss: nan\n",
      "Epoch: 124, Loss: nan\n",
      "Epoch: 125, Loss: nan\n",
      "Epoch: 126, Loss: nan\n",
      "Epoch: 127, Loss: nan\n",
      "Epoch: 128, Loss: nan\n",
      "Epoch: 129, Loss: nan\n",
      "Epoch: 130, Loss: nan\n",
      "Epoch: 131, Loss: nan\n",
      "Epoch: 132, Loss: nan\n",
      "Epoch: 133, Loss: nan\n",
      "Epoch: 134, Loss: nan\n",
      "Epoch: 135, Loss: nan\n",
      "Epoch: 136, Loss: nan\n",
      "Epoch: 137, Loss: nan\n",
      "Epoch: 138, Loss: nan\n",
      "Epoch: 139, Loss: nan\n",
      "Epoch: 140, Loss: nan\n",
      "Epoch: 141, Loss: nan\n",
      "Epoch: 142, Loss: nan\n",
      "Epoch: 143, Loss: nan\n",
      "Epoch: 144, Loss: nan\n",
      "Epoch: 145, Loss: nan\n",
      "Epoch: 146, Loss: nan\n",
      "Epoch: 147, Loss: nan\n",
      "Epoch: 148, Loss: nan\n",
      "Epoch: 149, Loss: nan\n",
      "Epoch: 150, Loss: nan\n",
      "Epoch: 151, Loss: nan\n",
      "Epoch: 152, Loss: nan\n",
      "Epoch: 153, Loss: nan\n",
      "Epoch: 154, Loss: nan\n",
      "Epoch: 155, Loss: nan\n",
      "Epoch: 156, Loss: nan\n",
      "Epoch: 157, Loss: nan\n",
      "Epoch: 158, Loss: nan\n",
      "Epoch: 159, Loss: nan\n",
      "Epoch: 160, Loss: nan\n",
      "Epoch: 161, Loss: nan\n",
      "Epoch: 162, Loss: nan\n",
      "Epoch: 163, Loss: nan\n",
      "Epoch: 164, Loss: nan\n",
      "Epoch: 165, Loss: nan\n",
      "Epoch: 166, Loss: nan\n",
      "Epoch: 167, Loss: nan\n",
      "Epoch: 168, Loss: nan\n",
      "Epoch: 169, Loss: nan\n",
      "Epoch: 170, Loss: nan\n",
      "Epoch: 171, Loss: nan\n",
      "Epoch: 172, Loss: nan\n",
      "Epoch: 173, Loss: nan\n",
      "Epoch: 174, Loss: nan\n",
      "Epoch: 175, Loss: nan\n",
      "Epoch: 176, Loss: nan\n",
      "Epoch: 177, Loss: nan\n",
      "Epoch: 178, Loss: nan\n",
      "Epoch: 179, Loss: nan\n",
      "Epoch: 180, Loss: nan\n",
      "Epoch: 181, Loss: nan\n",
      "Epoch: 182, Loss: nan\n",
      "Epoch: 183, Loss: nan\n",
      "Epoch: 184, Loss: nan\n",
      "Epoch: 185, Loss: nan\n",
      "Epoch: 186, Loss: nan\n",
      "Epoch: 187, Loss: nan\n",
      "Epoch: 188, Loss: nan\n",
      "Epoch: 189, Loss: nan\n",
      "Epoch: 190, Loss: nan\n",
      "Epoch: 191, Loss: nan\n",
      "Epoch: 192, Loss: nan\n",
      "Epoch: 193, Loss: nan\n",
      "Epoch: 194, Loss: nan\n",
      "Epoch: 195, Loss: nan\n",
      "Epoch: 196, Loss: nan\n",
      "Epoch: 197, Loss: nan\n",
      "Epoch: 198, Loss: nan\n",
      "Epoch: 199, Loss: nan\n",
      "Epoch: 200, Loss: nan\n",
      "Epoch: 201, Loss: nan\n",
      "Epoch: 202, Loss: nan\n",
      "Epoch: 203, Loss: nan\n",
      "Epoch: 204, Loss: nan\n",
      "Epoch: 205, Loss: nan\n",
      "Epoch: 206, Loss: nan\n",
      "Epoch: 207, Loss: nan\n",
      "Epoch: 208, Loss: nan\n",
      "Epoch: 209, Loss: nan\n",
      "Epoch: 210, Loss: nan\n",
      "Epoch: 211, Loss: nan\n",
      "Epoch: 212, Loss: nan\n",
      "Epoch: 213, Loss: nan\n",
      "Epoch: 214, Loss: nan\n",
      "Epoch: 215, Loss: nan\n",
      "Epoch: 216, Loss: nan\n",
      "Epoch: 217, Loss: nan\n",
      "Epoch: 218, Loss: nan\n",
      "Epoch: 219, Loss: nan\n",
      "Epoch: 220, Loss: nan\n",
      "Epoch: 221, Loss: nan\n",
      "Epoch: 222, Loss: nan\n",
      "Epoch: 223, Loss: nan\n",
      "Epoch: 224, Loss: nan\n",
      "Epoch: 225, Loss: nan\n",
      "Epoch: 226, Loss: nan\n",
      "Epoch: 227, Loss: nan\n",
      "Epoch: 228, Loss: nan\n",
      "Epoch: 229, Loss: nan\n",
      "Epoch: 230, Loss: nan\n",
      "Epoch: 231, Loss: nan\n",
      "Epoch: 232, Loss: nan\n",
      "Epoch: 233, Loss: nan\n",
      "Epoch: 234, Loss: nan\n",
      "Epoch: 235, Loss: nan\n",
      "Epoch: 236, Loss: nan\n",
      "Epoch: 237, Loss: nan\n",
      "Epoch: 238, Loss: nan\n",
      "Epoch: 239, Loss: nan\n",
      "Epoch: 240, Loss: nan\n",
      "Epoch: 241, Loss: nan\n",
      "Epoch: 242, Loss: nan\n",
      "Epoch: 243, Loss: nan\n",
      "Epoch: 244, Loss: nan\n",
      "Epoch: 245, Loss: nan\n",
      "Epoch: 246, Loss: nan\n",
      "Epoch: 247, Loss: nan\n",
      "Epoch: 248, Loss: nan\n",
      "Epoch: 249, Loss: nan\n",
      "Epoch: 250, Loss: nan\n",
      "Epoch: 251, Loss: nan\n",
      "Epoch: 252, Loss: nan\n",
      "Epoch: 253, Loss: nan\n",
      "Epoch: 254, Loss: nan\n",
      "Epoch: 255, Loss: nan\n",
      "Epoch: 256, Loss: nan\n",
      "Epoch: 257, Loss: nan\n",
      "Epoch: 258, Loss: nan\n",
      "Epoch: 259, Loss: nan\n",
      "Epoch: 260, Loss: nan\n",
      "Epoch: 261, Loss: nan\n",
      "Epoch: 262, Loss: nan\n",
      "Epoch: 263, Loss: nan\n",
      "Epoch: 264, Loss: nan\n",
      "Epoch: 265, Loss: nan\n",
      "Epoch: 266, Loss: nan\n",
      "Epoch: 267, Loss: nan\n",
      "Epoch: 268, Loss: nan\n",
      "Epoch: 269, Loss: nan\n",
      "Epoch: 270, Loss: nan\n",
      "Epoch: 271, Loss: nan\n",
      "Epoch: 272, Loss: nan\n",
      "Epoch: 273, Loss: nan\n",
      "Epoch: 274, Loss: nan\n",
      "Epoch: 275, Loss: nan\n",
      "Epoch: 276, Loss: nan\n",
      "Epoch: 277, Loss: nan\n",
      "Epoch: 278, Loss: nan\n",
      "Epoch: 279, Loss: nan\n",
      "Epoch: 280, Loss: nan\n",
      "Epoch: 281, Loss: nan\n",
      "Epoch: 282, Loss: nan\n",
      "Epoch: 283, Loss: nan\n",
      "Epoch: 284, Loss: nan\n",
      "Epoch: 285, Loss: nan\n",
      "Epoch: 286, Loss: nan\n",
      "Epoch: 287, Loss: nan\n",
      "Epoch: 288, Loss: nan\n",
      "Epoch: 289, Loss: nan\n",
      "Epoch: 290, Loss: nan\n",
      "Epoch: 291, Loss: nan\n",
      "Epoch: 292, Loss: nan\n",
      "Epoch: 293, Loss: nan\n",
      "Epoch: 294, Loss: nan\n",
      "Epoch: 295, Loss: nan\n",
      "Epoch: 296, Loss: nan\n",
      "Epoch: 297, Loss: nan\n",
      "Epoch: 298, Loss: nan\n",
      "Epoch: 299, Loss: nan\n",
      "Epoch: 300, Loss: nan\n",
      "Epoch: 301, Loss: nan\n",
      "Epoch: 302, Loss: nan\n",
      "Epoch: 303, Loss: nan\n",
      "Epoch: 304, Loss: nan\n",
      "Epoch: 305, Loss: nan\n",
      "Epoch: 306, Loss: nan\n",
      "Epoch: 307, Loss: nan\n",
      "Epoch: 308, Loss: nan\n",
      "Epoch: 309, Loss: nan\n",
      "Epoch: 310, Loss: nan\n",
      "Epoch: 311, Loss: nan\n",
      "Epoch: 312, Loss: nan\n",
      "Epoch: 313, Loss: nan\n",
      "Epoch: 314, Loss: nan\n",
      "Epoch: 315, Loss: nan\n",
      "Epoch: 316, Loss: nan\n",
      "Epoch: 317, Loss: nan\n",
      "Epoch: 318, Loss: nan\n",
      "Epoch: 319, Loss: nan\n",
      "Epoch: 320, Loss: nan\n",
      "Epoch: 321, Loss: nan\n",
      "Epoch: 322, Loss: nan\n",
      "Epoch: 323, Loss: nan\n",
      "Epoch: 324, Loss: nan\n",
      "Epoch: 325, Loss: nan\n",
      "Epoch: 326, Loss: nan\n",
      "Epoch: 327, Loss: nan\n",
      "Epoch: 328, Loss: nan\n",
      "Epoch: 329, Loss: nan\n",
      "Epoch: 330, Loss: nan\n",
      "Epoch: 331, Loss: nan\n",
      "Epoch: 332, Loss: nan\n",
      "Epoch: 333, Loss: nan\n",
      "Epoch: 334, Loss: nan\n",
      "Epoch: 335, Loss: nan\n",
      "Epoch: 336, Loss: nan\n",
      "Epoch: 337, Loss: nan\n",
      "Epoch: 338, Loss: nan\n",
      "Epoch: 339, Loss: nan\n",
      "Epoch: 340, Loss: nan\n",
      "Epoch: 341, Loss: nan\n",
      "Epoch: 342, Loss: nan\n",
      "Epoch: 343, Loss: nan\n",
      "Epoch: 344, Loss: nan\n",
      "Epoch: 345, Loss: nan\n",
      "Epoch: 346, Loss: nan\n",
      "Epoch: 347, Loss: nan\n",
      "Epoch: 348, Loss: nan\n",
      "Epoch: 349, Loss: nan\n",
      "Epoch: 350, Loss: nan\n",
      "Epoch: 351, Loss: nan\n",
      "Epoch: 352, Loss: nan\n",
      "Epoch: 353, Loss: nan\n",
      "Epoch: 354, Loss: nan\n",
      "Epoch: 355, Loss: nan\n",
      "Epoch: 356, Loss: nan\n",
      "Epoch: 357, Loss: nan\n",
      "Epoch: 358, Loss: nan\n",
      "Epoch: 359, Loss: nan\n",
      "Epoch: 360, Loss: nan\n",
      "Epoch: 361, Loss: nan\n",
      "Epoch: 362, Loss: nan\n",
      "Epoch: 363, Loss: nan\n",
      "Epoch: 364, Loss: nan\n",
      "Epoch: 365, Loss: nan\n",
      "Epoch: 366, Loss: nan\n",
      "Epoch: 367, Loss: nan\n",
      "Epoch: 368, Loss: nan\n",
      "Epoch: 369, Loss: nan\n",
      "Epoch: 370, Loss: nan\n",
      "Epoch: 371, Loss: nan\n",
      "Epoch: 372, Loss: nan\n",
      "Epoch: 373, Loss: nan\n",
      "Epoch: 374, Loss: nan\n",
      "Epoch: 375, Loss: nan\n",
      "Epoch: 376, Loss: nan\n",
      "Epoch: 377, Loss: nan\n",
      "Epoch: 378, Loss: nan\n",
      "Epoch: 379, Loss: nan\n",
      "Epoch: 380, Loss: nan\n",
      "Epoch: 381, Loss: nan\n",
      "Epoch: 382, Loss: nan\n",
      "Epoch: 383, Loss: nan\n",
      "Epoch: 384, Loss: nan\n",
      "Epoch: 385, Loss: nan\n",
      "Epoch: 386, Loss: nan\n",
      "Epoch: 387, Loss: nan\n",
      "Epoch: 388, Loss: nan\n",
      "Epoch: 389, Loss: nan\n",
      "Epoch: 390, Loss: nan\n",
      "Epoch: 391, Loss: nan\n",
      "Epoch: 392, Loss: nan\n",
      "Epoch: 393, Loss: nan\n",
      "Epoch: 394, Loss: nan\n",
      "Epoch: 395, Loss: nan\n",
      "Epoch: 396, Loss: nan\n",
      "Epoch: 397, Loss: nan\n",
      "Epoch: 398, Loss: nan\n",
      "Epoch: 399, Loss: nan\n",
      "Epoch: 400, Loss: nan\n"
     ]
    }
   ],
   "source": [
    "model = GCN()\n",
    "criterion = torch.nn.CrossEntropyLoss()  #Initialize the CrossEntropyLoss function.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # Initialize the Adam optimizer.\n",
    "\n",
    "def train(node_features, edge_index, edge_attr, node_labels):\n",
    "\n",
    "    # for i in range(node_features.shape[0]):\n",
    "    optimizer.zero_grad()  # Clear gradients.\n",
    "    out, h = model(node_features, edge_index, edge_attr)  # Perform a single forward pass.\n",
    "    loss = criterion(out, node_labels)  # Compute the loss solely based on the training nodes.\n",
    "    loss.backward()  # Derive gradients.\n",
    "    optimizer.step()  # Update parameters based on gradients.\n",
    "\n",
    "    return loss, h\n",
    "\n",
    "\n",
    "for epoch in range(401):\n",
    "    #torch_edge_weight = torch.tensor(np.sum(edge_features_x, axis=2), dtype=torch.float)\n",
    "    torch_edge_weight = torch.tensor(np.random.randn(12,1), dtype=torch.float)\n",
    "    loss, h = train(torch_node_features_x[0], torch_edge_index, torch_edge_weight, torch_node_labels[0])\n",
    "    print(f'Epoch: {epoch}, Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eece571f",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
