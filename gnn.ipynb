{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "num_qubits = 7\n",
    "dataset_filename = \"dataset/dataset_tesi/NN1_Dataset(<=10Cx)_balanced1.csv\"\n",
    "df = pd.read_csv(dataset_filename)\n",
    "\n",
    "links = [set([0,1]), set([1,2]), set([1,3]), set([3,5]), set([4,5]), set([5,6])]\n",
    "def generate_columns(header, links, in_links=False):\n",
    "    if in_links:\n",
    "        return [header+str(i)+str(j) for i in range(num_qubits) for j in range(num_qubits) if set([i,j]) in links]\n",
    "    else:\n",
    "        return [header+str(i)+str(j) for i in range(num_qubits) for j in range(num_qubits) if set([i,j]) not in links and i!=j]\n",
    "\n",
    "useless_columns = ['Unnamed: 0', 'last_update_date', 'N_qubtis', 'N_measure', 'N_cx', 'backend_name']\n",
    "useless_columns += generate_columns(\"cx_\", links)\n",
    "useless_columns += generate_columns(\"edge_length_\", links)\n",
    "useless_columns += generate_columns(\"edge_error_\", links)\n",
    "useless_columns += [\"measure_\"+str(i) for i in range(num_qubits)]\n",
    "# Note that cx/edge_error/edge_length_xy is not neccessarily the same as cx/edge_length/edge_error_yx\n",
    "df.drop(columns=useless_columns, inplace=True)\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7026, 57), (7026, 49), (1757, 57), (1757, 49))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "df_train_x, df_train_y= train.iloc[:, :-num_qubits], train.iloc[:, -num_qubits:]\n",
    "df_test_x, df_test_y= test.iloc[:, :-num_qubits], test.iloc[:, -num_qubits:]\n",
    "\n",
    "train_x = scaler.fit_transform(df_train_x)\n",
    "test_x = scaler.fit_transform(df_test_x)\n",
    "\n",
    "\n",
    "# for every row in y, convert to 1 hot-encoding and flatten\n",
    "train_y = []\n",
    "for _, row in df_train_y.iterrows():\n",
    "    train_y.append(pd.get_dummies(row).values.flatten())\n",
    "train_y = np.array(train_y)\n",
    "\n",
    "test_y = []\n",
    "for _, row in df_test_y.iterrows():\n",
    "    test_y.append(pd.get_dummies(row).values.flatten())\n",
    "test_y = np.array(test_y)\n",
    "\n",
    "(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacky/anaconda3/envs/eece571f/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.lin1 = Linear(num_features, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin3 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin4 = Linear(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = x.relu()\n",
    "        x = self.lin2(x)\n",
    "        x = x.relu()\n",
    "        x = self.lin3(x)\n",
    "        x = x.relu()\n",
    "        x = self.lin4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(57, 49, hidden_channels=128)\n",
    "criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.03, weight_decay=5e-4)  # Define optimizer.\n",
    "\n",
    "torch_train_x = torch.tensor(train_x, dtype=torch.float)\n",
    "torch_train_y = torch.tensor(train_y, dtype=torch.float)\n",
    "torch_test_x = torch.tensor(test_x, dtype=torch.float)\n",
    "torch_test_y = torch.tensor(test_y, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(torch_train_x)  # Perform a single forward pass.\n",
    "      loss = criterion(out, torch_train_y)  # Compute the loss solely based on the training nodes.\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test(x, y):\n",
    "      model.eval()\n",
    "      pred = model(x)\n",
    "\n",
    "      test_correct = 0\n",
    "      for i, j in zip(pred, y):\n",
    "          pred_i = np.argmax(i.detach().numpy().reshape(7,7), axis=1)\n",
    "          label_j = np.argmax(j.detach().numpy().reshape(7,7), axis=1)\n",
    "          test_correct += np.array_equal(pred_i, label_j)\n",
    "      test_acc = int(test_correct) / int(y.shape[0])  # Derive ratio of correct predictions.\n",
    "      return test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 27.0806\n",
      "Epoch: 002, Loss: 21.9311\n",
      "Epoch: 003, Loss: 21.3686\n",
      "Epoch: 004, Loss: 20.4121\n",
      "Epoch: 005, Loss: 19.8376\n",
      "Epoch: 006, Loss: 19.9583\n",
      "Epoch: 007, Loss: 19.2463\n",
      "Epoch: 008, Loss: 19.0033\n",
      "Epoch: 009, Loss: 18.1184\n",
      "Epoch: 010, Loss: 18.0844\n",
      "Epoch: 011, Loss: 17.5067\n",
      "Epoch: 012, Loss: 17.1493\n",
      "Epoch: 013, Loss: 17.2074\n",
      "Epoch: 014, Loss: 17.1998\n",
      "Epoch: 015, Loss: 17.0094\n",
      "Epoch: 016, Loss: 17.1948\n",
      "Epoch: 017, Loss: 17.0084\n",
      "Epoch: 018, Loss: 16.9061\n",
      "Epoch: 019, Loss: 16.8015\n",
      "Epoch: 020, Loss: 16.7865\n",
      "Epoch: 021, Loss: 16.6172\n",
      "Epoch: 022, Loss: 16.5777\n",
      "Epoch: 023, Loss: 16.5110\n",
      "Epoch: 024, Loss: 16.4326\n",
      "Epoch: 025, Loss: 16.3511\n",
      "Epoch: 026, Loss: 16.3091\n",
      "Epoch: 027, Loss: 16.2670\n",
      "Epoch: 028, Loss: 16.2193\n",
      "Epoch: 029, Loss: 16.2123\n",
      "Epoch: 030, Loss: 16.1537\n",
      "Epoch: 031, Loss: 16.1580\n",
      "Epoch: 032, Loss: 16.1349\n",
      "Epoch: 033, Loss: 16.1038\n",
      "Epoch: 034, Loss: 16.0695\n",
      "Epoch: 035, Loss: 16.0515\n",
      "Epoch: 036, Loss: 16.0234\n",
      "Epoch: 037, Loss: 16.0063\n",
      "Epoch: 038, Loss: 15.9858\n",
      "Epoch: 039, Loss: 15.9464\n",
      "Epoch: 040, Loss: 15.9297\n",
      "Epoch: 041, Loss: 15.9038\n",
      "Epoch: 042, Loss: 15.8748\n",
      "Epoch: 043, Loss: 15.8402\n",
      "Epoch: 044, Loss: 15.8079\n",
      "Epoch: 045, Loss: 15.7933\n",
      "Epoch: 046, Loss: 15.7757\n",
      "Epoch: 047, Loss: 15.7632\n",
      "Epoch: 048, Loss: 15.6774\n",
      "Epoch: 049, Loss: 15.6568\n",
      "Epoch: 050, Loss: 15.6299\n",
      "Epoch: 051, Loss: 15.5724\n",
      "Epoch: 052, Loss: 15.5446\n",
      "Epoch: 053, Loss: 15.4921\n",
      "Epoch: 054, Loss: 15.4733\n",
      "Epoch: 055, Loss: 15.4474\n",
      "Epoch: 056, Loss: 15.4167\n",
      "Epoch: 057, Loss: 15.3913\n",
      "Epoch: 058, Loss: 15.3819\n",
      "Epoch: 059, Loss: 15.4210\n",
      "Epoch: 060, Loss: 15.7030\n",
      "Epoch: 061, Loss: 15.4269\n",
      "Epoch: 062, Loss: 15.6265\n",
      "Epoch: 063, Loss: 15.7050\n",
      "Epoch: 064, Loss: 15.6104\n",
      "Epoch: 065, Loss: 15.5033\n",
      "Epoch: 066, Loss: 15.4841\n",
      "Epoch: 067, Loss: 15.4083\n",
      "Epoch: 068, Loss: 15.4527\n",
      "Epoch: 069, Loss: 15.3552\n",
      "Epoch: 070, Loss: 15.3420\n",
      "Epoch: 071, Loss: 15.3339\n",
      "Epoch: 072, Loss: 15.2618\n",
      "Epoch: 073, Loss: 15.2782\n",
      "Epoch: 074, Loss: 15.1999\n",
      "Epoch: 075, Loss: 15.2126\n",
      "Epoch: 076, Loss: 15.1664\n",
      "Epoch: 077, Loss: 15.1462\n",
      "Epoch: 078, Loss: 15.1285\n",
      "Epoch: 079, Loss: 15.1061\n",
      "Epoch: 080, Loss: 15.0822\n",
      "Epoch: 081, Loss: 15.0689\n",
      "Epoch: 082, Loss: 15.0424\n",
      "Epoch: 083, Loss: 15.0511\n",
      "Epoch: 084, Loss: 15.0361\n",
      "Epoch: 085, Loss: 15.0492\n",
      "Epoch: 086, Loss: 15.0559\n",
      "Epoch: 087, Loss: 15.0153\n",
      "Epoch: 088, Loss: 14.9723\n",
      "Epoch: 089, Loss: 14.9769\n",
      "Epoch: 090, Loss: 14.9575\n",
      "Epoch: 091, Loss: 14.9369\n",
      "Epoch: 092, Loss: 14.9301\n",
      "Epoch: 093, Loss: 14.9129\n",
      "Epoch: 094, Loss: 14.9141\n",
      "Epoch: 095, Loss: 14.9022\n",
      "Epoch: 096, Loss: 14.8954\n",
      "Epoch: 097, Loss: 14.8792\n",
      "Epoch: 098, Loss: 14.8885\n",
      "Epoch: 099, Loss: 14.9474\n",
      "Epoch: 100, Loss: 14.9410\n",
      "Epoch: 101, Loss: 14.8543\n",
      "Epoch: 102, Loss: 14.9378\n",
      "Epoch: 103, Loss: 14.9208\n",
      "Epoch: 104, Loss: 14.9071\n",
      "Epoch: 105, Loss: 14.8734\n",
      "Epoch: 106, Loss: 14.8682\n",
      "Epoch: 107, Loss: 14.8707\n",
      "Epoch: 108, Loss: 14.8058\n",
      "Epoch: 109, Loss: 14.8061\n",
      "Epoch: 110, Loss: 14.7850\n",
      "Epoch: 111, Loss: 14.8158\n",
      "Epoch: 112, Loss: 14.8049\n",
      "Epoch: 113, Loss: 14.8601\n",
      "Epoch: 114, Loss: 14.8834\n",
      "Epoch: 115, Loss: 14.9159\n",
      "Epoch: 116, Loss: 14.7507\n",
      "Epoch: 117, Loss: 14.8525\n",
      "Epoch: 118, Loss: 14.7629\n",
      "Epoch: 119, Loss: 14.7881\n",
      "Epoch: 120, Loss: 14.7630\n",
      "Epoch: 121, Loss: 14.7570\n",
      "Epoch: 122, Loss: 14.7393\n",
      "Epoch: 123, Loss: 14.7233\n",
      "Epoch: 124, Loss: 14.6958\n",
      "Epoch: 125, Loss: 14.6871\n",
      "Epoch: 126, Loss: 14.6786\n",
      "Epoch: 127, Loss: 14.6518\n",
      "Epoch: 128, Loss: 14.6419\n",
      "Epoch: 129, Loss: 14.6230\n",
      "Epoch: 130, Loss: 14.6271\n",
      "Epoch: 131, Loss: 14.5964\n",
      "Epoch: 132, Loss: 14.5925\n",
      "Epoch: 133, Loss: 14.6113\n",
      "Epoch: 134, Loss: 14.6797\n",
      "Epoch: 135, Loss: 14.8247\n",
      "Epoch: 136, Loss: 15.3377\n",
      "Epoch: 137, Loss: 14.8325\n",
      "Epoch: 138, Loss: 14.9444\n",
      "Epoch: 139, Loss: 14.8041\n",
      "Epoch: 140, Loss: 14.7982\n",
      "Epoch: 141, Loss: 14.8515\n",
      "Epoch: 142, Loss: 14.7940\n",
      "Epoch: 143, Loss: 14.7405\n",
      "Epoch: 144, Loss: 14.7677\n",
      "Epoch: 145, Loss: 14.7251\n",
      "Epoch: 146, Loss: 14.7216\n",
      "Epoch: 147, Loss: 14.6918\n",
      "Epoch: 148, Loss: 14.7054\n",
      "Epoch: 149, Loss: 14.6829\n",
      "Epoch: 150, Loss: 14.6890\n",
      "Epoch: 151, Loss: 14.6406\n",
      "Epoch: 152, Loss: 14.6233\n",
      "Epoch: 153, Loss: 14.6460\n",
      "Epoch: 154, Loss: 14.6065\n",
      "Epoch: 155, Loss: 14.5730\n",
      "Epoch: 156, Loss: 14.5970\n",
      "Epoch: 157, Loss: 14.5600\n",
      "Epoch: 158, Loss: 14.5278\n",
      "Epoch: 159, Loss: 14.5198\n",
      "Epoch: 160, Loss: 14.5176\n",
      "Epoch: 161, Loss: 14.4957\n",
      "Epoch: 162, Loss: 14.4876\n",
      "Epoch: 163, Loss: 14.4784\n",
      "Epoch: 164, Loss: 14.4962\n",
      "Epoch: 165, Loss: 14.5200\n",
      "Epoch: 166, Loss: 14.6171\n",
      "Epoch: 167, Loss: 14.5445\n",
      "Epoch: 168, Loss: 14.4591\n",
      "Epoch: 169, Loss: 14.4986\n",
      "Epoch: 170, Loss: 14.4672\n",
      "Epoch: 171, Loss: 14.4427\n",
      "Epoch: 172, Loss: 14.4503\n",
      "Epoch: 173, Loss: 14.4266\n",
      "Epoch: 174, Loss: 14.4297\n",
      "Epoch: 175, Loss: 14.4204\n",
      "Epoch: 176, Loss: 14.4155\n",
      "Epoch: 177, Loss: 14.4296\n",
      "Epoch: 178, Loss: 14.4516\n",
      "Epoch: 179, Loss: 14.4246\n",
      "Epoch: 180, Loss: 14.4408\n",
      "Epoch: 181, Loss: 14.4173\n",
      "Epoch: 182, Loss: 14.4079\n",
      "Epoch: 183, Loss: 14.4265\n",
      "Epoch: 184, Loss: 14.4238\n",
      "Epoch: 185, Loss: 14.4208\n",
      "Epoch: 186, Loss: 14.3882\n",
      "Epoch: 187, Loss: 14.4017\n",
      "Epoch: 188, Loss: 14.3820\n",
      "Epoch: 189, Loss: 14.3810\n",
      "Epoch: 190, Loss: 14.3810\n",
      "Epoch: 191, Loss: 14.3690\n",
      "Epoch: 192, Loss: 14.3589\n",
      "Epoch: 193, Loss: 14.3538\n",
      "Epoch: 194, Loss: 14.3612\n",
      "Epoch: 195, Loss: 14.3997\n",
      "Epoch: 196, Loss: 14.4733\n",
      "Epoch: 197, Loss: 14.4206\n",
      "Epoch: 198, Loss: 14.5198\n",
      "Epoch: 199, Loss: 14.3832\n",
      "Epoch: 200, Loss: 14.4093\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 201):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9210\n",
      "Training Accuracy: 0.8708\n"
     ]
    }
   ],
   "source": [
    "training_acc = test(torch_train_x, torch_train_y)\n",
    "print(f'Training Accuracy: {training_acc:.4f}')\n",
    "\n",
    "testing_acc = test(torch_test_x, torch_test_y)\n",
    "print(f'Training Accuracy: {testing_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Here's how you would parse the data for GNNs if edge features can be added\n",
    "################################################################################\n",
    "\n",
    "# node_prefix = [\"T1\", \"T2\", \"readout_error\"]\n",
    "# node_features_x = []\n",
    "# for k in range(df_train_x.shape[0]):\n",
    "#     node_features_j = []\n",
    "#     for j in range(num_qubits):\n",
    "#         row_features = df_train_x.iloc[k][[i + \"_\" + str(j) for i in node_prefix]].values.flatten()\n",
    "#         node_features_j.append(row_features)\n",
    "#     node_features_j = scaler.fit_transform(node_features_j)\n",
    "#     node_features_x.append(node_features_j)\n",
    "# node_features_x = np.array(node_features_x)\n",
    "\n",
    "# edge_prefix = [\"cx_\", \"edge_length_\", \"edge_error_\"]\n",
    "# edge_index = [[],[]]\n",
    "# edge_features_x = []\n",
    "# for k in range(df_train_x.shape[0]):\n",
    "#     edge_features_j = []\n",
    "#     for i in range(num_qubits):\n",
    "#         for j in range(num_qubits):\n",
    "#             if set([i,j]) in links:\n",
    "#                 row_features = df_train_x.iloc[k][[prefix + str(i) + str(j) for prefix in edge_prefix]].values.flatten()\n",
    "#                 edge_features_j.append(row_features)\n",
    "#                 if k == 0: # only need to do this once\n",
    "#                     edge_index[0].append(i)\n",
    "#                     edge_index[1].append(j)\n",
    "#     edge_features_j = scaler.fit_transform(edge_features_j)\n",
    "#     edge_features_x.append(edge_features_j)\n",
    "# edge_features_x = np.array(edge_features_x)\n",
    "# edge_index = np.array(edge_index)\n",
    "\n",
    "# node_labels = df_train_y.to_numpy()\n",
    "# print(node_features_x.shape, edge_index.shape, edge_features_x.shape, node_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7026, 7, 12) (2, 12) (7026, 49)\n",
      "(1757, 7, 12) (2, 12) (1757, 49)\n"
     ]
    }
   ],
   "source": [
    "# Since GCN doesn't support edge features, we can just concatenate them to the node features\n",
    "def get_datasets(df_x, df_y):\n",
    "    node_prefix = [\"T1\", \"T2\", \"readout_error\"]\n",
    "    edge_prefix = [\"cx\", \"edge_length\", \"edge_error\"]\n",
    "    node_features_x = []\n",
    "    edge_index = [[],[]]\n",
    "    for k in range(df_x.shape[0]):\n",
    "        node_features_i = []\n",
    "        for i in range(num_qubits):\n",
    "            node_features_i.append(list(df_train_x.iloc[k][[l + \"_\" + str(i) for l in node_prefix]].values.flatten()))\n",
    "            for j in range(num_qubits):\n",
    "                if set([i,j]) in links:\n",
    "                    node_features_i[i].extend(df_train_x.iloc[k][[l + \"_\" + str(i)+str(j) for l in edge_prefix]].values.flatten())\n",
    "                    if(k == 0): # only do this once\n",
    "                        edge_index[0].append(i)\n",
    "                        edge_index[1].append(j)\n",
    "            if(len(node_features_i[i]) < 12): # pad features to 12 with standard normal\n",
    "                node_features_i[i].extend(np.random.randn(12-len(node_features_i[i])))\n",
    "        node_features_i = scaler.fit_transform(node_features_i)\n",
    "        node_features_x.append(node_features_i)\n",
    "    node_features_x = np.array(node_features_x)\n",
    "    edge_index = np.array(edge_index)\n",
    "\n",
    "    node_labels = []\n",
    "    for _, row in df_y.iterrows():\n",
    "        node_labels.append(pd.get_dummies(row).values.flatten())\n",
    "    node_labels= np.array(node_labels)\n",
    "\n",
    "    return node_features_x, edge_index, node_labels\n",
    "\n",
    "train_gnn_x, edge_index, train_gnn_y = get_datasets(df_train_x, df_train_y)\n",
    "test_gnn_x, edge_index, test_gnn_y = get_datasets(df_test_x, df_test_y)\n",
    "print(train_gnn_x.shape, edge_index.shape, train_gnn_y.shape)\n",
    "print(test_gnn_x.shape, edge_index.shape, test_gnn_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(12, 12)\n",
      "  (conv2): GCNConv(12, 12)\n",
      "  (classifier): Linear(in_features=84, out_features=49, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(12, 12)\n",
    "        self.conv2 = GCNConv(12, 12)\n",
    "        self.classifier = Linear(84, 49)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = self.conv1(x, edge_index)\n",
    "        h = h.tanh()\n",
    "        h = self.conv2(h, edge_index)\n",
    "        h = h.tanh()\n",
    "        \n",
    "        # Apply a final (linear) classifier.\n",
    "        out = self.classifier(h.flatten(start_dim=1))\n",
    "\n",
    "        return out, h\n",
    "\n",
    "model = GCN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_train_gnn_x = torch.tensor(train_gnn_x, dtype=torch.float)\n",
    "torch_test_gnn_x = torch.tensor(test_gnn_x, dtype=torch.float)\n",
    "torch_edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "# torch_edge_features_x = torch.tensor(edge_features_x, dtype=torch.float)\n",
    "torch_train_gnn_y = torch.tensor(train_gnn_y, dtype=torch.float)\n",
    "torch_test_gnn_y = torch.tensor(test_gnn_y, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 27.11977767944336\n",
      "Epoch: 1, Loss: 19.595718383789062\n",
      "Epoch: 2, Loss: 19.82357406616211\n",
      "Epoch: 3, Loss: 21.80921745300293\n",
      "Epoch: 4, Loss: 20.159744262695312\n",
      "Epoch: 5, Loss: 18.985483169555664\n",
      "Epoch: 6, Loss: 19.65985870361328\n",
      "Epoch: 7, Loss: 17.632112503051758\n",
      "Epoch: 8, Loss: 18.035011291503906\n",
      "Epoch: 9, Loss: 17.932348251342773\n",
      "Epoch: 10, Loss: 18.10352897644043\n",
      "Epoch: 11, Loss: 17.677568435668945\n",
      "Epoch: 12, Loss: 17.189714431762695\n",
      "Epoch: 13, Loss: 17.124948501586914\n",
      "Epoch: 14, Loss: 17.122028350830078\n",
      "Epoch: 15, Loss: 17.19905662536621\n",
      "Epoch: 16, Loss: 17.088733673095703\n",
      "Epoch: 17, Loss: 16.907011032104492\n",
      "Epoch: 18, Loss: 16.75939178466797\n",
      "Epoch: 19, Loss: 16.65770721435547\n",
      "Epoch: 20, Loss: 16.727760314941406\n",
      "Epoch: 21, Loss: 16.72112464904785\n",
      "Epoch: 22, Loss: 16.646915435791016\n",
      "Epoch: 23, Loss: 16.51408576965332\n",
      "Epoch: 24, Loss: 16.492767333984375\n",
      "Epoch: 25, Loss: 16.498363494873047\n",
      "Epoch: 26, Loss: 16.49759864807129\n",
      "Epoch: 27, Loss: 16.49214744567871\n",
      "Epoch: 28, Loss: 16.455963134765625\n",
      "Epoch: 29, Loss: 16.36977195739746\n",
      "Epoch: 30, Loss: 16.346750259399414\n",
      "Epoch: 31, Loss: 16.326391220092773\n",
      "Epoch: 32, Loss: 16.338668823242188\n",
      "Epoch: 33, Loss: 16.32855987548828\n",
      "Epoch: 34, Loss: 16.303653717041016\n",
      "Epoch: 35, Loss: 16.263710021972656\n",
      "Epoch: 36, Loss: 16.235824584960938\n",
      "Epoch: 37, Loss: 16.219200134277344\n",
      "Epoch: 38, Loss: 16.229564666748047\n",
      "Epoch: 39, Loss: 16.213472366333008\n",
      "Epoch: 40, Loss: 16.199913024902344\n",
      "Epoch: 41, Loss: 16.17936897277832\n",
      "Epoch: 42, Loss: 16.159912109375\n",
      "Epoch: 43, Loss: 16.148143768310547\n",
      "Epoch: 44, Loss: 16.137849807739258\n",
      "Epoch: 45, Loss: 16.129955291748047\n",
      "Epoch: 46, Loss: 16.116636276245117\n",
      "Epoch: 47, Loss: 16.105960845947266\n",
      "Epoch: 48, Loss: 16.09647560119629\n",
      "Epoch: 49, Loss: 16.088459014892578\n",
      "Epoch: 50, Loss: 16.079660415649414\n",
      "Epoch: 51, Loss: 16.06526756286621\n",
      "Epoch: 52, Loss: 16.05832862854004\n",
      "Epoch: 53, Loss: 16.054386138916016\n",
      "Epoch: 54, Loss: 16.04491424560547\n",
      "Epoch: 55, Loss: 16.039043426513672\n",
      "Epoch: 56, Loss: 16.030733108520508\n",
      "Epoch: 57, Loss: 16.022727966308594\n",
      "Epoch: 58, Loss: 16.017412185668945\n",
      "Epoch: 59, Loss: 16.01256561279297\n",
      "Epoch: 60, Loss: 16.005029678344727\n",
      "Epoch: 61, Loss: 15.998963356018066\n",
      "Epoch: 62, Loss: 15.994985580444336\n",
      "Epoch: 63, Loss: 15.988383293151855\n",
      "Epoch: 64, Loss: 15.98278522491455\n",
      "Epoch: 65, Loss: 15.977887153625488\n",
      "Epoch: 66, Loss: 15.973844528198242\n",
      "Epoch: 67, Loss: 15.969561576843262\n",
      "Epoch: 68, Loss: 15.964455604553223\n",
      "Epoch: 69, Loss: 15.958989143371582\n",
      "Epoch: 70, Loss: 15.955046653747559\n",
      "Epoch: 71, Loss: 15.950989723205566\n",
      "Epoch: 72, Loss: 15.947000503540039\n",
      "Epoch: 73, Loss: 15.942120552062988\n",
      "Epoch: 74, Loss: 15.93769359588623\n",
      "Epoch: 75, Loss: 15.933603286743164\n",
      "Epoch: 76, Loss: 15.929408073425293\n",
      "Epoch: 77, Loss: 15.925354957580566\n",
      "Epoch: 78, Loss: 15.921377182006836\n",
      "Epoch: 79, Loss: 15.917458534240723\n",
      "Epoch: 80, Loss: 15.913209915161133\n",
      "Epoch: 81, Loss: 15.909296035766602\n",
      "Epoch: 82, Loss: 15.90559196472168\n",
      "Epoch: 83, Loss: 15.901702880859375\n",
      "Epoch: 84, Loss: 15.897903442382812\n",
      "Epoch: 85, Loss: 15.894351959228516\n",
      "Epoch: 86, Loss: 15.891186714172363\n",
      "Epoch: 87, Loss: 15.888654708862305\n",
      "Epoch: 88, Loss: 15.887537002563477\n",
      "Epoch: 89, Loss: 15.89104175567627\n",
      "Epoch: 90, Loss: 15.900715827941895\n",
      "Epoch: 91, Loss: 15.93519115447998\n",
      "Epoch: 92, Loss: 15.961004257202148\n",
      "Epoch: 93, Loss: 16.010910034179688\n",
      "Epoch: 94, Loss: 15.904995918273926\n",
      "Epoch: 95, Loss: 15.86287784576416\n",
      "Epoch: 96, Loss: 15.909900665283203\n",
      "Epoch: 97, Loss: 15.89897632598877\n",
      "Epoch: 98, Loss: 15.857545852661133\n",
      "Epoch: 99, Loss: 15.867634773254395\n",
      "Epoch: 100, Loss: 15.875911712646484\n",
      "Epoch: 101, Loss: 15.853707313537598\n",
      "Epoch: 102, Loss: 15.849567413330078\n",
      "Epoch: 103, Loss: 15.859850883483887\n",
      "Epoch: 104, Loss: 15.841591835021973\n",
      "Epoch: 105, Loss: 15.838362693786621\n",
      "Epoch: 106, Loss: 15.845433235168457\n",
      "Epoch: 107, Loss: 15.829838752746582\n",
      "Epoch: 108, Loss: 15.829270362854004\n",
      "Epoch: 109, Loss: 15.831832885742188\n",
      "Epoch: 110, Loss: 15.82121753692627\n",
      "Epoch: 111, Loss: 15.818572044372559\n",
      "Epoch: 112, Loss: 15.820775032043457\n",
      "Epoch: 113, Loss: 15.813042640686035\n",
      "Epoch: 114, Loss: 15.807899475097656\n",
      "Epoch: 115, Loss: 15.810482025146484\n",
      "Epoch: 116, Loss: 15.805784225463867\n",
      "Epoch: 117, Loss: 15.798758506774902\n",
      "Epoch: 118, Loss: 15.799741744995117\n",
      "Epoch: 119, Loss: 15.798949241638184\n",
      "Epoch: 120, Loss: 15.792085647583008\n",
      "Epoch: 121, Loss: 15.788287162780762\n",
      "Epoch: 122, Loss: 15.789132118225098\n",
      "Epoch: 123, Loss: 15.786638259887695\n",
      "Epoch: 124, Loss: 15.780933380126953\n",
      "Epoch: 125, Loss: 15.777396202087402\n",
      "Epoch: 126, Loss: 15.77659797668457\n",
      "Epoch: 127, Loss: 15.77576732635498\n",
      "Epoch: 128, Loss: 15.772478103637695\n",
      "Epoch: 129, Loss: 15.768176078796387\n",
      "Epoch: 130, Loss: 15.76456069946289\n",
      "Epoch: 131, Loss: 15.762616157531738\n",
      "Epoch: 132, Loss: 15.761427879333496\n",
      "Epoch: 133, Loss: 15.76008415222168\n",
      "Epoch: 134, Loss: 15.75860595703125\n",
      "Epoch: 135, Loss: 15.756122589111328\n",
      "Epoch: 136, Loss: 15.753776550292969\n",
      "Epoch: 137, Loss: 15.750916481018066\n",
      "Epoch: 138, Loss: 15.748623847961426\n",
      "Epoch: 139, Loss: 15.746232986450195\n",
      "Epoch: 140, Loss: 15.744595527648926\n",
      "Epoch: 141, Loss: 15.743305206298828\n",
      "Epoch: 142, Loss: 15.744016647338867\n",
      "Epoch: 143, Loss: 15.745887756347656\n",
      "Epoch: 144, Loss: 15.755541801452637\n",
      "Epoch: 145, Loss: 15.766650199890137\n",
      "Epoch: 146, Loss: 15.803915023803711\n",
      "Epoch: 147, Loss: 15.80073356628418\n",
      "Epoch: 148, Loss: 15.8145751953125\n",
      "Epoch: 149, Loss: 15.749719619750977\n",
      "Epoch: 150, Loss: 15.721671104431152\n",
      "Epoch: 151, Loss: 15.738504409790039\n",
      "Epoch: 152, Loss: 15.753369331359863\n",
      "Epoch: 153, Loss: 15.745813369750977\n",
      "Epoch: 154, Loss: 15.716204643249512\n",
      "Epoch: 155, Loss: 15.725317001342773\n",
      "Epoch: 156, Loss: 15.745871543884277\n",
      "Epoch: 157, Loss: 15.722546577453613\n",
      "Epoch: 158, Loss: 15.707828521728516\n",
      "Epoch: 159, Loss: 15.71938419342041\n",
      "Epoch: 160, Loss: 15.721268653869629\n",
      "Epoch: 161, Loss: 15.708205223083496\n",
      "Epoch: 162, Loss: 15.699923515319824\n",
      "Epoch: 163, Loss: 15.706892013549805\n",
      "Epoch: 164, Loss: 15.711614608764648\n",
      "Epoch: 165, Loss: 15.699516296386719\n",
      "Epoch: 166, Loss: 15.691752433776855\n",
      "Epoch: 167, Loss: 15.695069313049316\n",
      "Epoch: 168, Loss: 15.697991371154785\n",
      "Epoch: 169, Loss: 15.695815086364746\n",
      "Epoch: 170, Loss: 15.687017440795898\n",
      "Epoch: 171, Loss: 15.682782173156738\n",
      "Epoch: 172, Loss: 15.683652877807617\n",
      "Epoch: 173, Loss: 15.685964584350586\n",
      "Epoch: 174, Loss: 15.687103271484375\n",
      "Epoch: 175, Loss: 15.683310508728027\n",
      "Epoch: 176, Loss: 15.679369926452637\n",
      "Epoch: 177, Loss: 15.674835205078125\n",
      "Epoch: 178, Loss: 15.671319961547852\n",
      "Epoch: 179, Loss: 15.669035911560059\n",
      "Epoch: 180, Loss: 15.66771411895752\n",
      "Epoch: 181, Loss: 15.667394638061523\n",
      "Epoch: 182, Loss: 15.668036460876465\n",
      "Epoch: 183, Loss: 15.671443939208984\n",
      "Epoch: 184, Loss: 15.678240776062012\n",
      "Epoch: 185, Loss: 15.698807716369629\n",
      "Epoch: 186, Loss: 15.724778175354004\n",
      "Epoch: 187, Loss: 15.800471305847168\n",
      "Epoch: 188, Loss: 15.77135181427002\n",
      "Epoch: 189, Loss: 15.740068435668945\n",
      "Epoch: 190, Loss: 15.664660453796387\n",
      "Epoch: 191, Loss: 15.681340217590332\n",
      "Epoch: 192, Loss: 15.734201431274414\n",
      "Epoch: 193, Loss: 15.678311347961426\n",
      "Epoch: 194, Loss: 15.66163444519043\n",
      "Epoch: 195, Loss: 15.69787311553955\n",
      "Epoch: 196, Loss: 15.673189163208008\n",
      "Epoch: 197, Loss: 15.651530265808105\n",
      "Epoch: 198, Loss: 15.671314239501953\n",
      "Epoch: 199, Loss: 15.666110038757324\n",
      "Epoch: 200, Loss: 15.64896297454834\n",
      "Epoch: 201, Loss: 15.651686668395996\n",
      "Epoch: 202, Loss: 15.659551620483398\n",
      "Epoch: 203, Loss: 15.651010513305664\n",
      "Epoch: 204, Loss: 15.641182899475098\n",
      "Epoch: 205, Loss: 15.649232864379883\n",
      "Epoch: 206, Loss: 15.652725219726562\n",
      "Epoch: 207, Loss: 15.641135215759277\n",
      "Epoch: 208, Loss: 15.635018348693848\n",
      "Epoch: 209, Loss: 15.642138481140137\n",
      "Epoch: 210, Loss: 15.644102096557617\n",
      "Epoch: 211, Loss: 15.637187957763672\n",
      "Epoch: 212, Loss: 15.630159378051758\n",
      "Epoch: 213, Loss: 15.630541801452637\n",
      "Epoch: 214, Loss: 15.634904861450195\n",
      "Epoch: 215, Loss: 15.633745193481445\n",
      "Epoch: 216, Loss: 15.630922317504883\n",
      "Epoch: 217, Loss: 15.626656532287598\n",
      "Epoch: 218, Loss: 15.624146461486816\n",
      "Epoch: 219, Loss: 15.623954772949219\n",
      "Epoch: 220, Loss: 15.623501777648926\n",
      "Epoch: 221, Loss: 15.623539924621582\n",
      "Epoch: 222, Loss: 15.625913619995117\n",
      "Epoch: 223, Loss: 15.631820678710938\n",
      "Epoch: 224, Loss: 15.651774406433105\n",
      "Epoch: 225, Loss: 15.657462120056152\n",
      "Epoch: 226, Loss: 15.671330451965332\n",
      "Epoch: 227, Loss: 15.649255752563477\n",
      "Epoch: 228, Loss: 15.625102043151855\n",
      "Epoch: 229, Loss: 15.626978874206543\n",
      "Epoch: 230, Loss: 15.637896537780762\n",
      "Epoch: 231, Loss: 15.643649101257324\n",
      "Epoch: 232, Loss: 15.6548433303833\n",
      "Epoch: 233, Loss: 15.684678077697754\n",
      "Epoch: 234, Loss: 15.710064888000488\n",
      "Epoch: 235, Loss: 15.74127197265625\n",
      "Epoch: 236, Loss: 15.684629440307617\n",
      "Epoch: 237, Loss: 15.624368667602539\n",
      "Epoch: 238, Loss: 15.620196342468262\n",
      "Epoch: 239, Loss: 15.650874137878418\n",
      "Epoch: 240, Loss: 15.64999008178711\n",
      "Epoch: 241, Loss: 15.61361026763916\n",
      "Epoch: 242, Loss: 15.62273120880127\n",
      "Epoch: 243, Loss: 15.649645805358887\n",
      "Epoch: 244, Loss: 15.625187873840332\n",
      "Epoch: 245, Loss: 15.611934661865234\n",
      "Epoch: 246, Loss: 15.635286331176758\n",
      "Epoch: 247, Loss: 15.638036727905273\n",
      "Epoch: 248, Loss: 15.60833740234375\n",
      "Epoch: 249, Loss: 15.613585472106934\n",
      "Epoch: 250, Loss: 15.639260292053223\n",
      "Epoch: 251, Loss: 15.613869667053223\n",
      "Epoch: 252, Loss: 15.59996509552002\n",
      "Epoch: 253, Loss: 15.621381759643555\n",
      "Epoch: 254, Loss: 15.619946479797363\n",
      "Epoch: 255, Loss: 15.59541130065918\n",
      "Epoch: 256, Loss: 15.602031707763672\n",
      "Epoch: 257, Loss: 15.607730865478516\n",
      "Epoch: 258, Loss: 15.592377662658691\n",
      "Epoch: 259, Loss: 15.598722457885742\n",
      "Epoch: 260, Loss: 15.601593971252441\n",
      "Epoch: 261, Loss: 15.585729598999023\n",
      "Epoch: 262, Loss: 15.58593463897705\n",
      "Epoch: 263, Loss: 15.592217445373535\n",
      "Epoch: 264, Loss: 15.587664604187012\n",
      "Epoch: 265, Loss: 15.584267616271973\n",
      "Epoch: 266, Loss: 15.58710765838623\n",
      "Epoch: 267, Loss: 15.587385177612305\n",
      "Epoch: 268, Loss: 15.580662727355957\n",
      "Epoch: 269, Loss: 15.581908226013184\n",
      "Epoch: 270, Loss: 15.587841033935547\n",
      "Epoch: 271, Loss: 15.587010383605957\n",
      "Epoch: 272, Loss: 15.585660934448242\n",
      "Epoch: 273, Loss: 15.591731071472168\n",
      "Epoch: 274, Loss: 15.595980644226074\n",
      "Epoch: 275, Loss: 15.60250186920166\n",
      "Epoch: 276, Loss: 15.615348815917969\n",
      "Epoch: 277, Loss: 15.624303817749023\n",
      "Epoch: 278, Loss: 15.643607139587402\n",
      "Epoch: 279, Loss: 15.62690258026123\n",
      "Epoch: 280, Loss: 15.60009479522705\n",
      "Epoch: 281, Loss: 15.57664966583252\n",
      "Epoch: 282, Loss: 15.58161449432373\n",
      "Epoch: 283, Loss: 15.593866348266602\n",
      "Epoch: 284, Loss: 15.58755874633789\n",
      "Epoch: 285, Loss: 15.571958541870117\n",
      "Epoch: 286, Loss: 15.56422233581543\n",
      "Epoch: 287, Loss: 15.571491241455078\n",
      "Epoch: 288, Loss: 15.579545974731445\n",
      "Epoch: 289, Loss: 15.578288078308105\n",
      "Epoch: 290, Loss: 15.568689346313477\n",
      "Epoch: 291, Loss: 15.559511184692383\n",
      "Epoch: 292, Loss: 15.557182312011719\n",
      "Epoch: 293, Loss: 15.560502052307129\n",
      "Epoch: 294, Loss: 15.566593170166016\n",
      "Epoch: 295, Loss: 15.569780349731445\n",
      "Epoch: 296, Loss: 15.567920684814453\n",
      "Epoch: 297, Loss: 15.564088821411133\n",
      "Epoch: 298, Loss: 15.559514045715332\n",
      "Epoch: 299, Loss: 15.558670043945312\n",
      "Epoch: 300, Loss: 15.558140754699707\n",
      "Epoch: 301, Loss: 15.557500839233398\n",
      "Epoch: 302, Loss: 15.557706832885742\n",
      "Epoch: 303, Loss: 15.556824684143066\n",
      "Epoch: 304, Loss: 15.558270454406738\n",
      "Epoch: 305, Loss: 15.564047813415527\n",
      "Epoch: 306, Loss: 15.57733154296875\n",
      "Epoch: 307, Loss: 15.609885215759277\n",
      "Epoch: 308, Loss: 15.662823677062988\n",
      "Epoch: 309, Loss: 15.741777420043945\n",
      "Epoch: 310, Loss: 15.720582008361816\n",
      "Epoch: 311, Loss: 15.590177536010742\n",
      "Epoch: 312, Loss: 15.561151504516602\n",
      "Epoch: 313, Loss: 15.629277229309082\n",
      "Epoch: 314, Loss: 15.618757247924805\n",
      "Epoch: 315, Loss: 15.558796882629395\n",
      "Epoch: 316, Loss: 15.61850643157959\n",
      "Epoch: 317, Loss: 15.641280174255371\n",
      "Epoch: 318, Loss: 15.55770206451416\n",
      "Epoch: 319, Loss: 15.631404876708984\n",
      "Epoch: 320, Loss: 15.67106819152832\n",
      "Epoch: 321, Loss: 15.563736915588379\n",
      "Epoch: 322, Loss: 15.672101974487305\n",
      "Epoch: 323, Loss: 15.679085731506348\n",
      "Epoch: 324, Loss: 15.612252235412598\n",
      "Epoch: 325, Loss: 15.701242446899414\n",
      "Epoch: 326, Loss: 15.641862869262695\n",
      "Epoch: 327, Loss: 15.700403213500977\n",
      "Epoch: 328, Loss: 15.674843788146973\n",
      "Epoch: 329, Loss: 15.672277450561523\n",
      "Epoch: 330, Loss: 15.741357803344727\n",
      "Epoch: 331, Loss: 15.663558959960938\n",
      "Epoch: 332, Loss: 15.723873138427734\n",
      "Epoch: 333, Loss: 15.632519721984863\n",
      "Epoch: 334, Loss: 15.647085189819336\n",
      "Epoch: 335, Loss: 15.618873596191406\n",
      "Epoch: 336, Loss: 15.63577651977539\n",
      "Epoch: 337, Loss: 15.594185829162598\n",
      "Epoch: 338, Loss: 15.61102294921875\n",
      "Epoch: 339, Loss: 15.585862159729004\n",
      "Epoch: 340, Loss: 15.59593677520752\n",
      "Epoch: 341, Loss: 15.582259178161621\n",
      "Epoch: 342, Loss: 15.584927558898926\n",
      "Epoch: 343, Loss: 15.567251205444336\n",
      "Epoch: 344, Loss: 15.571952819824219\n",
      "Epoch: 345, Loss: 15.565164566040039\n",
      "Epoch: 346, Loss: 15.563660621643066\n",
      "Epoch: 347, Loss: 15.55444049835205\n",
      "Epoch: 348, Loss: 15.556112289428711\n",
      "Epoch: 349, Loss: 15.552898406982422\n",
      "Epoch: 350, Loss: 15.543310165405273\n",
      "Epoch: 351, Loss: 15.54334831237793\n",
      "Epoch: 352, Loss: 15.541275024414062\n",
      "Epoch: 353, Loss: 15.538406372070312\n",
      "Epoch: 354, Loss: 15.535823822021484\n",
      "Epoch: 355, Loss: 15.535040855407715\n",
      "Epoch: 356, Loss: 15.535435676574707\n",
      "Epoch: 357, Loss: 15.52933120727539\n",
      "Epoch: 358, Loss: 15.532395362854004\n",
      "Epoch: 359, Loss: 15.527307510375977\n",
      "Epoch: 360, Loss: 15.528424263000488\n",
      "Epoch: 361, Loss: 15.526847839355469\n",
      "Epoch: 362, Loss: 15.5219144821167\n",
      "Epoch: 363, Loss: 15.522808074951172\n",
      "Epoch: 364, Loss: 15.517570495605469\n",
      "Epoch: 365, Loss: 15.519580841064453\n",
      "Epoch: 366, Loss: 15.51926326751709\n",
      "Epoch: 367, Loss: 15.517354965209961\n",
      "Epoch: 368, Loss: 15.520099639892578\n",
      "Epoch: 369, Loss: 15.518144607543945\n",
      "Epoch: 370, Loss: 15.52059268951416\n",
      "Epoch: 371, Loss: 15.522941589355469\n",
      "Epoch: 372, Loss: 15.528695106506348\n",
      "Epoch: 373, Loss: 15.537742614746094\n",
      "Epoch: 374, Loss: 15.560802459716797\n",
      "Epoch: 375, Loss: 15.570998191833496\n",
      "Epoch: 376, Loss: 15.5970458984375\n",
      "Epoch: 377, Loss: 15.606196403503418\n",
      "Epoch: 378, Loss: 15.62316608428955\n",
      "Epoch: 379, Loss: 15.6239595413208\n",
      "Epoch: 380, Loss: 15.584444999694824\n",
      "Epoch: 381, Loss: 15.563881874084473\n",
      "Epoch: 382, Loss: 15.557998657226562\n",
      "Epoch: 383, Loss: 15.563809394836426\n",
      "Epoch: 384, Loss: 15.549861907958984\n",
      "Epoch: 385, Loss: 15.557991981506348\n",
      "Epoch: 386, Loss: 15.547558784484863\n",
      "Epoch: 387, Loss: 15.537880897521973\n",
      "Epoch: 388, Loss: 15.54442310333252\n",
      "Epoch: 389, Loss: 15.53515625\n",
      "Epoch: 390, Loss: 15.536227226257324\n",
      "Epoch: 391, Loss: 15.529938697814941\n",
      "Epoch: 392, Loss: 15.52644157409668\n",
      "Epoch: 393, Loss: 15.527156829833984\n",
      "Epoch: 394, Loss: 15.525097846984863\n",
      "Epoch: 395, Loss: 15.522561073303223\n",
      "Epoch: 396, Loss: 15.516779899597168\n",
      "Epoch: 397, Loss: 15.519627571105957\n",
      "Epoch: 398, Loss: 15.517012596130371\n",
      "Epoch: 399, Loss: 15.515506744384766\n",
      "Epoch: 400, Loss: 15.515878677368164\n"
     ]
    }
   ],
   "source": [
    "model = GCN()\n",
    "criterion = torch.nn.CrossEntropyLoss()  #Initialize the CrossEntropyLoss function.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1, weight_decay=5e-4)  # Initialize the Adam optimizer.\n",
    "\n",
    "def train(node_features, edge_index, node_labels):\n",
    "\n",
    "    optimizer.zero_grad()  # Clear gradients.\n",
    "    out, h = model(node_features, edge_index)  # Perform a single forward pass.\n",
    "    loss = criterion(out, node_labels)  # Compute the loss solely based on the training nodes.\n",
    "    loss.backward()  # Derive gradients.\n",
    "    optimizer.step()  # Update parameters based on gradients.\n",
    "\n",
    "    return loss, h\n",
    "\n",
    "\n",
    "for epoch in range(401):\n",
    "    #torch_edge_weight = torch.tensor(np.sum(edge_features_x, axis=2), dtype=torch.float)\n",
    "    #torch_edge_weight = torch.tensor(np.random.randn(12,1), dtype=torch.float)\n",
    "    loss, h = train(torch_train_gnn_x, torch_edge_index, torch_train_gnn_y)\n",
    "    print(f'Epoch: {epoch}, Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(x, e, y):\n",
    "    pred, h = model(x, e)\n",
    "\n",
    "    test_correct = 0\n",
    "    for i, j in zip(pred, y):\n",
    "        pred_i  = np.argmax(i.detach().numpy().reshape(7,7), axis=1)\n",
    "        label_j = np.argmax(j.detach().numpy().reshape(7,7), axis=1)\n",
    "        test_correct += np.array_equal(pred_i, label_j)\n",
    "        test_acc = int(test_correct) / int(y.shape[0])  # Derive ratio of correct predictions.\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN Training =  0.7356959863364646 GNN Testing =  0.42970973249857713\n"
     ]
    }
   ],
   "source": [
    "print(\"GNN Training = \", test(torch_train_gnn_x, torch_edge_index, torch_train_gnn_y), \"GNN Testing = \", test(torch_test_gnn_x, torch_edge_index, torch_test_gnn_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical ML methods: RF-Gini and SVM-RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_svm_y(y):\n",
    "    svm_y = []\n",
    "    for i in y:\n",
    "        list_mapping = np.argmax(i.reshape(7,7), axis=1)\n",
    "        value = 0\n",
    "        for i in range(len(list_mapping)):\n",
    "            value += list_mapping[i]*num_qubits**i\n",
    "        svm_y.append(value)\n",
    "    return svm_y\n",
    "\n",
    "svm_train_y = get_svm_y(train_y)\n",
    "svm_test_y = get_svm_y(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM-RBF Training =  0.9648448619413607 SVM-RBF Testing =  0.7159931701764372\n",
      "SVM-RBF Training =  0.9985767150583547 SVM-RBF Testing =  0.7398975526465567\n",
      "SVM-RBF Training =  0.9990037005408483 SVM-RBF Testing =  0.7398975526465567\n",
      "SVM-RBF Training =  0.9990037005408483 SVM-RBF Testing =  0.7404667046101309\n",
      "SVM-RBF Training =  0.9990037005408483 SVM-RBF Testing =  0.7398975526465567\n",
      "SVM-RBF Training =  0.9990037005408483 SVM-RBF Testing =  0.7398975526465567\n",
      "SVM-RBF Training =  0.9990037005408483 SVM-RBF Testing =  0.7398975526465567\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "c_list = [1, 20, 40, 60, 80, 100, 500]\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "for c in c_list:\n",
    "    svc_model = SVC(kernel='rbf', C=c, gamma=1,decision_function_shape='ovo')\n",
    "    svc_model.fit(train_x, svm_train_y)\n",
    "    print(\"SVM-RBF Training = \", svc_model.score(train_x, svm_train_y), \"SVM-RBF Testing = \", svc_model.score(test_x, svm_test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF-Gini-2 Training =  0.6511528608027327 RF-Gini-2 Testing =  0.6334661354581673\n",
      "RF-Gini-2 Training =  0.6608311984059209 RF-Gini-2 Testing =  0.6420034149117815\n",
      "RF-Gini-2 Training =  0.6625391403358952 RF-Gini-2 Testing =  0.6442800227660785\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "n_estimators = [50, 500, 1000]\n",
    "for n_est in n_estimators:\n",
    "    clf = RandomForestClassifier(n_estimators=n_est, max_depth=2, random_state=0)\n",
    "    clf.fit(train_x, svm_train_y)\n",
    "    print(\"RF-Gini-2 Training = \", clf.score(train_x, svm_train_y), \"RF-Gini-2 Testing = \", clf.score(test_x, svm_test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF-Gini-10 Training =  0.9611443210930828 RF-Gini-10 Testing =  0.9197495731360273\n",
      "RF-Gini-10 Training =  0.9612866495872474 RF-Gini-10 Testing =  0.9163346613545816\n",
      "RF-Gini-10 Training =  0.9618559635639055 RF-Gini-10 Testing =  0.916903813318156\n"
     ]
    }
   ],
   "source": [
    "n_estimators = [50, 500, 1000]\n",
    "for n_est in n_estimators:\n",
    "    clf = RandomForestClassifier(n_estimators=n_est, max_depth=10, random_state=0)\n",
    "    clf.fit(train_x, svm_train_y)\n",
    "    print(\"RF-Gini-10 Training = \", clf.score(train_x, svm_train_y), \"RF-Gini-10 Testing = \", clf.score(test_x, svm_test_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eece571f",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
