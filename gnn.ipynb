{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "num_qubits = 7\n",
    "dataset_filename = \"dataset/dataset_tesi/NN1_Dataset(<=10Cx)_balanced1.csv\"\n",
    "df = pd.read_csv(dataset_filename)\n",
    "\n",
    "links = [set([0,1]), set([1,2]), set([1,3]), set([3,5]), set([4,5]), set([5,6])]\n",
    "def generate_columns(header, links, in_links=False):\n",
    "    if in_links:\n",
    "        return [header+str(i)+str(j) for i in range(num_qubits) for j in range(num_qubits) if set([i,j]) in links]\n",
    "    else:\n",
    "        return [header+str(i)+str(j) for i in range(num_qubits) for j in range(num_qubits) if set([i,j]) not in links and i!=j]\n",
    "\n",
    "useless_columns = ['Unnamed: 0', 'last_update_date', 'N_qubtis', 'N_measure', 'N_cx', 'backend_name']\n",
    "useless_columns += generate_columns(\"cx_\", links)\n",
    "useless_columns += generate_columns(\"edge_length_\", links)\n",
    "useless_columns += generate_columns(\"edge_error_\", links)\n",
    "useless_columns += [\"measure_\"+str(i) for i in range(num_qubits)]\n",
    "# Note that cx/edge_error/edge_length_xy is not neccessarily the same as cx/edge_length/edge_error_yx\n",
    "df.drop(columns=useless_columns, inplace=True)\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7026, 57), (7026, 49), (1757, 57), (1757, 49))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "df_train_x, df_train_y= train.iloc[:, :-num_qubits], train.iloc[:, -num_qubits:]\n",
    "df_test_x, df_test_y= test.iloc[:, :-num_qubits], test.iloc[:, -num_qubits:]\n",
    "\n",
    "train_x = scaler.fit_transform(df_train_x)\n",
    "test_x = scaler.fit_transform(df_test_x)\n",
    "\n",
    "\n",
    "# for every row in y, convert to 1 hot-encoding and flatten\n",
    "train_y = []\n",
    "for _, row in df_train_y.iterrows():\n",
    "    train_y.append(pd.get_dummies(row).values.flatten())\n",
    "train_y = np.array(train_y)\n",
    "\n",
    "test_y = []\n",
    "for _, row in df_test_y.iterrows():\n",
    "    test_y.append(pd.get_dummies(row).values.flatten())\n",
    "test_y = np.array(test_y)\n",
    "\n",
    "(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacky/anaconda3/envs/eece571f/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.lin1 = Linear(num_features, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin3 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin4 = Linear(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = x.relu()\n",
    "        x = self.lin2(x)\n",
    "        x = x.relu()\n",
    "        x = self.lin3(x)\n",
    "        x = x.relu()\n",
    "        x = self.lin4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(57, 49, hidden_channels=128)\n",
    "criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.03, weight_decay=5e-4)  # Define optimizer.\n",
    "\n",
    "torch_train_x = torch.tensor(train_x, dtype=torch.float)\n",
    "torch_train_y = torch.tensor(train_y, dtype=torch.float)\n",
    "torch_test_x = torch.tensor(test_x, dtype=torch.float)\n",
    "torch_test_y = torch.tensor(test_y, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(torch_train_x)  # Perform a single forward pass.\n",
    "      loss = criterion(out, torch_train_y)  # Compute the loss solely based on the training nodes.\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test(x, y):\n",
    "      model.eval()\n",
    "      pred = model(x)\n",
    "\n",
    "      test_correct = 0\n",
    "      for i, j in zip(pred, y):\n",
    "          pred_i = np.argmax(i.detach().numpy().reshape(7,7), axis=1)\n",
    "          label_j = np.argmax(j.detach().numpy().reshape(7,7), axis=1)\n",
    "          test_correct += np.array_equal(pred_i, label_j)\n",
    "      test_acc = int(test_correct) / int(y.shape[0])  # Derive ratio of correct predictions.\n",
    "      return test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 27.0800\n",
      "Epoch: 002, Loss: 21.9036\n",
      "Epoch: 003, Loss: 21.3844\n",
      "Epoch: 004, Loss: 20.5604\n",
      "Epoch: 005, Loss: 19.8649\n",
      "Epoch: 006, Loss: 19.5504\n",
      "Epoch: 007, Loss: 19.3385\n",
      "Epoch: 008, Loss: 18.7612\n",
      "Epoch: 009, Loss: 18.3172\n",
      "Epoch: 010, Loss: 17.9141\n",
      "Epoch: 011, Loss: 17.4990\n",
      "Epoch: 012, Loss: 17.2550\n",
      "Epoch: 013, Loss: 17.1155\n",
      "Epoch: 014, Loss: 17.0426\n",
      "Epoch: 015, Loss: 17.0478\n",
      "Epoch: 016, Loss: 17.0295\n",
      "Epoch: 017, Loss: 16.9399\n",
      "Epoch: 018, Loss: 16.8995\n",
      "Epoch: 019, Loss: 16.8222\n",
      "Epoch: 020, Loss: 16.7528\n",
      "Epoch: 021, Loss: 16.6812\n",
      "Epoch: 022, Loss: 16.6238\n",
      "Epoch: 023, Loss: 16.5539\n",
      "Epoch: 024, Loss: 16.4822\n",
      "Epoch: 025, Loss: 16.4323\n",
      "Epoch: 026, Loss: 16.3780\n",
      "Epoch: 027, Loss: 16.3590\n",
      "Epoch: 028, Loss: 16.3174\n",
      "Epoch: 029, Loss: 16.2600\n",
      "Epoch: 030, Loss: 16.2344\n",
      "Epoch: 031, Loss: 16.1893\n",
      "Epoch: 032, Loss: 16.1512\n",
      "Epoch: 033, Loss: 16.1436\n",
      "Epoch: 034, Loss: 16.1210\n",
      "Epoch: 035, Loss: 16.0966\n",
      "Epoch: 036, Loss: 16.0782\n",
      "Epoch: 037, Loss: 16.0537\n",
      "Epoch: 038, Loss: 16.0302\n",
      "Epoch: 039, Loss: 16.0262\n",
      "Epoch: 040, Loss: 16.0264\n",
      "Epoch: 041, Loss: 15.9744\n",
      "Epoch: 042, Loss: 15.9355\n",
      "Epoch: 043, Loss: 15.9705\n",
      "Epoch: 044, Loss: 16.0905\n",
      "Epoch: 045, Loss: 15.9119\n",
      "Epoch: 046, Loss: 15.9134\n",
      "Epoch: 047, Loss: 15.8925\n",
      "Epoch: 048, Loss: 15.7734\n",
      "Epoch: 049, Loss: 15.8487\n",
      "Epoch: 050, Loss: 15.7073\n",
      "Epoch: 051, Loss: 15.6873\n",
      "Epoch: 052, Loss: 15.6665\n",
      "Epoch: 053, Loss: 15.5709\n",
      "Epoch: 054, Loss: 15.5178\n",
      "Epoch: 055, Loss: 15.5580\n",
      "Epoch: 056, Loss: 15.6395\n",
      "Epoch: 057, Loss: 15.4488\n",
      "Epoch: 058, Loss: 15.4497\n",
      "Epoch: 059, Loss: 15.5807\n",
      "Epoch: 060, Loss: 15.4349\n",
      "Epoch: 061, Loss: 15.4125\n",
      "Epoch: 062, Loss: 15.3639\n",
      "Epoch: 063, Loss: 15.3457\n",
      "Epoch: 064, Loss: 15.3297\n",
      "Epoch: 065, Loss: 15.2980\n",
      "Epoch: 066, Loss: 15.2930\n",
      "Epoch: 067, Loss: 15.2185\n",
      "Epoch: 068, Loss: 15.2213\n",
      "Epoch: 069, Loss: 15.1990\n",
      "Epoch: 070, Loss: 15.2103\n",
      "Epoch: 071, Loss: 15.1419\n",
      "Epoch: 072, Loss: 15.1382\n",
      "Epoch: 073, Loss: 15.1409\n",
      "Epoch: 074, Loss: 15.0858\n",
      "Epoch: 075, Loss: 15.1488\n",
      "Epoch: 076, Loss: 15.1982\n",
      "Epoch: 077, Loss: 15.3702\n",
      "Epoch: 078, Loss: 15.1955\n",
      "Epoch: 079, Loss: 15.1405\n",
      "Epoch: 080, Loss: 15.2460\n",
      "Epoch: 081, Loss: 15.1418\n",
      "Epoch: 082, Loss: 15.1669\n",
      "Epoch: 083, Loss: 15.1009\n",
      "Epoch: 084, Loss: 15.0701\n",
      "Epoch: 085, Loss: 15.0600\n",
      "Epoch: 086, Loss: 15.0387\n",
      "Epoch: 087, Loss: 15.0259\n",
      "Epoch: 088, Loss: 14.9998\n",
      "Epoch: 089, Loss: 14.9913\n",
      "Epoch: 090, Loss: 14.9840\n",
      "Epoch: 091, Loss: 14.9498\n",
      "Epoch: 092, Loss: 14.9391\n",
      "Epoch: 093, Loss: 14.9323\n",
      "Epoch: 094, Loss: 14.9518\n",
      "Epoch: 095, Loss: 14.9497\n",
      "Epoch: 096, Loss: 14.9909\n",
      "Epoch: 097, Loss: 14.9849\n",
      "Epoch: 098, Loss: 15.0099\n",
      "Epoch: 099, Loss: 14.9286\n",
      "Epoch: 100, Loss: 14.9146\n",
      "Epoch: 101, Loss: 14.9266\n",
      "Epoch: 102, Loss: 14.8861\n",
      "Epoch: 103, Loss: 14.8761\n",
      "Epoch: 104, Loss: 14.8747\n",
      "Epoch: 105, Loss: 14.8305\n",
      "Epoch: 106, Loss: 14.8398\n",
      "Epoch: 107, Loss: 14.8412\n",
      "Epoch: 108, Loss: 14.9157\n",
      "Epoch: 109, Loss: 14.9811\n",
      "Epoch: 110, Loss: 14.8265\n",
      "Epoch: 111, Loss: 14.8752\n",
      "Epoch: 112, Loss: 14.8231\n",
      "Epoch: 113, Loss: 14.8312\n",
      "Epoch: 114, Loss: 14.8122\n",
      "Epoch: 115, Loss: 14.8134\n",
      "Epoch: 116, Loss: 14.8681\n",
      "Epoch: 117, Loss: 14.9722\n",
      "Epoch: 118, Loss: 15.1944\n",
      "Epoch: 119, Loss: 15.0581\n",
      "Epoch: 120, Loss: 14.8966\n",
      "Epoch: 121, Loss: 15.0533\n",
      "Epoch: 122, Loss: 14.9994\n",
      "Epoch: 123, Loss: 14.9090\n",
      "Epoch: 124, Loss: 14.9346\n",
      "Epoch: 125, Loss: 14.9034\n",
      "Epoch: 126, Loss: 14.8962\n",
      "Epoch: 127, Loss: 14.8691\n",
      "Epoch: 128, Loss: 14.8606\n",
      "Epoch: 129, Loss: 14.8336\n",
      "Epoch: 130, Loss: 14.8465\n",
      "Epoch: 131, Loss: 14.8014\n",
      "Epoch: 132, Loss: 14.8042\n",
      "Epoch: 133, Loss: 14.7908\n",
      "Epoch: 134, Loss: 14.7511\n",
      "Epoch: 135, Loss: 14.7490\n",
      "Epoch: 136, Loss: 14.7650\n",
      "Epoch: 137, Loss: 14.7170\n",
      "Epoch: 138, Loss: 14.7312\n",
      "Epoch: 139, Loss: 14.7699\n",
      "Epoch: 140, Loss: 14.7541\n",
      "Epoch: 141, Loss: 14.7845\n",
      "Epoch: 142, Loss: 14.7372\n",
      "Epoch: 143, Loss: 14.6687\n",
      "Epoch: 144, Loss: 14.7174\n",
      "Epoch: 145, Loss: 14.6884\n",
      "Epoch: 146, Loss: 14.6563\n",
      "Epoch: 147, Loss: 14.6672\n",
      "Epoch: 148, Loss: 14.6164\n",
      "Epoch: 149, Loss: 14.6387\n",
      "Epoch: 150, Loss: 14.6248\n",
      "Epoch: 151, Loss: 14.5895\n",
      "Epoch: 152, Loss: 14.6106\n",
      "Epoch: 153, Loss: 14.5766\n",
      "Epoch: 154, Loss: 14.5977\n",
      "Epoch: 155, Loss: 14.7166\n",
      "Epoch: 156, Loss: 14.7443\n",
      "Epoch: 157, Loss: 15.4330\n",
      "Epoch: 158, Loss: 14.8419\n",
      "Epoch: 159, Loss: 15.4665\n",
      "Epoch: 160, Loss: 15.7309\n",
      "Epoch: 161, Loss: 15.4032\n",
      "Epoch: 162, Loss: 15.0569\n",
      "Epoch: 163, Loss: 15.3135\n",
      "Epoch: 164, Loss: 15.2748\n",
      "Epoch: 165, Loss: 15.0643\n",
      "Epoch: 166, Loss: 15.1492\n",
      "Epoch: 167, Loss: 15.1039\n",
      "Epoch: 168, Loss: 14.9728\n",
      "Epoch: 169, Loss: 15.0047\n",
      "Epoch: 170, Loss: 14.9792\n",
      "Epoch: 171, Loss: 14.9121\n",
      "Epoch: 172, Loss: 14.8804\n",
      "Epoch: 173, Loss: 14.9018\n",
      "Epoch: 174, Loss: 14.8081\n",
      "Epoch: 175, Loss: 14.7890\n",
      "Epoch: 176, Loss: 14.8072\n",
      "Epoch: 177, Loss: 14.7676\n",
      "Epoch: 178, Loss: 14.7249\n",
      "Epoch: 179, Loss: 14.7563\n",
      "Epoch: 180, Loss: 14.6977\n",
      "Epoch: 181, Loss: 14.6853\n",
      "Epoch: 182, Loss: 14.6778\n",
      "Epoch: 183, Loss: 14.6415\n",
      "Epoch: 184, Loss: 14.6360\n",
      "Epoch: 185, Loss: 14.6169\n",
      "Epoch: 186, Loss: 14.5993\n",
      "Epoch: 187, Loss: 14.5905\n",
      "Epoch: 188, Loss: 14.5614\n",
      "Epoch: 189, Loss: 14.5537\n",
      "Epoch: 190, Loss: 14.5400\n",
      "Epoch: 191, Loss: 14.5266\n",
      "Epoch: 192, Loss: 14.5120\n",
      "Epoch: 193, Loss: 14.5056\n",
      "Epoch: 194, Loss: 14.4890\n",
      "Epoch: 195, Loss: 14.4823\n",
      "Epoch: 196, Loss: 14.4753\n",
      "Epoch: 197, Loss: 14.4677\n",
      "Epoch: 198, Loss: 14.4648\n",
      "Epoch: 199, Loss: 14.4670\n",
      "Epoch: 200, Loss: 14.4724\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 201):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9069\n",
      "Training Accuracy: 0.8645\n"
     ]
    }
   ],
   "source": [
    "training_acc = test(torch_train_x, torch_train_y)\n",
    "print(f'Training Accuracy: {training_acc:.4f}')\n",
    "\n",
    "testing_acc = test(torch_test_x, torch_test_y)\n",
    "print(f'Training Accuracy: {testing_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7026, 7, 3) (2, 12) (7026, 12, 3) (7026, 7)\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Here's how you would parse the data for GNNs if edge features can be added\n",
    "################################################################################\n",
    "\n",
    "# node_prefix = [\"T1\", \"T2\", \"readout_error\"]\n",
    "# node_features_x = []\n",
    "# for k in range(df_train_x.shape[0]):\n",
    "#     node_features_j = []\n",
    "#     for j in range(num_qubits):\n",
    "#         row_features = df_train_x.iloc[k][[i + \"_\" + str(j) for i in node_prefix]].values.flatten()\n",
    "#         node_features_j.append(row_features)\n",
    "#     node_features_j = scaler.fit_transform(node_features_j)\n",
    "#     node_features_x.append(node_features_j)\n",
    "# node_features_x = np.array(node_features_x)\n",
    "\n",
    "# edge_prefix = [\"cx_\", \"edge_length_\", \"edge_error_\"]\n",
    "# edge_index = [[],[]]\n",
    "# edge_features_x = []\n",
    "# for k in range(df_train_x.shape[0]):\n",
    "#     edge_features_j = []\n",
    "#     for i in range(num_qubits):\n",
    "#         for j in range(num_qubits):\n",
    "#             if set([i,j]) in links:\n",
    "#                 row_features = df_train_x.iloc[k][[prefix + str(i) + str(j) for prefix in edge_prefix]].values.flatten()\n",
    "#                 edge_features_j.append(row_features)\n",
    "#                 if k == 0: # only need to do this once\n",
    "#                     edge_index[0].append(i)\n",
    "#                     edge_index[1].append(j)\n",
    "#     edge_features_j = scaler.fit_transform(edge_features_j)\n",
    "#     edge_features_x.append(edge_features_j)\n",
    "# edge_features_x = np.array(edge_features_x)\n",
    "# edge_index = np.array(edge_index)\n",
    "\n",
    "# node_labels = df_train_y.to_numpy()\n",
    "# print(node_features_x.shape, edge_index.shape, edge_features_x.shape, node_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7026, 7, 12) (2, 12) (7026, 49)\n"
     ]
    }
   ],
   "source": [
    "# Since GCN doesn't support edge features, we can just concatenate them to the node features\n",
    "node_prefix = [\"T1\", \"T2\", \"readout_error\"]\n",
    "edge_prefix = [\"cx\", \"edge_length\", \"edge_error\"]\n",
    "node_features_x = []\n",
    "edge_index = [[],[]]\n",
    "for k in range(df_train_x.shape[0]):\n",
    "    node_features_i = []\n",
    "    for i in range(num_qubits):\n",
    "        node_features_i.append(list(df_train_x.iloc[k][[l + \"_\" + str(i) for l in node_prefix]].values.flatten()))\n",
    "        for j in range(num_qubits):\n",
    "            if set([i,j]) in links:\n",
    "                node_features_i[i].extend(df_train_x.iloc[k][[l + \"_\" + str(i)+str(j) for l in edge_prefix]].values.flatten())\n",
    "                if(k == 0): # only do this once\n",
    "                    edge_index[0].append(i)\n",
    "                    edge_index[1].append(j)\n",
    "        if(len(node_features_i[i]) < 12): # pad features to 12 with standard normal\n",
    "            node_features_i[i].extend(np.random.randn(12-len(node_features_i[i])))\n",
    "    node_features_i = scaler.fit_transform(node_features_i)\n",
    "    node_features_x.append(node_features_i)\n",
    "node_features_x = np.array(node_features_x)\n",
    "edge_index = np.array(edge_index)\n",
    "\n",
    "node_labels = []\n",
    "for _, row in df_train_y.iterrows():\n",
    "    node_labels.append(pd.get_dummies(row).values.flatten())\n",
    "node_labels= np.array(node_labels)\n",
    "print(node_features_x.shape, edge_index.shape, node_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(12, 12)\n",
      "  (conv2): GCNConv(12, 12)\n",
      "  (conv3): GCNConv(12, 12)\n",
      "  (classifier): Linear(in_features=84, out_features=49, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(12, 12)\n",
    "        self.conv2 = GCNConv(12, 12)\n",
    "        self.conv3 = GCNConv(12, 12)\n",
    "        self.classifier = Linear(84, 49)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = self.conv1(x, edge_index)\n",
    "        h = h.tanh()\n",
    "        h = self.conv2(h, edge_index)\n",
    "        h = h.tanh()\n",
    "        h = self.conv3(h, edge_index)\n",
    "        h = h.tanh()  # Final GNN embedding space.\n",
    "        \n",
    "        # Apply a final (linear) classifier.\n",
    "        out = self.classifier(h.flatten(start_dim=1))\n",
    "\n",
    "        return out, h\n",
    "\n",
    "model = GCN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_node_features_x = torch.tensor(node_features_x, dtype=torch.float)\n",
    "torch_edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "# torch_edge_features_x = torch.tensor(edge_features_x, dtype=torch.float)\n",
    "torch_node_labels = torch.tensor(node_labels, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 27.121423721313477\n",
      "Epoch: 1, Loss: 20.728830337524414\n",
      "Epoch: 2, Loss: 22.785831451416016\n",
      "Epoch: 3, Loss: 25.467321395874023\n",
      "Epoch: 4, Loss: 25.3110294342041\n",
      "Epoch: 5, Loss: 22.398834228515625\n",
      "Epoch: 6, Loss: 22.706439971923828\n",
      "Epoch: 7, Loss: 23.402084350585938\n",
      "Epoch: 8, Loss: 23.98918914794922\n",
      "Epoch: 9, Loss: 21.2069149017334\n",
      "Epoch: 10, Loss: 21.076271057128906\n",
      "Epoch: 11, Loss: 19.971805572509766\n",
      "Epoch: 12, Loss: 19.81683349609375\n",
      "Epoch: 13, Loss: 19.089189529418945\n",
      "Epoch: 14, Loss: 19.312286376953125\n",
      "Epoch: 15, Loss: 18.83991813659668\n",
      "Epoch: 16, Loss: 17.79392433166504\n",
      "Epoch: 17, Loss: 18.355215072631836\n",
      "Epoch: 18, Loss: 18.448511123657227\n",
      "Epoch: 19, Loss: 18.05849838256836\n",
      "Epoch: 20, Loss: 17.688302993774414\n",
      "Epoch: 21, Loss: 17.5908145904541\n",
      "Epoch: 22, Loss: 17.977209091186523\n",
      "Epoch: 23, Loss: 17.705795288085938\n",
      "Epoch: 24, Loss: 17.392627716064453\n",
      "Epoch: 25, Loss: 17.198596954345703\n",
      "Epoch: 26, Loss: 17.211027145385742\n",
      "Epoch: 27, Loss: 17.283689498901367\n",
      "Epoch: 28, Loss: 17.157384872436523\n",
      "Epoch: 29, Loss: 17.058881759643555\n",
      "Epoch: 30, Loss: 16.98021697998047\n",
      "Epoch: 31, Loss: 16.995025634765625\n",
      "Epoch: 32, Loss: 16.935850143432617\n",
      "Epoch: 33, Loss: 16.86343765258789\n",
      "Epoch: 34, Loss: 16.826662063598633\n",
      "Epoch: 35, Loss: 16.765560150146484\n",
      "Epoch: 36, Loss: 16.729944229125977\n",
      "Epoch: 37, Loss: 16.677488327026367\n",
      "Epoch: 38, Loss: 16.69504737854004\n",
      "Epoch: 39, Loss: 16.67499351501465\n",
      "Epoch: 40, Loss: 16.640777587890625\n",
      "Epoch: 41, Loss: 16.613147735595703\n",
      "Epoch: 42, Loss: 16.572179794311523\n",
      "Epoch: 43, Loss: 16.570819854736328\n",
      "Epoch: 44, Loss: 16.552043914794922\n",
      "Epoch: 45, Loss: 16.527748107910156\n",
      "Epoch: 46, Loss: 16.500896453857422\n",
      "Epoch: 47, Loss: 16.483434677124023\n",
      "Epoch: 48, Loss: 16.48341941833496\n",
      "Epoch: 49, Loss: 16.460426330566406\n",
      "Epoch: 50, Loss: 16.444616317749023\n",
      "Epoch: 51, Loss: 16.43256378173828\n",
      "Epoch: 52, Loss: 16.420303344726562\n",
      "Epoch: 53, Loss: 16.410268783569336\n",
      "Epoch: 54, Loss: 16.393503189086914\n",
      "Epoch: 55, Loss: 16.384382247924805\n",
      "Epoch: 56, Loss: 16.378955841064453\n",
      "Epoch: 57, Loss: 16.3643856048584\n",
      "Epoch: 58, Loss: 16.35228729248047\n",
      "Epoch: 59, Loss: 16.342370986938477\n",
      "Epoch: 60, Loss: 16.335411071777344\n",
      "Epoch: 61, Loss: 16.328001022338867\n",
      "Epoch: 62, Loss: 16.315763473510742\n",
      "Epoch: 63, Loss: 16.306467056274414\n",
      "Epoch: 64, Loss: 16.296833038330078\n",
      "Epoch: 65, Loss: 16.28693199157715\n",
      "Epoch: 66, Loss: 16.279382705688477\n",
      "Epoch: 67, Loss: 16.269123077392578\n",
      "Epoch: 68, Loss: 16.259586334228516\n",
      "Epoch: 69, Loss: 16.251903533935547\n",
      "Epoch: 70, Loss: 16.243282318115234\n",
      "Epoch: 71, Loss: 16.235645294189453\n",
      "Epoch: 72, Loss: 16.2283935546875\n",
      "Epoch: 73, Loss: 16.21969223022461\n",
      "Epoch: 74, Loss: 16.21154022216797\n",
      "Epoch: 75, Loss: 16.20539093017578\n",
      "Epoch: 76, Loss: 16.198720932006836\n",
      "Epoch: 77, Loss: 16.190881729125977\n",
      "Epoch: 78, Loss: 16.18377685546875\n",
      "Epoch: 79, Loss: 16.17756462097168\n",
      "Epoch: 80, Loss: 16.17053985595703\n",
      "Epoch: 81, Loss: 16.163280487060547\n",
      "Epoch: 82, Loss: 16.156814575195312\n",
      "Epoch: 83, Loss: 16.150711059570312\n",
      "Epoch: 84, Loss: 16.144065856933594\n",
      "Epoch: 85, Loss: 16.137418746948242\n",
      "Epoch: 86, Loss: 16.131092071533203\n",
      "Epoch: 87, Loss: 16.125158309936523\n",
      "Epoch: 88, Loss: 16.118865966796875\n",
      "Epoch: 89, Loss: 16.112323760986328\n",
      "Epoch: 90, Loss: 16.10611343383789\n",
      "Epoch: 91, Loss: 16.100133895874023\n",
      "Epoch: 92, Loss: 16.094209671020508\n",
      "Epoch: 93, Loss: 16.08843994140625\n",
      "Epoch: 94, Loss: 16.082744598388672\n",
      "Epoch: 95, Loss: 16.077007293701172\n",
      "Epoch: 96, Loss: 16.071382522583008\n",
      "Epoch: 97, Loss: 16.065797805786133\n",
      "Epoch: 98, Loss: 16.060218811035156\n",
      "Epoch: 99, Loss: 16.054723739624023\n",
      "Epoch: 100, Loss: 16.04941177368164\n",
      "Epoch: 101, Loss: 16.0441837310791\n",
      "Epoch: 102, Loss: 16.039134979248047\n",
      "Epoch: 103, Loss: 16.034677505493164\n",
      "Epoch: 104, Loss: 16.031959533691406\n",
      "Epoch: 105, Loss: 16.033679962158203\n",
      "Epoch: 106, Loss: 16.05473518371582\n",
      "Epoch: 107, Loss: 16.100622177124023\n",
      "Epoch: 108, Loss: 16.279827117919922\n",
      "Epoch: 109, Loss: 16.105600357055664\n",
      "Epoch: 110, Loss: 16.00775718688965\n",
      "Epoch: 111, Loss: 16.042455673217773\n",
      "Epoch: 112, Loss: 16.06029510498047\n",
      "Epoch: 113, Loss: 16.043617248535156\n",
      "Epoch: 114, Loss: 15.997506141662598\n",
      "Epoch: 115, Loss: 16.06656837463379\n",
      "Epoch: 116, Loss: 16.05558967590332\n",
      "Epoch: 117, Loss: 15.999774932861328\n",
      "Epoch: 118, Loss: 16.05852508544922\n",
      "Epoch: 119, Loss: 16.037208557128906\n",
      "Epoch: 120, Loss: 15.981637001037598\n",
      "Epoch: 121, Loss: 16.052640914916992\n",
      "Epoch: 122, Loss: 15.98705005645752\n",
      "Epoch: 123, Loss: 15.984503746032715\n",
      "Epoch: 124, Loss: 16.01262092590332\n",
      "Epoch: 125, Loss: 15.95110034942627\n",
      "Epoch: 126, Loss: 15.978466987609863\n",
      "Epoch: 127, Loss: 15.956262588500977\n",
      "Epoch: 128, Loss: 15.941547393798828\n",
      "Epoch: 129, Loss: 15.954216003417969\n",
      "Epoch: 130, Loss: 15.930835723876953\n",
      "Epoch: 131, Loss: 15.937471389770508\n",
      "Epoch: 132, Loss: 15.939120292663574\n",
      "Epoch: 133, Loss: 15.914754867553711\n",
      "Epoch: 134, Loss: 15.933402061462402\n",
      "Epoch: 135, Loss: 15.92060375213623\n",
      "Epoch: 136, Loss: 15.906147956848145\n",
      "Epoch: 137, Loss: 15.9210786819458\n",
      "Epoch: 138, Loss: 15.909162521362305\n",
      "Epoch: 139, Loss: 15.895068168640137\n",
      "Epoch: 140, Loss: 15.906492233276367\n",
      "Epoch: 141, Loss: 15.902026176452637\n",
      "Epoch: 142, Loss: 15.882271766662598\n",
      "Epoch: 143, Loss: 15.891913414001465\n",
      "Epoch: 144, Loss: 15.893112182617188\n",
      "Epoch: 145, Loss: 15.871710777282715\n",
      "Epoch: 146, Loss: 15.87635326385498\n",
      "Epoch: 147, Loss: 15.881657600402832\n",
      "Epoch: 148, Loss: 15.865385055541992\n",
      "Epoch: 149, Loss: 15.859649658203125\n",
      "Epoch: 150, Loss: 15.866334915161133\n",
      "Epoch: 151, Loss: 15.859681129455566\n",
      "Epoch: 152, Loss: 15.849250793457031\n",
      "Epoch: 153, Loss: 15.847674369812012\n",
      "Epoch: 154, Loss: 15.84980583190918\n",
      "Epoch: 155, Loss: 15.846699714660645\n",
      "Epoch: 156, Loss: 15.83840560913086\n",
      "Epoch: 157, Loss: 15.83403205871582\n",
      "Epoch: 158, Loss: 15.83480453491211\n",
      "Epoch: 159, Loss: 15.834318161010742\n",
      "Epoch: 160, Loss: 15.830599784851074\n",
      "Epoch: 161, Loss: 15.82435131072998\n",
      "Epoch: 162, Loss: 15.819849967956543\n",
      "Epoch: 163, Loss: 15.818161964416504\n",
      "Epoch: 164, Loss: 15.817717552185059\n",
      "Epoch: 165, Loss: 15.817204475402832\n",
      "Epoch: 166, Loss: 15.814765930175781\n",
      "Epoch: 167, Loss: 15.811623573303223\n",
      "Epoch: 168, Loss: 15.807519912719727\n",
      "Epoch: 169, Loss: 15.803854942321777\n",
      "Epoch: 170, Loss: 15.800263404846191\n",
      "Epoch: 171, Loss: 15.797149658203125\n",
      "Epoch: 172, Loss: 15.794330596923828\n",
      "Epoch: 173, Loss: 15.791657447814941\n",
      "Epoch: 174, Loss: 15.789151191711426\n",
      "Epoch: 175, Loss: 15.786703109741211\n",
      "Epoch: 176, Loss: 15.784334182739258\n",
      "Epoch: 177, Loss: 15.782106399536133\n",
      "Epoch: 178, Loss: 15.780195236206055\n",
      "Epoch: 179, Loss: 15.779166221618652\n",
      "Epoch: 180, Loss: 15.780646324157715\n",
      "Epoch: 181, Loss: 15.791419982910156\n",
      "Epoch: 182, Loss: 15.823942184448242\n",
      "Epoch: 183, Loss: 15.94653606414795\n",
      "Epoch: 184, Loss: 16.027894973754883\n",
      "Epoch: 185, Loss: 16.111650466918945\n",
      "Epoch: 186, Loss: 15.793550491333008\n",
      "Epoch: 187, Loss: 16.091169357299805\n",
      "Epoch: 188, Loss: 16.307598114013672\n",
      "Epoch: 189, Loss: 16.074731826782227\n",
      "Epoch: 190, Loss: 16.093494415283203\n",
      "Epoch: 191, Loss: 15.967639923095703\n",
      "Epoch: 192, Loss: 16.02598762512207\n",
      "Epoch: 193, Loss: 16.00020408630371\n",
      "Epoch: 194, Loss: 15.911809921264648\n",
      "Epoch: 195, Loss: 15.9662446975708\n",
      "Epoch: 196, Loss: 15.894436836242676\n",
      "Epoch: 197, Loss: 15.914373397827148\n",
      "Epoch: 198, Loss: 15.87353515625\n",
      "Epoch: 199, Loss: 15.885931968688965\n",
      "Epoch: 200, Loss: 15.84676742553711\n",
      "Epoch: 201, Loss: 15.860750198364258\n",
      "Epoch: 202, Loss: 15.827628135681152\n",
      "Epoch: 203, Loss: 15.831106185913086\n",
      "Epoch: 204, Loss: 15.813368797302246\n",
      "Epoch: 205, Loss: 15.810842514038086\n",
      "Epoch: 206, Loss: 15.789679527282715\n",
      "Epoch: 207, Loss: 15.801645278930664\n",
      "Epoch: 208, Loss: 15.782777786254883\n",
      "Epoch: 209, Loss: 15.774637222290039\n",
      "Epoch: 210, Loss: 15.784446716308594\n",
      "Epoch: 211, Loss: 15.75744915008545\n",
      "Epoch: 212, Loss: 15.767977714538574\n",
      "Epoch: 213, Loss: 15.755454063415527\n",
      "Epoch: 214, Loss: 15.747039794921875\n",
      "Epoch: 215, Loss: 15.751708984375\n",
      "Epoch: 216, Loss: 15.73666000366211\n",
      "Epoch: 217, Loss: 15.742735862731934\n",
      "Epoch: 218, Loss: 15.729866027832031\n",
      "Epoch: 219, Loss: 15.733570098876953\n",
      "Epoch: 220, Loss: 15.724625587463379\n",
      "Epoch: 221, Loss: 15.724908828735352\n",
      "Epoch: 222, Loss: 15.719350814819336\n",
      "Epoch: 223, Loss: 15.717317581176758\n",
      "Epoch: 224, Loss: 15.714020729064941\n",
      "Epoch: 225, Loss: 15.709906578063965\n",
      "Epoch: 226, Loss: 15.709858894348145\n",
      "Epoch: 227, Loss: 15.703852653503418\n",
      "Epoch: 228, Loss: 15.704761505126953\n",
      "Epoch: 229, Loss: 15.699241638183594\n",
      "Epoch: 230, Loss: 15.698760986328125\n",
      "Epoch: 231, Loss: 15.695371627807617\n",
      "Epoch: 232, Loss: 15.693215370178223\n",
      "Epoch: 233, Loss: 15.691157341003418\n",
      "Epoch: 234, Loss: 15.688703536987305\n",
      "Epoch: 235, Loss: 15.686507225036621\n",
      "Epoch: 236, Loss: 15.684403419494629\n",
      "Epoch: 237, Loss: 15.68237590789795\n",
      "Epoch: 238, Loss: 15.680088996887207\n",
      "Epoch: 239, Loss: 15.678287506103516\n",
      "Epoch: 240, Loss: 15.676094055175781\n",
      "Epoch: 241, Loss: 15.674227714538574\n",
      "Epoch: 242, Loss: 15.672154426574707\n",
      "Epoch: 243, Loss: 15.670315742492676\n",
      "Epoch: 244, Loss: 15.668482780456543\n",
      "Epoch: 245, Loss: 15.666406631469727\n",
      "Epoch: 246, Loss: 15.664833068847656\n",
      "Epoch: 247, Loss: 15.662891387939453\n",
      "Epoch: 248, Loss: 15.66107177734375\n",
      "Epoch: 249, Loss: 15.659499168395996\n",
      "Epoch: 250, Loss: 15.657303810119629\n",
      "Epoch: 251, Loss: 15.656051635742188\n",
      "Epoch: 252, Loss: 15.653890609741211\n",
      "Epoch: 253, Loss: 15.652373313903809\n",
      "Epoch: 254, Loss: 15.650555610656738\n",
      "Epoch: 255, Loss: 15.64891242980957\n",
      "Epoch: 256, Loss: 15.64714241027832\n",
      "Epoch: 257, Loss: 15.645498275756836\n",
      "Epoch: 258, Loss: 15.643782615661621\n",
      "Epoch: 259, Loss: 15.642130851745605\n",
      "Epoch: 260, Loss: 15.640493392944336\n",
      "Epoch: 261, Loss: 15.638799667358398\n",
      "Epoch: 262, Loss: 15.637178421020508\n",
      "Epoch: 263, Loss: 15.63555908203125\n",
      "Epoch: 264, Loss: 15.633879661560059\n",
      "Epoch: 265, Loss: 15.632308959960938\n",
      "Epoch: 266, Loss: 15.630675315856934\n",
      "Epoch: 267, Loss: 15.629053115844727\n",
      "Epoch: 268, Loss: 15.627477645874023\n",
      "Epoch: 269, Loss: 15.625853538513184\n",
      "Epoch: 270, Loss: 15.624284744262695\n",
      "Epoch: 271, Loss: 15.62270450592041\n",
      "Epoch: 272, Loss: 15.621159553527832\n",
      "Epoch: 273, Loss: 15.619568824768066\n",
      "Epoch: 274, Loss: 15.618045806884766\n",
      "Epoch: 275, Loss: 15.616472244262695\n",
      "Epoch: 276, Loss: 15.614945411682129\n",
      "Epoch: 277, Loss: 15.613409042358398\n",
      "Epoch: 278, Loss: 15.611900329589844\n",
      "Epoch: 279, Loss: 15.61037540435791\n",
      "Epoch: 280, Loss: 15.608874320983887\n",
      "Epoch: 281, Loss: 15.60738468170166\n",
      "Epoch: 282, Loss: 15.605884552001953\n",
      "Epoch: 283, Loss: 15.60441780090332\n",
      "Epoch: 284, Loss: 15.602940559387207\n",
      "Epoch: 285, Loss: 15.601484298706055\n",
      "Epoch: 286, Loss: 15.600027084350586\n",
      "Epoch: 287, Loss: 15.598593711853027\n",
      "Epoch: 288, Loss: 15.597161293029785\n",
      "Epoch: 289, Loss: 15.595768928527832\n",
      "Epoch: 290, Loss: 15.594420433044434\n",
      "Epoch: 291, Loss: 15.593182563781738\n",
      "Epoch: 292, Loss: 15.592172622680664\n",
      "Epoch: 293, Loss: 15.591773986816406\n",
      "Epoch: 294, Loss: 15.592669486999512\n",
      "Epoch: 295, Loss: 15.597636222839355\n",
      "Epoch: 296, Loss: 15.609517097473145\n",
      "Epoch: 297, Loss: 15.648198127746582\n",
      "Epoch: 298, Loss: 15.688011169433594\n",
      "Epoch: 299, Loss: 15.787126541137695\n",
      "Epoch: 300, Loss: 15.666770935058594\n",
      "Epoch: 301, Loss: 15.60330581665039\n",
      "Epoch: 302, Loss: 15.662267684936523\n",
      "Epoch: 303, Loss: 15.648691177368164\n",
      "Epoch: 304, Loss: 15.602621078491211\n",
      "Epoch: 305, Loss: 15.6048002243042\n",
      "Epoch: 306, Loss: 15.635232925415039\n",
      "Epoch: 307, Loss: 15.627115249633789\n",
      "Epoch: 308, Loss: 15.584582328796387\n",
      "Epoch: 309, Loss: 15.619226455688477\n",
      "Epoch: 310, Loss: 15.658164978027344\n",
      "Epoch: 311, Loss: 15.605886459350586\n",
      "Epoch: 312, Loss: 15.583638191223145\n",
      "Epoch: 313, Loss: 15.63662338256836\n",
      "Epoch: 314, Loss: 15.607007026672363\n",
      "Epoch: 315, Loss: 15.577162742614746\n",
      "Epoch: 316, Loss: 15.59223461151123\n",
      "Epoch: 317, Loss: 15.59713363647461\n",
      "Epoch: 318, Loss: 15.588953971862793\n",
      "Epoch: 319, Loss: 15.566694259643555\n",
      "Epoch: 320, Loss: 15.594791412353516\n",
      "Epoch: 321, Loss: 15.602777481079102\n",
      "Epoch: 322, Loss: 15.576294898986816\n",
      "Epoch: 323, Loss: 15.567837715148926\n",
      "Epoch: 324, Loss: 15.602980613708496\n",
      "Epoch: 325, Loss: 15.590964317321777\n",
      "Epoch: 326, Loss: 15.567873001098633\n",
      "Epoch: 327, Loss: 15.565084457397461\n",
      "Epoch: 328, Loss: 15.582398414611816\n",
      "Epoch: 329, Loss: 15.592585563659668\n",
      "Epoch: 330, Loss: 15.556617736816406\n",
      "Epoch: 331, Loss: 15.567854881286621\n",
      "Epoch: 332, Loss: 15.590841293334961\n",
      "Epoch: 333, Loss: 15.57996654510498\n",
      "Epoch: 334, Loss: 15.549187660217285\n",
      "Epoch: 335, Loss: 15.575276374816895\n",
      "Epoch: 336, Loss: 15.580936431884766\n",
      "Epoch: 337, Loss: 15.575601577758789\n",
      "Epoch: 338, Loss: 15.545703887939453\n",
      "Epoch: 339, Loss: 15.570359230041504\n",
      "Epoch: 340, Loss: 15.585241317749023\n",
      "Epoch: 341, Loss: 15.565811157226562\n",
      "Epoch: 342, Loss: 15.545211791992188\n",
      "Epoch: 343, Loss: 15.567940711975098\n",
      "Epoch: 344, Loss: 15.570647239685059\n",
      "Epoch: 345, Loss: 15.560026168823242\n",
      "Epoch: 346, Loss: 15.538607597351074\n",
      "Epoch: 347, Loss: 15.554832458496094\n",
      "Epoch: 348, Loss: 15.575465202331543\n",
      "Epoch: 349, Loss: 15.544532775878906\n",
      "Epoch: 350, Loss: 15.533507347106934\n",
      "Epoch: 351, Loss: 15.550163269042969\n",
      "Epoch: 352, Loss: 15.555282592773438\n",
      "Epoch: 353, Loss: 15.539692878723145\n",
      "Epoch: 354, Loss: 15.53056526184082\n",
      "Epoch: 355, Loss: 15.539083480834961\n",
      "Epoch: 356, Loss: 15.558309555053711\n",
      "Epoch: 357, Loss: 15.541794776916504\n",
      "Epoch: 358, Loss: 15.53367805480957\n",
      "Epoch: 359, Loss: 15.534534454345703\n",
      "Epoch: 360, Loss: 15.545180320739746\n",
      "Epoch: 361, Loss: 15.54240608215332\n",
      "Epoch: 362, Loss: 15.52936840057373\n",
      "Epoch: 363, Loss: 15.518608093261719\n",
      "Epoch: 364, Loss: 15.52017879486084\n",
      "Epoch: 365, Loss: 15.525188446044922\n",
      "Epoch: 366, Loss: 15.525861740112305\n",
      "Epoch: 367, Loss: 15.5187349319458\n",
      "Epoch: 368, Loss: 15.511428833007812\n",
      "Epoch: 369, Loss: 15.512446403503418\n",
      "Epoch: 370, Loss: 15.515606880187988\n",
      "Epoch: 371, Loss: 15.520256996154785\n",
      "Epoch: 372, Loss: 15.517581939697266\n",
      "Epoch: 373, Loss: 15.51486587524414\n",
      "Epoch: 374, Loss: 15.510842323303223\n",
      "Epoch: 375, Loss: 15.5096435546875\n",
      "Epoch: 376, Loss: 15.5098237991333\n",
      "Epoch: 377, Loss: 15.511953353881836\n",
      "Epoch: 378, Loss: 15.513047218322754\n",
      "Epoch: 379, Loss: 15.51292896270752\n",
      "Epoch: 380, Loss: 15.510260581970215\n",
      "Epoch: 381, Loss: 15.506551742553711\n",
      "Epoch: 382, Loss: 15.501862525939941\n",
      "Epoch: 383, Loss: 15.498127937316895\n",
      "Epoch: 384, Loss: 15.495382308959961\n",
      "Epoch: 385, Loss: 15.494022369384766\n",
      "Epoch: 386, Loss: 15.493252754211426\n",
      "Epoch: 387, Loss: 15.493562698364258\n",
      "Epoch: 388, Loss: 15.494406700134277\n",
      "Epoch: 389, Loss: 15.496292114257812\n",
      "Epoch: 390, Loss: 15.499484062194824\n",
      "Epoch: 391, Loss: 15.50551986694336\n",
      "Epoch: 392, Loss: 15.51390266418457\n",
      "Epoch: 393, Loss: 15.530210494995117\n",
      "Epoch: 394, Loss: 15.537423133850098\n",
      "Epoch: 395, Loss: 15.566353797912598\n",
      "Epoch: 396, Loss: 15.549796104431152\n",
      "Epoch: 397, Loss: 15.546582221984863\n",
      "Epoch: 398, Loss: 15.53852653503418\n",
      "Epoch: 399, Loss: 15.567639350891113\n",
      "Epoch: 400, Loss: 15.535099983215332\n"
     ]
    }
   ],
   "source": [
    "model = GCN()\n",
    "criterion = torch.nn.CrossEntropyLoss()  #Initialize the CrossEntropyLoss function.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1, weight_decay=5e-4)  # Initialize the Adam optimizer.\n",
    "\n",
    "def train(node_features, edge_index, node_labels):\n",
    "\n",
    "    # for i in range(node_features.shape[0]):\n",
    "    #     optimizer.zero_grad()  # Clear gradients.\n",
    "    #     out, h = model(node_features[i], edge_index)  # Perform a single forward pass.\n",
    "    #     loss = criterion(out, node_labels[i])  # Compute the loss solely based on the training nodes.\n",
    "    #     loss.backward()  # Derive gradients.\n",
    "    #     optimizer.step()  # Update parameters based on gradients.\n",
    "    optimizer.zero_grad()  # Clear gradients.\n",
    "    out, h = model(node_features, edge_index)  # Perform a single forward pass.\n",
    "    loss = criterion(out, node_labels)  # Compute the loss solely based on the training nodes.\n",
    "    loss.backward()  # Derive gradients.\n",
    "    optimizer.step()  # Update parameters based on gradients.\n",
    "\n",
    "    return loss, h\n",
    "\n",
    "\n",
    "for epoch in range(401):\n",
    "    #torch_edge_weight = torch.tensor(np.sum(edge_features_x, axis=2), dtype=torch.float)\n",
    "    #torch_edge_weight = torch.tensor(np.random.randn(12,1), dtype=torch.float)\n",
    "    loss, h = train(torch_node_features_x, torch_edge_index, torch_node_labels)\n",
    "    print(f'Epoch: {epoch}, Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(x, e, y):\n",
    "    pred, h = model(x, e)\n",
    "\n",
    "    test_correct = 0\n",
    "    for i, j in zip(pred, y):\n",
    "        pred_i  = np.argmax(i.detach().numpy().reshape(7,7), axis=1)\n",
    "        label_j = np.argmax(j.detach().numpy().reshape(7,7), axis=1)\n",
    "        test_correct += np.array_equal(pred_i, label_j)\n",
    "        test_acc = int(test_correct) / int(y.shape[0])  # Derive ratio of correct predictions.\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7433817250213492"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(torch_node_features_x, torch_edge_index, torch_node_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eece571f",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
