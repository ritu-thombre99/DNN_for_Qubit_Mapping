{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "num_qubits = 7\n",
    "dataset_filename = \"dataset/dataset_tesi/NN1_Dataset(<=10Cx)_balanced1.csv\"\n",
    "df = pd.read_csv(dataset_filename)\n",
    "\n",
    "links = [set([0,1]), set([1,2]), set([1,3]), set([3,5]), set([4,5]), set([5,6])]\n",
    "def generate_columns(header, links, in_links=False):\n",
    "    if in_links:\n",
    "        return [header+str(i)+str(j) for i in range(num_qubits) for j in range(num_qubits) if set([i,j]) in links]\n",
    "    else:\n",
    "        return [header+str(i)+str(j) for i in range(num_qubits) for j in range(num_qubits) if set([i,j]) not in links and i!=j]\n",
    "\n",
    "useless_columns = ['Unnamed: 0', 'last_update_date', 'N_qubtis', 'N_measure', 'N_cx', 'backend_name']\n",
    "useless_columns += generate_columns(\"cx_\", links)\n",
    "useless_columns += generate_columns(\"edge_length_\", links)\n",
    "useless_columns += generate_columns(\"edge_error_\", links)\n",
    "useless_columns += [\"measure_\"+str(i) for i in range(num_qubits)]\n",
    "# Note that cx/edge_error/edge_length_xy is not neccessarily the same as cx/edge_length/edge_error_yx\n",
    "df.drop(columns=useless_columns, inplace=True)\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "df_train_x, df_train_y= train.iloc[:, :-num_qubits], train.iloc[:, -num_qubits:]\n",
    "df_test_x, df_test_y= test.iloc[:, :-num_qubits], test.iloc[:, -num_qubits:]\n",
    "\n",
    "train_x = scaler.fit_transform(df_train_x)\n",
    "test_x = scaler.fit_transform(df_test_x)\n",
    "\n",
    "\n",
    "# for every row in y, convert to 1 hot-encoding and flatten\n",
    "train_y = []\n",
    "for _, row in df_train_y.iterrows():\n",
    "    train_y.append(pd.get_dummies(row).values.flatten())\n",
    "train_y = np.array(train_y)\n",
    "\n",
    "test_y = []\n",
    "for _, row in df_test_y.iterrows():\n",
    "    test_y.append(pd.get_dummies(row).values.flatten())\n",
    "test_y = np.array(test_y)\n",
    "\n",
    "(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.lin1 = Linear(num_features, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin3 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin4 = Linear(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = x.relu()\n",
    "        x = self.lin2(x)\n",
    "        x = x.relu()\n",
    "        x = self.lin3(x)\n",
    "        x = x.relu()\n",
    "        x = self.lin4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(57, 49, hidden_channels=128)\n",
    "criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)  # Define optimizer.\n",
    "\n",
    "torch_train_x = torch.tensor(train_x, dtype=torch.float)\n",
    "torch_train_y = torch.tensor(train_y, dtype=torch.float)\n",
    "torch_test_x = torch.tensor(test_x, dtype=torch.float)\n",
    "torch_test_y = torch.tensor(test_y, dtype=torch.float)\n",
    "\n",
    "def train():\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(torch_train_x)  # Perform a single forward pass.\n",
    "      loss = criterion(out, torch_train_y)  # Compute the loss solely based on the training nodes.\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test():\n",
    "      model.eval()\n",
    "      out = model(torch_train_x)\n",
    "      pred = out  # Use the class with highest probability.\n",
    "\n",
    "      test_correct = 0\n",
    "      for i, j in zip(pred, torch_train_y):\n",
    "          pred_i = np.argmax(i.detach().numpy().reshape(7,7), axis=1)\n",
    "          train_y = np.argmax(j.detach().numpy().reshape(7,7), axis=1)\n",
    "          test_correct += np.array_equal(pred_i, train_y)\n",
    "\n",
    "      test_acc = int(test_correct) / int(torch_train_y.shape[0])  # Derive ratio of correct predictions.\n",
    "      return test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 201):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_acc = test()\n",
    "print(f'Training Accuracy: {training_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_prefix = [\"T1\", \"T2\", \"readout_error\"]\n",
    "node_features_x = []\n",
    "for k in range(df_train_x.shape[0]):\n",
    "    node_features_j = []\n",
    "    for j in range(num_qubits):\n",
    "        row_features = df_train_x.iloc[k][[i + \"_\" + str(j) for i in node_prefix]].values.flatten()\n",
    "        node_features_j.append(row_features)\n",
    "    node_features_j = scaler.fit_transform(node_features_j)\n",
    "    node_features_x.append(node_features_j)\n",
    "node_features_x = np.array(node_features_x)\n",
    "\n",
    "edge_prefix = [\"cx_\", \"edge_length_\", \"edge_error_\"]\n",
    "edge_index = [[],[]]\n",
    "edge_features_x = []\n",
    "for k in range(df_train_x.shape[0]):\n",
    "    edge_features_j = []\n",
    "    for i in range(num_qubits):\n",
    "        for j in range(num_qubits):\n",
    "            if set([i,j]) in links:\n",
    "                row_features = df_train_x.iloc[k][[prefix + str(i) + str(j) for prefix in edge_prefix]].values.flatten()\n",
    "                edge_features_j.append(row_features)\n",
    "                if k == 0: # only need to do this once\n",
    "                    edge_index[0].append(i)\n",
    "                    edge_index[1].append(j)\n",
    "    edge_features_j = scaler.fit_transform(edge_features_j)\n",
    "    edge_features_x.append(edge_features_j)\n",
    "edge_features_x = np.array(edge_features_x)\n",
    "edge_index = np.array(edge_index)\n",
    "\n",
    "node_labels = df_train_y.to_numpy()\n",
    "print(node_features_x.shape, edge_index.shape, edge_features_x.shape, node_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(3, 2)\n",
    "        self.conv2 = GCNConv(2, 2)\n",
    "        self.conv3 = GCNConv(2, 1)\n",
    "        self.classifier = Linear(1, 7)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        h = self.conv1(x, edge_index, edge_attr)\n",
    "        h = h.tanh()\n",
    "        h = self.conv2(h, edge_index, edge_attr)\n",
    "        h = h.tanh()\n",
    "        h = self.conv3(h, edge_index, edge_attr)\n",
    "        h = h.tanh()  # Final GNN embedding space.\n",
    "        \n",
    "        # Apply a final (linear) classifier.\n",
    "        out = self.classifier(h)\n",
    "\n",
    "        return out, h\n",
    "\n",
    "model = GCN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_node_features_x = torch.tensor(node_features_x, dtype=torch.float)\n",
    "torch_edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "torch_edge_features_x = torch.tensor(edge_features_x, dtype=torch.float)\n",
    "torch_node_labels = torch.tensor(node_labels, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: nan\n",
      "Epoch: 1, Loss: nan\n",
      "Epoch: 2, Loss: nan\n",
      "Epoch: 3, Loss: nan\n",
      "Epoch: 4, Loss: nan\n",
      "Epoch: 5, Loss: nan\n",
      "Epoch: 6, Loss: nan\n",
      "Epoch: 7, Loss: nan\n",
      "Epoch: 8, Loss: nan\n",
      "Epoch: 9, Loss: nan\n",
      "Epoch: 10, Loss: nan\n",
      "Epoch: 11, Loss: nan\n",
      "Epoch: 12, Loss: nan\n",
      "Epoch: 13, Loss: nan\n",
      "Epoch: 14, Loss: nan\n",
      "Epoch: 15, Loss: nan\n",
      "Epoch: 16, Loss: nan\n",
      "Epoch: 17, Loss: nan\n",
      "Epoch: 18, Loss: nan\n",
      "Epoch: 19, Loss: nan\n",
      "Epoch: 20, Loss: nan\n",
      "Epoch: 21, Loss: nan\n",
      "Epoch: 22, Loss: nan\n",
      "Epoch: 23, Loss: nan\n",
      "Epoch: 24, Loss: nan\n",
      "Epoch: 25, Loss: nan\n",
      "Epoch: 26, Loss: nan\n",
      "Epoch: 27, Loss: nan\n",
      "Epoch: 28, Loss: nan\n",
      "Epoch: 29, Loss: nan\n",
      "Epoch: 30, Loss: nan\n",
      "Epoch: 31, Loss: nan\n",
      "Epoch: 32, Loss: nan\n",
      "Epoch: 33, Loss: nan\n",
      "Epoch: 34, Loss: nan\n",
      "Epoch: 35, Loss: nan\n",
      "Epoch: 36, Loss: nan\n",
      "Epoch: 37, Loss: nan\n",
      "Epoch: 38, Loss: nan\n",
      "Epoch: 39, Loss: nan\n",
      "Epoch: 40, Loss: nan\n",
      "Epoch: 41, Loss: nan\n",
      "Epoch: 42, Loss: nan\n",
      "Epoch: 43, Loss: nan\n",
      "Epoch: 44, Loss: nan\n",
      "Epoch: 45, Loss: nan\n",
      "Epoch: 46, Loss: nan\n",
      "Epoch: 47, Loss: nan\n",
      "Epoch: 48, Loss: nan\n",
      "Epoch: 49, Loss: nan\n",
      "Epoch: 50, Loss: nan\n",
      "Epoch: 51, Loss: nan\n",
      "Epoch: 52, Loss: nan\n",
      "Epoch: 53, Loss: nan\n",
      "Epoch: 54, Loss: nan\n",
      "Epoch: 55, Loss: nan\n",
      "Epoch: 56, Loss: nan\n",
      "Epoch: 57, Loss: nan\n",
      "Epoch: 58, Loss: nan\n",
      "Epoch: 59, Loss: nan\n",
      "Epoch: 60, Loss: nan\n",
      "Epoch: 61, Loss: nan\n",
      "Epoch: 62, Loss: nan\n",
      "Epoch: 63, Loss: nan\n",
      "Epoch: 64, Loss: nan\n",
      "Epoch: 65, Loss: nan\n",
      "Epoch: 66, Loss: nan\n",
      "Epoch: 67, Loss: nan\n",
      "Epoch: 68, Loss: nan\n",
      "Epoch: 69, Loss: nan\n",
      "Epoch: 70, Loss: nan\n",
      "Epoch: 71, Loss: nan\n",
      "Epoch: 72, Loss: nan\n",
      "Epoch: 73, Loss: nan\n",
      "Epoch: 74, Loss: nan\n",
      "Epoch: 75, Loss: nan\n",
      "Epoch: 76, Loss: nan\n",
      "Epoch: 77, Loss: nan\n",
      "Epoch: 78, Loss: nan\n",
      "Epoch: 79, Loss: nan\n",
      "Epoch: 80, Loss: nan\n",
      "Epoch: 81, Loss: nan\n",
      "Epoch: 82, Loss: nan\n",
      "Epoch: 83, Loss: nan\n",
      "Epoch: 84, Loss: nan\n",
      "Epoch: 85, Loss: nan\n",
      "Epoch: 86, Loss: nan\n",
      "Epoch: 87, Loss: nan\n",
      "Epoch: 88, Loss: nan\n",
      "Epoch: 89, Loss: nan\n",
      "Epoch: 90, Loss: nan\n",
      "Epoch: 91, Loss: nan\n",
      "Epoch: 92, Loss: nan\n",
      "Epoch: 93, Loss: nan\n",
      "Epoch: 94, Loss: nan\n",
      "Epoch: 95, Loss: nan\n",
      "Epoch: 96, Loss: nan\n",
      "Epoch: 97, Loss: nan\n",
      "Epoch: 98, Loss: nan\n",
      "Epoch: 99, Loss: nan\n",
      "Epoch: 100, Loss: nan\n",
      "Epoch: 101, Loss: nan\n",
      "Epoch: 102, Loss: nan\n",
      "Epoch: 103, Loss: nan\n",
      "Epoch: 104, Loss: nan\n",
      "Epoch: 105, Loss: nan\n",
      "Epoch: 106, Loss: nan\n",
      "Epoch: 107, Loss: nan\n",
      "Epoch: 108, Loss: nan\n",
      "Epoch: 109, Loss: nan\n",
      "Epoch: 110, Loss: nan\n",
      "Epoch: 111, Loss: nan\n",
      "Epoch: 112, Loss: nan\n",
      "Epoch: 113, Loss: nan\n",
      "Epoch: 114, Loss: nan\n",
      "Epoch: 115, Loss: nan\n",
      "Epoch: 116, Loss: nan\n",
      "Epoch: 117, Loss: nan\n",
      "Epoch: 118, Loss: nan\n",
      "Epoch: 119, Loss: nan\n",
      "Epoch: 120, Loss: nan\n",
      "Epoch: 121, Loss: nan\n",
      "Epoch: 122, Loss: nan\n",
      "Epoch: 123, Loss: nan\n",
      "Epoch: 124, Loss: nan\n",
      "Epoch: 125, Loss: nan\n",
      "Epoch: 126, Loss: nan\n",
      "Epoch: 127, Loss: nan\n",
      "Epoch: 128, Loss: nan\n",
      "Epoch: 129, Loss: nan\n",
      "Epoch: 130, Loss: nan\n",
      "Epoch: 131, Loss: nan\n",
      "Epoch: 132, Loss: nan\n",
      "Epoch: 133, Loss: nan\n",
      "Epoch: 134, Loss: nan\n",
      "Epoch: 135, Loss: nan\n",
      "Epoch: 136, Loss: nan\n",
      "Epoch: 137, Loss: nan\n",
      "Epoch: 138, Loss: nan\n",
      "Epoch: 139, Loss: nan\n",
      "Epoch: 140, Loss: nan\n",
      "Epoch: 141, Loss: nan\n",
      "Epoch: 142, Loss: nan\n",
      "Epoch: 143, Loss: nan\n",
      "Epoch: 144, Loss: nan\n",
      "Epoch: 145, Loss: nan\n",
      "Epoch: 146, Loss: nan\n",
      "Epoch: 147, Loss: nan\n",
      "Epoch: 148, Loss: nan\n",
      "Epoch: 149, Loss: nan\n",
      "Epoch: 150, Loss: nan\n",
      "Epoch: 151, Loss: nan\n",
      "Epoch: 152, Loss: nan\n",
      "Epoch: 153, Loss: nan\n",
      "Epoch: 154, Loss: nan\n",
      "Epoch: 155, Loss: nan\n",
      "Epoch: 156, Loss: nan\n",
      "Epoch: 157, Loss: nan\n",
      "Epoch: 158, Loss: nan\n",
      "Epoch: 159, Loss: nan\n",
      "Epoch: 160, Loss: nan\n",
      "Epoch: 161, Loss: nan\n",
      "Epoch: 162, Loss: nan\n",
      "Epoch: 163, Loss: nan\n",
      "Epoch: 164, Loss: nan\n",
      "Epoch: 165, Loss: nan\n",
      "Epoch: 166, Loss: nan\n",
      "Epoch: 167, Loss: nan\n",
      "Epoch: 168, Loss: nan\n",
      "Epoch: 169, Loss: nan\n",
      "Epoch: 170, Loss: nan\n",
      "Epoch: 171, Loss: nan\n",
      "Epoch: 172, Loss: nan\n",
      "Epoch: 173, Loss: nan\n",
      "Epoch: 174, Loss: nan\n",
      "Epoch: 175, Loss: nan\n",
      "Epoch: 176, Loss: nan\n",
      "Epoch: 177, Loss: nan\n",
      "Epoch: 178, Loss: nan\n",
      "Epoch: 179, Loss: nan\n",
      "Epoch: 180, Loss: nan\n",
      "Epoch: 181, Loss: nan\n",
      "Epoch: 182, Loss: nan\n",
      "Epoch: 183, Loss: nan\n",
      "Epoch: 184, Loss: nan\n",
      "Epoch: 185, Loss: nan\n",
      "Epoch: 186, Loss: nan\n",
      "Epoch: 187, Loss: nan\n",
      "Epoch: 188, Loss: nan\n",
      "Epoch: 189, Loss: nan\n",
      "Epoch: 190, Loss: nan\n",
      "Epoch: 191, Loss: nan\n",
      "Epoch: 192, Loss: nan\n",
      "Epoch: 193, Loss: nan\n",
      "Epoch: 194, Loss: nan\n",
      "Epoch: 195, Loss: nan\n",
      "Epoch: 196, Loss: nan\n",
      "Epoch: 197, Loss: nan\n",
      "Epoch: 198, Loss: nan\n",
      "Epoch: 199, Loss: nan\n",
      "Epoch: 200, Loss: nan\n",
      "Epoch: 201, Loss: nan\n",
      "Epoch: 202, Loss: nan\n",
      "Epoch: 203, Loss: nan\n",
      "Epoch: 204, Loss: nan\n",
      "Epoch: 205, Loss: nan\n",
      "Epoch: 206, Loss: nan\n",
      "Epoch: 207, Loss: nan\n",
      "Epoch: 208, Loss: nan\n",
      "Epoch: 209, Loss: nan\n",
      "Epoch: 210, Loss: nan\n",
      "Epoch: 211, Loss: nan\n",
      "Epoch: 212, Loss: nan\n",
      "Epoch: 213, Loss: nan\n",
      "Epoch: 214, Loss: nan\n",
      "Epoch: 215, Loss: nan\n",
      "Epoch: 216, Loss: nan\n",
      "Epoch: 217, Loss: nan\n",
      "Epoch: 218, Loss: nan\n",
      "Epoch: 219, Loss: nan\n",
      "Epoch: 220, Loss: nan\n",
      "Epoch: 221, Loss: nan\n",
      "Epoch: 222, Loss: nan\n",
      "Epoch: 223, Loss: nan\n",
      "Epoch: 224, Loss: nan\n",
      "Epoch: 225, Loss: nan\n",
      "Epoch: 226, Loss: nan\n",
      "Epoch: 227, Loss: nan\n",
      "Epoch: 228, Loss: nan\n",
      "Epoch: 229, Loss: nan\n",
      "Epoch: 230, Loss: nan\n",
      "Epoch: 231, Loss: nan\n",
      "Epoch: 232, Loss: nan\n",
      "Epoch: 233, Loss: nan\n",
      "Epoch: 234, Loss: nan\n",
      "Epoch: 235, Loss: nan\n",
      "Epoch: 236, Loss: nan\n",
      "Epoch: 237, Loss: nan\n",
      "Epoch: 238, Loss: nan\n",
      "Epoch: 239, Loss: nan\n",
      "Epoch: 240, Loss: nan\n",
      "Epoch: 241, Loss: nan\n",
      "Epoch: 242, Loss: nan\n",
      "Epoch: 243, Loss: nan\n",
      "Epoch: 244, Loss: nan\n",
      "Epoch: 245, Loss: nan\n",
      "Epoch: 246, Loss: nan\n",
      "Epoch: 247, Loss: nan\n",
      "Epoch: 248, Loss: nan\n",
      "Epoch: 249, Loss: nan\n",
      "Epoch: 250, Loss: nan\n",
      "Epoch: 251, Loss: nan\n",
      "Epoch: 252, Loss: nan\n",
      "Epoch: 253, Loss: nan\n",
      "Epoch: 254, Loss: nan\n",
      "Epoch: 255, Loss: nan\n",
      "Epoch: 256, Loss: nan\n",
      "Epoch: 257, Loss: nan\n",
      "Epoch: 258, Loss: nan\n",
      "Epoch: 259, Loss: nan\n",
      "Epoch: 260, Loss: nan\n",
      "Epoch: 261, Loss: nan\n",
      "Epoch: 262, Loss: nan\n",
      "Epoch: 263, Loss: nan\n",
      "Epoch: 264, Loss: nan\n",
      "Epoch: 265, Loss: nan\n",
      "Epoch: 266, Loss: nan\n",
      "Epoch: 267, Loss: nan\n",
      "Epoch: 268, Loss: nan\n",
      "Epoch: 269, Loss: nan\n",
      "Epoch: 270, Loss: nan\n",
      "Epoch: 271, Loss: nan\n",
      "Epoch: 272, Loss: nan\n",
      "Epoch: 273, Loss: nan\n",
      "Epoch: 274, Loss: nan\n",
      "Epoch: 275, Loss: nan\n",
      "Epoch: 276, Loss: nan\n",
      "Epoch: 277, Loss: nan\n",
      "Epoch: 278, Loss: nan\n",
      "Epoch: 279, Loss: nan\n",
      "Epoch: 280, Loss: nan\n",
      "Epoch: 281, Loss: nan\n",
      "Epoch: 282, Loss: nan\n",
      "Epoch: 283, Loss: nan\n",
      "Epoch: 284, Loss: nan\n",
      "Epoch: 285, Loss: nan\n",
      "Epoch: 286, Loss: nan\n",
      "Epoch: 287, Loss: nan\n",
      "Epoch: 288, Loss: nan\n",
      "Epoch: 289, Loss: nan\n",
      "Epoch: 290, Loss: nan\n",
      "Epoch: 291, Loss: nan\n",
      "Epoch: 292, Loss: nan\n",
      "Epoch: 293, Loss: nan\n",
      "Epoch: 294, Loss: nan\n",
      "Epoch: 295, Loss: nan\n",
      "Epoch: 296, Loss: nan\n",
      "Epoch: 297, Loss: nan\n",
      "Epoch: 298, Loss: nan\n",
      "Epoch: 299, Loss: nan\n",
      "Epoch: 300, Loss: nan\n",
      "Epoch: 301, Loss: nan\n",
      "Epoch: 302, Loss: nan\n",
      "Epoch: 303, Loss: nan\n",
      "Epoch: 304, Loss: nan\n",
      "Epoch: 305, Loss: nan\n",
      "Epoch: 306, Loss: nan\n",
      "Epoch: 307, Loss: nan\n",
      "Epoch: 308, Loss: nan\n",
      "Epoch: 309, Loss: nan\n",
      "Epoch: 310, Loss: nan\n",
      "Epoch: 311, Loss: nan\n",
      "Epoch: 312, Loss: nan\n",
      "Epoch: 313, Loss: nan\n",
      "Epoch: 314, Loss: nan\n",
      "Epoch: 315, Loss: nan\n",
      "Epoch: 316, Loss: nan\n",
      "Epoch: 317, Loss: nan\n",
      "Epoch: 318, Loss: nan\n",
      "Epoch: 319, Loss: nan\n",
      "Epoch: 320, Loss: nan\n",
      "Epoch: 321, Loss: nan\n",
      "Epoch: 322, Loss: nan\n",
      "Epoch: 323, Loss: nan\n",
      "Epoch: 324, Loss: nan\n",
      "Epoch: 325, Loss: nan\n",
      "Epoch: 326, Loss: nan\n",
      "Epoch: 327, Loss: nan\n",
      "Epoch: 328, Loss: nan\n",
      "Epoch: 329, Loss: nan\n",
      "Epoch: 330, Loss: nan\n",
      "Epoch: 331, Loss: nan\n",
      "Epoch: 332, Loss: nan\n",
      "Epoch: 333, Loss: nan\n",
      "Epoch: 334, Loss: nan\n",
      "Epoch: 335, Loss: nan\n",
      "Epoch: 336, Loss: nan\n",
      "Epoch: 337, Loss: nan\n",
      "Epoch: 338, Loss: nan\n",
      "Epoch: 339, Loss: nan\n",
      "Epoch: 340, Loss: nan\n",
      "Epoch: 341, Loss: nan\n",
      "Epoch: 342, Loss: nan\n",
      "Epoch: 343, Loss: nan\n",
      "Epoch: 344, Loss: nan\n",
      "Epoch: 345, Loss: nan\n",
      "Epoch: 346, Loss: nan\n",
      "Epoch: 347, Loss: nan\n",
      "Epoch: 348, Loss: nan\n",
      "Epoch: 349, Loss: nan\n",
      "Epoch: 350, Loss: nan\n",
      "Epoch: 351, Loss: nan\n",
      "Epoch: 352, Loss: nan\n",
      "Epoch: 353, Loss: nan\n",
      "Epoch: 354, Loss: nan\n",
      "Epoch: 355, Loss: nan\n",
      "Epoch: 356, Loss: nan\n",
      "Epoch: 357, Loss: nan\n",
      "Epoch: 358, Loss: nan\n",
      "Epoch: 359, Loss: nan\n",
      "Epoch: 360, Loss: nan\n",
      "Epoch: 361, Loss: nan\n",
      "Epoch: 362, Loss: nan\n",
      "Epoch: 363, Loss: nan\n",
      "Epoch: 364, Loss: nan\n",
      "Epoch: 365, Loss: nan\n",
      "Epoch: 366, Loss: nan\n",
      "Epoch: 367, Loss: nan\n",
      "Epoch: 368, Loss: nan\n",
      "Epoch: 369, Loss: nan\n",
      "Epoch: 370, Loss: nan\n",
      "Epoch: 371, Loss: nan\n",
      "Epoch: 372, Loss: nan\n",
      "Epoch: 373, Loss: nan\n",
      "Epoch: 374, Loss: nan\n",
      "Epoch: 375, Loss: nan\n",
      "Epoch: 376, Loss: nan\n",
      "Epoch: 377, Loss: nan\n",
      "Epoch: 378, Loss: nan\n",
      "Epoch: 379, Loss: nan\n",
      "Epoch: 380, Loss: nan\n",
      "Epoch: 381, Loss: nan\n",
      "Epoch: 382, Loss: nan\n",
      "Epoch: 383, Loss: nan\n",
      "Epoch: 384, Loss: nan\n",
      "Epoch: 385, Loss: nan\n",
      "Epoch: 386, Loss: nan\n",
      "Epoch: 387, Loss: nan\n",
      "Epoch: 388, Loss: nan\n",
      "Epoch: 389, Loss: nan\n",
      "Epoch: 390, Loss: nan\n",
      "Epoch: 391, Loss: nan\n",
      "Epoch: 392, Loss: nan\n",
      "Epoch: 393, Loss: nan\n",
      "Epoch: 394, Loss: nan\n",
      "Epoch: 395, Loss: nan\n",
      "Epoch: 396, Loss: nan\n",
      "Epoch: 397, Loss: nan\n",
      "Epoch: 398, Loss: nan\n",
      "Epoch: 399, Loss: nan\n",
      "Epoch: 400, Loss: nan\n"
     ]
    }
   ],
   "source": [
    "model = GCN()\n",
    "criterion = torch.nn.CrossEntropyLoss()  #Initialize the CrossEntropyLoss function.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # Initialize the Adam optimizer.\n",
    "\n",
    "def train(node_features, edge_index, edge_attr, node_labels):\n",
    "\n",
    "    # for i in range(node_features.shape[0]):\n",
    "    optimizer.zero_grad()  # Clear gradients.\n",
    "    out, h = model(node_features, edge_index, edge_attr)  # Perform a single forward pass.\n",
    "    loss = criterion(out, node_labels)  # Compute the loss solely based on the training nodes.\n",
    "    loss.backward()  # Derive gradients.\n",
    "    optimizer.step()  # Update parameters based on gradients.\n",
    "\n",
    "    return loss, h\n",
    "\n",
    "\n",
    "for epoch in range(401):\n",
    "    #torch_edge_weight = torch.tensor(np.sum(edge_features_x, axis=2), dtype=torch.float)\n",
    "    torch_edge_weight = torch.tensor(np.random.randn(12,1), dtype=torch.float)\n",
    "    loss, h = train(torch_node_features_x[0], torch_edge_index, torch_edge_weight, torch_node_labels[0])\n",
    "    print(f'Epoch: {epoch}, Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eece571f",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
